{
    "docs": [
        {
            "location": "/",
            "text": "Spartan is High Performance Computing (HPC) system operated by Research Platform Services (ResPlat) at The University of Melbourne. It combines a high performance bare-metal compute with flexible cloud infrastructure to suit a wide range of use-cases.\n\n\nIf your computing jobs take too long on your desktop computer, or are simply not possible due to a lack of speed and memory, a HPC system like Spartan can help.\n\n\nGetting Help\n\n\nTraining\n\n\nWe run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research. \n\n\nSignup here: \nhttp://melbourne.resbaz.edu.au/participate\n\n\nHelpdesk\n\n\nIf you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at \nhpc-support@unimelb.edu.au\n\n\nPlease submit one topic per ticket. If you require a assistance with a separate matter, compose a new ticket. Do not reply to existing or closed tickets.\n\n\nFor password resets please see \nthe FAQ\n or contact University Services on x40888 or \nservice-centre@unimelb.edu.au\n.\n\n\nSpecifications\n\n\nSpartan has a number of partitions available for general usage.\n\n\nCloud\n\n\n100 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow.\n\n\nPhysical\n\n\n21 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI).\n\n\nGPU\n\n\n3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking. This is the partition to use if your software takes advantage of GPU acceleration (e.g. using CUDA).\n\n\nCiting Spartan\n\n\nIf you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.\n\n\nUniversity of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa\n\n\nOther Resources\n\n\nSpartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions.\n\n\nNectar\n\n\nNectar\n is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories.\n\n\nSpartan runs some of it's computation resources in the Nectar cloud.\n\n\nMelbourne Bioinformatics\n\n\nMelbourne Bioinformatics\n run two large HPC systems for life sciences researchers.\n\n\n Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) \n \n\n\nMASSIVE\n is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.\n\n\n VicNode \n\n\nVicNode\n provides data storage and sharing services to Victorian researchers and their collaborators.",
            "title": "Home"
        },
        {
            "location": "/#getting-help",
            "text": "Training  We run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research.   Signup here:  http://melbourne.resbaz.edu.au/participate  Helpdesk  If you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at  hpc-support@unimelb.edu.au  Please submit one topic per ticket. If you require a assistance with a separate matter, compose a new ticket. Do not reply to existing or closed tickets.  For password resets please see  the FAQ  or contact University Services on x40888 or  service-centre@unimelb.edu.au .",
            "title": "Getting Help"
        },
        {
            "location": "/#specifications",
            "text": "Spartan has a number of partitions available for general usage.  Cloud  100 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow.  Physical  21 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI).  GPU  3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking. This is the partition to use if your software takes advantage of GPU acceleration (e.g. using CUDA).",
            "title": "Specifications"
        },
        {
            "location": "/#citing-spartan",
            "text": "If you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.  University of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa",
            "title": "Citing Spartan"
        },
        {
            "location": "/#other-resources",
            "text": "Spartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions.  Nectar  Nectar  is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories.  Spartan runs some of it's computation resources in the Nectar cloud.  Melbourne Bioinformatics  Melbourne Bioinformatics  run two large HPC systems for life sciences researchers.   Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE)     MASSIVE  is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.   VicNode   VicNode  provides data storage and sharing services to Victorian researchers and their collaborators.",
            "title": "Other Resources"
        },
        {
            "location": "/status/",
            "text": "Availability\n\n\nIs Spartan operating normally?\n\n\n\n    \n\n    \n\n\n\n\nCurrent Usage\n\n\nHow busy is Spartan today?\n\n\n\n\n\n\n\n\n\n\n  \n\n\nHow to Interpret\n\n\n\n\n\n\nThis plot provides indicates how many CPUs (cores) are in-use on Spartan.\n\n\nUtilization may occasionally exceed 100% as resources are added/removed from service (utilization is always relative to the most recent available CPU count).\n\n\n\n\n\n\n\n\n\n\n\nWait Time\n\n\nHow long will my job take to start?\n\n\n\n\n\n\n  \n\n    \n\n      Partition: \n\n    \n\n    \n\n      CPUs Requested: \n\n    \n\n    \n\n      Wall Time: \n\n    \n\n  \n\n\n\n\n\n\n\n\n  \n\n\nHow to Interpret\n\n\n\n\n\n\nThis plot provides data on how long previous jobs have taken to start, which can be used as guidance on how long your job might take to start.\n\n\nNote however that \"Past performance is no guarantee of future results\"; wait times can fluctuate quickly due to changes in usage or outages, and wait time could be considerably more or less than the historic average.\n\n\nDaily averages are shown, but points may be missing for days where there were no jobs matching the selected characteristics.\n\n\n\n\n\n\n\n\n\n\n\n\nvar sp = new StatusPage.page({ page: 'zxld2sws8c9x'});\n\nsp.summary({\n  success: function(data) {\n    // adds the text description to the dropdown\n    $('.color-description').text(data.status.description);\n    // appends the status indicator as a class name so we can use the right color for the status light thing\n    $('.color-dot').addClass(data.status.indicator);\n  }\n});\n\n\n\n\n\n\n    window.onload = function() {\n\n        Plotly.d3.json(\"https://swift.rc.nectar.org.au:8888/v1/AUTH_5634a7ad82ad49579e4192f4db90191f/spartan_metrics/wait_time.json\" + '?' + Math.floor(Math.random() * 1000), function (data) {\n\n            function assignOptions(options, selector) {\n                $.each(options, function (val, text) {\n                    selector.append(\n                        $('<option></option>').val(text).html(text)\n                    );\n                });\n            }\n\n            function updatePlot() {\n                var graphData = [\n                    {   x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y'],\n                        text: 'hours',\n                        mode: 'markers',\n                        hoverinfo: 'x+text+y',\n                        marker: {\n                            size: 10\n                        },\n                        name: 'Daily Average'\n                    },\n                    {\n                        x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y_rolling_average'],\n                        hoverinfo: 'none',\n                        line: {\n                            color: 'rgb(150, 150, 150)',\n                            width: 1,\n                            dash: 'dash'\n                        },\n                        name: 'Rolling 14d Mean'\n                    }\n                ];\n\n                var layout = {\n                    title: 'Job Wait Time',\n                    width: 700,\n                    height: 400,\n                    yaxis: {\n                        title: 'Hours',\n                        hoverformat: '.2f hours'\n                    },\n                    legend: {\n                        x: 0.5, \n                        y: 1.2,\n                        orientation: 'h',\n                        xanchor: 'center',\n                    }\n                };\n                Plotly.newPlot('graph', graphData, layout);\n            }\n\n            var partitionSelect = $('#partitionSelect');\n            var coreSelect = $('#coreSelect');\n            var wallTimeSelect = $('#wallTimeSelect');\n\n            // Populate drop-down options\n            assignOptions(data['options']['partitions'], partitionSelect);\n            assignOptions(data['options']['cores'], coreSelect);\n            assignOptions(data['options']['wall_times'], wallTimeSelect);\n\n            // Attach listeners\n            partitionSelect.change(updatePlot);\n            coreSelect.change(updatePlot);\n            wallTimeSelect.change(updatePlot);\n\n            // Build initial plot\n            updatePlot()\n\n        });\n    }",
            "title": "Status"
        },
        {
            "location": "/status/#availability",
            "text": "Is Spartan operating normally?",
            "title": "Availability"
        },
        {
            "location": "/status/#current-usage",
            "text": "How busy is Spartan today?     \n    How to Interpret    This plot provides indicates how many CPUs (cores) are in-use on Spartan.  Utilization may occasionally exceed 100% as resources are added/removed from service (utilization is always relative to the most recent available CPU count).",
            "title": "Current Usage"
        },
        {
            "location": "/status/#wait-time",
            "text": "How long will my job take to start?   \n   \n     \n      Partition:  \n     \n     \n      CPUs Requested:  \n     \n     \n      Wall Time:  \n     \n      \n    How to Interpret    This plot provides data on how long previous jobs have taken to start, which can be used as guidance on how long your job might take to start.  Note however that \"Past performance is no guarantee of future results\"; wait times can fluctuate quickly due to changes in usage or outages, and wait time could be considerably more or less than the historic average.  Daily averages are shown, but points may be missing for days where there were no jobs matching the selected characteristics.      \nvar sp = new StatusPage.page({ page: 'zxld2sws8c9x'});\n\nsp.summary({\n  success: function(data) {\n    // adds the text description to the dropdown\n    $('.color-description').text(data.status.description);\n    // appends the status indicator as a class name so we can use the right color for the status light thing\n    $('.color-dot').addClass(data.status.indicator);\n  }\n});  \n\n    window.onload = function() {\n\n        Plotly.d3.json(\"https://swift.rc.nectar.org.au:8888/v1/AUTH_5634a7ad82ad49579e4192f4db90191f/spartan_metrics/wait_time.json\" + '?' + Math.floor(Math.random() * 1000), function (data) {\n\n            function assignOptions(options, selector) {\n                $.each(options, function (val, text) {\n                    selector.append(\n                        $('<option></option>').val(text).html(text)\n                    );\n                });\n            }\n\n            function updatePlot() {\n                var graphData = [\n                    {   x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y'],\n                        text: 'hours',\n                        mode: 'markers',\n                        hoverinfo: 'x+text+y',\n                        marker: {\n                            size: 10\n                        },\n                        name: 'Daily Average'\n                    },\n                    {\n                        x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y_rolling_average'],\n                        hoverinfo: 'none',\n                        line: {\n                            color: 'rgb(150, 150, 150)',\n                            width: 1,\n                            dash: 'dash'\n                        },\n                        name: 'Rolling 14d Mean'\n                    }\n                ];\n\n                var layout = {\n                    title: 'Job Wait Time',\n                    width: 700,\n                    height: 400,\n                    yaxis: {\n                        title: 'Hours',\n                        hoverformat: '.2f hours'\n                    },\n                    legend: {\n                        x: 0.5, \n                        y: 1.2,\n                        orientation: 'h',\n                        xanchor: 'center',\n                    }\n                };\n                Plotly.newPlot('graph', graphData, layout);\n            }\n\n            var partitionSelect = $('#partitionSelect');\n            var coreSelect = $('#coreSelect');\n            var wallTimeSelect = $('#wallTimeSelect');\n\n            // Populate drop-down options\n            assignOptions(data['options']['partitions'], partitionSelect);\n            assignOptions(data['options']['cores'], coreSelect);\n            assignOptions(data['options']['wall_times'], wallTimeSelect);\n\n            // Attach listeners\n            partitionSelect.change(updatePlot);\n            coreSelect.change(updatePlot);\n            wallTimeSelect.change(updatePlot);\n\n            // Build initial plot\n            updatePlot()\n\n        });\n    }",
            "title": "Wait Time"
        },
        {
            "location": "/getting_started/",
            "text": "Prerequisite:\n You'll need a basic understanding of the Linux command line to use Spartan. But don't worry, you don't need to be an expert, and there are many resources out there to help you. \nThis tutorial\n is a good place to start.\n\n\n1. Create an account\n\n\nGo to \nKaraage\n to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one.\n\n\n\n\n2. Login to Spartan via SSH\n\n\nNote that your password for Spartan is created during sign-up, and is different to your university password.\n\n\nWindows\n\n\nDownload an SSH client such as \nPuTTY\n, set hostname as \nspartan.hpc.unimelb.edu.au\n  and click Open. You'll be asked for your Spartan username and password.\n\n\nPosix (Linux, OS X)\n\n\nYou'll already have an SSH client installed. Easy! Open a terminal and enter:\n\n\n$ ssh yourUsername@spartan.hpc.unimelb.edu.au\n\n\n\n\n3. Create a job\n\n\nSpartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset.\n\n\nCopy the example into your home directory, and change working directory:\n\n\n$ cp -r /usr/local/common/Python ~/\n\n\n$ cd ~/Python\n\n\nThe dataset is in \nminitwitter.csv\n, and the analysis code in \ntwitter_search_541635.py\n. The files ending in \n.slurm\n tell Spartan how to run your job. For example, \ntwitter_one_node_eight_cores.slurm\n requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for).\n\n\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --time=0-12:00:00\n\n# Load required modules\nmodule load Python/3.5.2-intel-2016.u3\n\n# Launch multiple process python code\necho \"Searching for mentions\"\ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m\necho \"Searching for topics\"\ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t\necho \"Searching for the keyword 'jumping'\"\ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping\n\n\n\n\n\n\n3. Submit your job\n\n\nFirst off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node. \nPlease don't run jobs on the login node!\n\n\nInstead, use the scheduling tool \nSlurm\n, and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available.\n\n\nGo ahead and launch your job using \nsbatch\n:\n\n\n$ sbatch twitter_one_node_eight_cores.slurm\n> Submitted batch job 27300\n\n\n\n\nWe can check how it's progressing using \nsqueue\n:\n\n\n$ squeue --job 27300\n>            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             27300     cloud twitter_   perryd  R      10:48      1 spartan040\n\n\n\n\nWhen complete, an output file is created which logs the output from your job, for the above this has the filename \nslurm-27300.out\n.\n\n\nYou can also perform interactive work using the \nsinteractive\n command. This is handy for testing and debugging. This will allocate and log you in to a computing node.",
            "title": "Getting Started"
        },
        {
            "location": "/managing_data/",
            "text": "Chances are you need to run your HPC job against a dataset, perhaps quite a sizable one. There are a number of places to store data on Spartan while you're working with it, and ways to get data in and out.\n\n\nNot for Long-Term Storage\n\n\nWhile it's often essential to have fast nearby storage while working on your data, please don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first). \n\n\nData and Storage Solutions Beyond Spartan\n\n\nThe University offers a range of other data storage and management solutions to meet your needs, beyond the short-term storage available on Spartan, which are described \nhere\n. \n\n\nIn some cases it's possible to integrate these resources with your account on Spartan to streamline your workflow. \nGet in touch\n if you'd like to find out more for your particular application.\n\n\nTransferring Files to Spartan\n\n\nThere is a mismatched MTU between the Spartan physical nodes and their VLAN, which will be fixed in the next maintenance window. If downloads to Spartan are not working it is recommended that a proxy is established before using \nwget\n or \ncurl\n etc. For example:\n\n\nexport http_proxy|https_proxy|ftp_proxy=http://wwwproxy.unimelb.edu.au:8000\n    \n\n\nwget --no-check-certificate ftp://ftp.broadinstitute.org/distribution/83RC2_q_image.tgz\n\n\nWhere to Store Your Data on Spartan\n\n\nProjects Directory\n\n\nYour projects directory is the best place to store research data while you're working on it. It's located at \n/data/projects/myprojectname\n.\n\n\nOthers in your project can access it, and 500 GB of storage is available per project. If you need more than this, \nget in touch\n and we'll try to find a solution.\n\n\nHome Directory\n\n\nYour home directory, i.e. \n/home/yourusername\n can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our \ngetting started\n tutorial or testing out new software.\n\n\nOthers in your project won't have access, and you're limited to 50GB of storage.\n\n\nScratch Space\n\n\nYou can store temporary working data while your job is running at \n/scratch/\n. This is handy if your job generates large files while it's running that you don't need to keep. Total scratch space is limited to 8TB, shared among all users.\n\n\nHow to Transfer Data In and Out of Spartan\n\n\nSecure Copy (scp)\n\n\nYou can use the \nscp\n command to move data from your local machine to Spartan. For example, to move \nmydata.dat\n from your current working directory to your project directory on Spartan:\n\n\n$ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n\n\nYou can transfer files from Spartan to your local machine by reversing the order of the arguments like so:\n\n\n$ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  local.dat\n\n\nFor Windows users, PuTTY provides an equivalent tool called \npscp\n. If you're data is located on a remote machine, you can SSH into that system first, and then use \nscp\n from that machine to transfer your data into Spartan.\n\n\nIf you'd prefer a GUI interface, you can use tools like \nFileZilla\n (cross-platform) or \nCyberDuck\n (OS X & Windows).\n\n\nrsync\n\n\nRepeatedly transferring large files in and out of Spartan via \nscp\n can be tedious. A good alternative is \nrsync\n, which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for \nscp\n.\n\n\n$ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n\n\nNote that the first argument is the source, and the second is the destination which will be modified to match the source.",
            "title": "Managing Data"
        },
        {
            "location": "/managing_data/#not-for-long-term-storage",
            "text": "While it's often essential to have fast nearby storage while working on your data, please don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first).",
            "title": "Not for Long-Term Storage"
        },
        {
            "location": "/managing_data/#data-and-storage-solutions-beyond-spartan",
            "text": "The University offers a range of other data storage and management solutions to meet your needs, beyond the short-term storage available on Spartan, which are described  here .   In some cases it's possible to integrate these resources with your account on Spartan to streamline your workflow.  Get in touch  if you'd like to find out more for your particular application.",
            "title": "Data and Storage Solutions Beyond Spartan"
        },
        {
            "location": "/managing_data/#transferring-files-to-spartan",
            "text": "There is a mismatched MTU between the Spartan physical nodes and their VLAN, which will be fixed in the next maintenance window. If downloads to Spartan are not working it is recommended that a proxy is established before using  wget  or  curl  etc. For example:  export http_proxy|https_proxy|ftp_proxy=http://wwwproxy.unimelb.edu.au:8000       wget --no-check-certificate ftp://ftp.broadinstitute.org/distribution/83RC2_q_image.tgz",
            "title": "Transferring Files to Spartan"
        },
        {
            "location": "/managing_data/#where-to-store-your-data-on-spartan",
            "text": "Projects Directory  Your projects directory is the best place to store research data while you're working on it. It's located at  /data/projects/myprojectname .  Others in your project can access it, and 500 GB of storage is available per project. If you need more than this,  get in touch  and we'll try to find a solution.  Home Directory  Your home directory, i.e.  /home/yourusername  can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our  getting started  tutorial or testing out new software.  Others in your project won't have access, and you're limited to 50GB of storage.  Scratch Space  You can store temporary working data while your job is running at  /scratch/ . This is handy if your job generates large files while it's running that you don't need to keep. Total scratch space is limited to 8TB, shared among all users.",
            "title": "Where to Store Your Data on Spartan"
        },
        {
            "location": "/managing_data/#how-to-transfer-data-in-and-out-of-spartan",
            "text": "Secure Copy (scp)  You can use the  scp  command to move data from your local machine to Spartan. For example, to move  mydata.dat  from your current working directory to your project directory on Spartan:  $ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  You can transfer files from Spartan to your local machine by reversing the order of the arguments like so:  $ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  local.dat  For Windows users, PuTTY provides an equivalent tool called  pscp . If you're data is located on a remote machine, you can SSH into that system first, and then use  scp  from that machine to transfer your data into Spartan.  If you'd prefer a GUI interface, you can use tools like  FileZilla  (cross-platform) or  CyberDuck  (OS X & Windows).  rsync  Repeatedly transferring large files in and out of Spartan via  scp  can be tedious. A good alternative is  rsync , which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for  scp .  $ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  Note that the first argument is the source, and the second is the destination which will be modified to match the source.",
            "title": "How to Transfer Data In and Out of Spartan"
        },
        {
            "location": "/software/",
            "text": "This page outlines usage and tips for some of the most popular software being used on Spartan. \n\n\nSpartan uses a modules system (\nlmod\n) to load and unload different packages, including different versions of the same software. This allows many different software packages to be installed on Spartan at once without interfering with each other\n\n\nYou can check what's currently installed using the \nmodule avail\n command, search using \nmodule spider\n, and load a particular module with the \nmodule load\n command. For example, to load MATLAB 2016a, use \nmodule load MATLAB/2016a\n.\n\n\nGenerally you shouldn't load modules from the login node, instead working on a compute node, either via an interactive session (launched with \nsinteractive\n), or from within your Slurm script.\n\n\nPython\n\n\nThere are multiple versions of Python installed on Spartan, which you can check using \nmodule spider Python\n. \n\n\nCommon packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using \npip install --user <package name>\n. This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.\n\n\nR\n\n\nR version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.\n\n\nMATLAB\n\n\nMATLAB 2016a is installed on Spartan, along with all of the standard toolboxes. \n\n\nMATLAB can be invoked with a particular script using \nmatlab -nodisplay -nodesktop -r \"run my_script.m\"\n. You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.",
            "title": "Software"
        },
        {
            "location": "/software/#python",
            "text": "There are multiple versions of Python installed on Spartan, which you can check using  module spider Python .   Common packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using  pip install --user <package name> . This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.",
            "title": "Python"
        },
        {
            "location": "/software/#r",
            "text": "R version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.",
            "title": "R"
        },
        {
            "location": "/software/#matlab",
            "text": "MATLAB 2016a is installed on Spartan, along with all of the standard toolboxes.   MATLAB can be invoked with a particular script using  matlab -nodisplay -nodesktop -r \"run my_script.m\" . You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.",
            "title": "MATLAB"
        },
        {
            "location": "/faq/",
            "text": "What's special about Spartan?\n\n\nMost modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed.\n\n\nFor certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of \nembarrassingly parallel\n. That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance.\n\n\nSpartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.\n\n\nHow do I get an account?\n\n\nAccess to Spartan requires an an account, which you can request using \nKaraage\n. \n\n\nAccounts are associated with a particular project; you can either join an existing project or create a new one.\n\n\nNew projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.\n\n\nHow do I access Spartan once I have an account?\n\n\nYou'll need an SSH client. Mac and Linux computers will already have one installed, just use the command \nssh yourUsername@spartan.hpc.unimelb.edu.au\n at your terminal.\n\n\nFor Windows, you'll need to download an SSH client such as \nPuTTY\n, set hostname as \nspartan.hpc.unimelb.edu.au\n and select Open. You'll be asked for your Spartan username and password.\n\n\nMy password isn't working!\n\n\n\n\n\n\nMake sure you're using your Spartan password that you set in \nKaraage\n. \n Your Spartan password is not necessarily the same as your central university password.\n\n\n\n\n\n\nYou can request a password reset \nhere\n.\n\n\n\n\n\n\nIf you are still having trouble, contact the University of Melbourne Service Desk on x40888 or email or \nservice-centre@unimelb.edu.au\n.\n\n\n\n\n\n\nWhat are Spartan's specifications?\n\n\nSpartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us.\n\n\nThe cloud partition nominally consists of 100 nodes with 8 cores each from the \nNectar\n research cloud, however it is capable of accessing more as the load on Spartan grows.\n\n\nThere also exist a number of specialist nodes with expanded memory or \nGPGPU\n hardware, as well as partitions dedicated to particular departments and research groups.\n\n\nWhat software is installed?\n\n\nSpartan uses a modules system (\nlmod\n) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the \nmodule avail\n command, and load a module with the \nmodule load\n command.\n\n\nTypically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with \nsinteractive\n). Instead you load the modules in your Slurm script before executing your particular software.\n\n\nWhat if the software I need is not installed?\n\n\nGet in contact\n with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that:\n\n\n\n\nIt works\n\n\nSoftware licenses are managed\n\n\nCode is compiled with the appropriate flags to maximize performance\n\n\nOthers users can also make use of the software.\n\n\n\n\nWhere do I go for help?\n\n\nFirst off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check \nhere\n for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at: \nhttp://melbourne.resbaz.edu.au/participate\n\n\nSecond, check the documentation here, as well as for the software you're running on Spartan (like Slurm).\n\n\nFinally, if you ever get stuck, please feel free to \nemail HPC support\n. We're here to help make your research more productive and enjoyable, and we'll do everything we can to help. \n\n\nHow do I get access to GPUs?\n\n\nYou'll need to add two parameters to your Slurm script, \n#SBATCH --partition gpu\n and \n#SBATCH --gres=gpu\n. You can access up four GPUs in a single job using \n#SBATCH --gres=gpu:4\n. There is also a specialist partition which can be accessed with \n#SBATCH --partition gpgpu\n. \n\n\nNote that with the generic resource request \ngpu\n you will be allocated gpus without differentiation. If you need specific gpus these can be specified by type.  We have two different types on the Spartan gpgpu partion, \nk80\n and \np100\n.\n\n\nFor example when submitting a a job that requests \n--gres=gpu\n for 1 GPU or \n--gres=gpu:2\n for 2 GPUs per task then that can be satisfied by either type. But if a specific type (for example P100) is neededthen the submission will require \n--gres=gpu:p100\n and if 2 per task is desired then \n--gres=gpu:p100:2\n is required.\n\n\nCUDA 7, 7.5 and 8 are available, along with NVidia driver 367.48. \n\n\nNote:\n: As of February 2018, the GPGPU parition (utilising NVidia P100 GPUs), is still undergoing testing and is not yet available for general use.\n\n\nHow do I submit a job?\n\n\nYou'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out \nGetting Started\n for an example.\n\n\nDo I need to know how to use Linux?\n\n\nJust the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as \nthis tutorial\n. \n\n\nHow do I create a multi-core job?\n\n\nThere are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this:\n\n\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n\n\n\n\nThis is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores.\n\n\nAlternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core:\n\n\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n\n\n\n\nThis approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick.\n\n\nKeep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.\n\n\nHow do I create a multi-node job?\n\n\nHere's an example of a job with two nodes, each using 12 cores.\n\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=12\n\n\n\n\nNote that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI.\n\n\nFor multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes. \n\n\nWhat other options are there for running my job?\n\n\nMany different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for \nSlurm\n (the job manager we use on Spartan) for details.\n\n\nHow do I create a job array?\n\n\nJob arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other.\n\n\nSay we have an array of files, \ndata_1.dat\n to \ndata_50.dat\n to process with \nmyProgram\n:\n\n\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --time=0-00:15:00\n#SBATCH --array=1-50\n\nmyProgram data_${SLURM_ARRAY_TASK_ID}.dat\n\n\n\n\nThis will create 50 jobs, each calling \nmyProgram\n with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)\n\n\nHow do I request more memory?\n\n\nBy default the scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. In some cases this might not be enough (e.g., very large dataset that needs to be loaded with low level of parallelisation).\n\n\nAdditional memory can be allocated with the \n--mem=[mem][M|G|T]\n directive (entire job) or \n--mem-per-cpu=[mem][M|G|T]\n (per core). Maximum should be based around total cores -1 (for system processes). The --mem-per-cpu directive is for threads for OpenMP applications and processor ranks for MPI.\n\n\nIt is best to reserve some memory (about 1 core's worth) for system processes.\n\n\nAre there more examples I can look at?\n\n\nIf you go to \n/usr/local/common/\n on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.\n\n\nHow do I make my program run fast on Spartan?\n\n\nSpartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, so improved performance is dependent on using parallel processing (MPI or OpenMP) or job arrays. \n\n\nHow do I cite Spartan in my publications?\n\n\nIf you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.\n\n\nUniversity of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa\n\n\nHow do I transition my work from the old HPC system Edward to Spartan?\n\n\nHere's a \nguide\n to help you.\n\n\nHow do setup passwordless SSH login?\n\n\nA passwordless SSH for Spartan will make your life easier. You won't\neven need to remember your password!\n\n\nIf you have a *nix system (e.g., UNIX, Linux, MacOS X) open up a\nterminal on your \nlocal\n system and generate a keypair.\n\n\n$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nCreated directory '/home/user/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\n43:51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 user@localhost\n\n\n\n\nNow append the new public key to \n~/.ssh/authorized_keys\n on Spartan (you'll be asked for your password one last time).\n\n\n$ cat .ssh/id_rsa.pub | ssh username@spartan.hpc.unimelb.edu.au 'cat >> .ssh/authorized_keys'\n\n\n\n\nDepending on your version of SSH you might also have to do the following\nchanges:\n\n\n\n\nPut the public key in .ssh/authorized_keys2\n\n\nChange the permissions of .ssh to 700\n\n\nChange the permissions of .ssh/authorized_keys2 to 640\n\n\n\n\nYou can now SSH to Spartan without having to enter your password!\n\n\nHow can I avoid typing myUsername@spartan.hpc.unimelb.edu.au everytime I connect?\n\n\nAn SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname. \n\n\nCreate the text file in your \n~/.ssh\n directory with your preferred text editor, for example, \nnano\n.\n\n\nnano .ssh/config\n\n\n\n\nEnter the following (replacing \nusername\n with your actual username of course!):\n\n\nHost *\nServerAliveInterval 120\nHost spartan\n       Hostname spartan.hpc.unimelb.edu.au\n       User username\n\n\n\n\nNow to connect to Spartan, you need only type \nssh spartan\n.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#whats-special-about-spartan",
            "text": "Most modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed.  For certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of  embarrassingly parallel . That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance.  Spartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.",
            "title": "What's special about Spartan?"
        },
        {
            "location": "/faq/#how-do-i-get-an-account",
            "text": "Access to Spartan requires an an account, which you can request using  Karaage .   Accounts are associated with a particular project; you can either join an existing project or create a new one.  New projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.",
            "title": "How do I get an account?"
        },
        {
            "location": "/faq/#how-do-i-access-spartan-once-i-have-an-account",
            "text": "You'll need an SSH client. Mac and Linux computers will already have one installed, just use the command  ssh yourUsername@spartan.hpc.unimelb.edu.au  at your terminal.  For Windows, you'll need to download an SSH client such as  PuTTY , set hostname as  spartan.hpc.unimelb.edu.au  and select Open. You'll be asked for your Spartan username and password.",
            "title": "How do I access Spartan once I have an account?"
        },
        {
            "location": "/faq/#my-password-isnt-working",
            "text": "Make sure you're using your Spartan password that you set in  Karaage .   Your Spartan password is not necessarily the same as your central university password.    You can request a password reset  here .    If you are still having trouble, contact the University of Melbourne Service Desk on x40888 or email or  service-centre@unimelb.edu.au .",
            "title": "My password isn't working!"
        },
        {
            "location": "/faq/#what-are-spartans-specifications",
            "text": "Spartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us.  The cloud partition nominally consists of 100 nodes with 8 cores each from the  Nectar  research cloud, however it is capable of accessing more as the load on Spartan grows.  There also exist a number of specialist nodes with expanded memory or  GPGPU  hardware, as well as partitions dedicated to particular departments and research groups.",
            "title": "What are Spartan's specifications?"
        },
        {
            "location": "/faq/#what-software-is-installed",
            "text": "Spartan uses a modules system ( lmod ) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the  module avail  command, and load a module with the  module load  command.  Typically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with  sinteractive ). Instead you load the modules in your Slurm script before executing your particular software.",
            "title": "What software is installed?"
        },
        {
            "location": "/faq/#what-if-the-software-i-need-is-not-installed",
            "text": "Get in contact  with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that:   It works  Software licenses are managed  Code is compiled with the appropriate flags to maximize performance  Others users can also make use of the software.",
            "title": "What if the software I need is not installed?"
        },
        {
            "location": "/faq/#where-do-i-go-for-help",
            "text": "First off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check  here  for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at:  http://melbourne.resbaz.edu.au/participate  Second, check the documentation here, as well as for the software you're running on Spartan (like Slurm).  Finally, if you ever get stuck, please feel free to  email HPC support . We're here to help make your research more productive and enjoyable, and we'll do everything we can to help.",
            "title": "Where do I go for help?"
        },
        {
            "location": "/faq/#how-do-i-get-access-to-gpus",
            "text": "You'll need to add two parameters to your Slurm script,  #SBATCH --partition gpu  and  #SBATCH --gres=gpu . You can access up four GPUs in a single job using  #SBATCH --gres=gpu:4 . There is also a specialist partition which can be accessed with  #SBATCH --partition gpgpu .   Note that with the generic resource request  gpu  you will be allocated gpus without differentiation. If you need specific gpus these can be specified by type.  We have two different types on the Spartan gpgpu partion,  k80  and  p100 .  For example when submitting a a job that requests  --gres=gpu  for 1 GPU or  --gres=gpu:2  for 2 GPUs per task then that can be satisfied by either type. But if a specific type (for example P100) is neededthen the submission will require  --gres=gpu:p100  and if 2 per task is desired then  --gres=gpu:p100:2  is required.  CUDA 7, 7.5 and 8 are available, along with NVidia driver 367.48.   Note: : As of February 2018, the GPGPU parition (utilising NVidia P100 GPUs), is still undergoing testing and is not yet available for general use.",
            "title": "How do I get access to GPUs?"
        },
        {
            "location": "/faq/#how-do-i-submit-a-job",
            "text": "You'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out  Getting Started  for an example.",
            "title": "How do I submit a job?"
        },
        {
            "location": "/faq/#do-i-need-to-know-how-to-use-linux",
            "text": "Just the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as  this tutorial .",
            "title": "Do I need to know how to use Linux?"
        },
        {
            "location": "/faq/#how-do-i-create-a-multi-core-job",
            "text": "There are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this:  #SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8  This is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores.  Alternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core:  #SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1  This approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick.  Keep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.",
            "title": "How do I create a multi-core job?"
        },
        {
            "location": "/faq/#how-do-i-create-a-multi-node-job",
            "text": "Here's an example of a job with two nodes, each using 12 cores.  #SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=12  Note that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI.  For multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes.",
            "title": "How do I create a multi-node job?"
        },
        {
            "location": "/faq/#what-other-options-are-there-for-running-my-job",
            "text": "Many different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for  Slurm  (the job manager we use on Spartan) for details.",
            "title": "What other options are there for running my job?"
        },
        {
            "location": "/faq/#how-do-i-create-a-job-array",
            "text": "Job arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other.  Say we have an array of files,  data_1.dat  to  data_50.dat  to process with  myProgram :  #!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --time=0-00:15:00\n#SBATCH --array=1-50\n\nmyProgram data_${SLURM_ARRAY_TASK_ID}.dat  This will create 50 jobs, each calling  myProgram  with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)",
            "title": "How do I create a job array?"
        },
        {
            "location": "/faq/#how-do-i-request-more-memory",
            "text": "By default the scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. In some cases this might not be enough (e.g., very large dataset that needs to be loaded with low level of parallelisation).  Additional memory can be allocated with the  --mem=[mem][M|G|T]  directive (entire job) or  --mem-per-cpu=[mem][M|G|T]  (per core). Maximum should be based around total cores -1 (for system processes). The --mem-per-cpu directive is for threads for OpenMP applications and processor ranks for MPI.  It is best to reserve some memory (about 1 core's worth) for system processes.",
            "title": "How do I request more memory?"
        },
        {
            "location": "/faq/#are-there-more-examples-i-can-look-at",
            "text": "If you go to  /usr/local/common/  on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.",
            "title": "Are there more examples I can look at?"
        },
        {
            "location": "/faq/#how-do-i-make-my-program-run-fast-on-spartan",
            "text": "Spartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, so improved performance is dependent on using parallel processing (MPI or OpenMP) or job arrays.",
            "title": "How do I make my program run fast on Spartan?"
        },
        {
            "location": "/faq/#how-do-i-cite-spartan-in-my-publications",
            "text": "If you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.  University of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa",
            "title": "How do I cite Spartan in my publications?"
        },
        {
            "location": "/faq/#how-do-i-transition-my-work-from-the-old-hpc-system-edward-to-spartan",
            "text": "Here's a  guide  to help you.",
            "title": "How do I transition my work from the old HPC system Edward to Spartan?"
        },
        {
            "location": "/faq/#how-do-setup-passwordless-ssh-login",
            "text": "A passwordless SSH for Spartan will make your life easier. You won't\neven need to remember your password!  If you have a *nix system (e.g., UNIX, Linux, MacOS X) open up a\nterminal on your  local  system and generate a keypair.  $ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nCreated directory '/home/user/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\n43:51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 user@localhost  Now append the new public key to  ~/.ssh/authorized_keys  on Spartan (you'll be asked for your password one last time).  $ cat .ssh/id_rsa.pub | ssh username@spartan.hpc.unimelb.edu.au 'cat >> .ssh/authorized_keys'  Depending on your version of SSH you might also have to do the following\nchanges:   Put the public key in .ssh/authorized_keys2  Change the permissions of .ssh to 700  Change the permissions of .ssh/authorized_keys2 to 640   You can now SSH to Spartan without having to enter your password!",
            "title": "How do setup passwordless SSH login?"
        },
        {
            "location": "/faq/#how-can-i-avoid-typing-myusernamespartanhpcunimelbeduau-everytime-i-connect",
            "text": "An SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname.   Create the text file in your  ~/.ssh  directory with your preferred text editor, for example,  nano .  nano .ssh/config  Enter the following (replacing  username  with your actual username of course!):  Host *\nServerAliveInterval 120\nHost spartan\n       Hostname spartan.hpc.unimelb.edu.au\n       User username  Now to connect to Spartan, you need only type  ssh spartan .",
            "title": "How can I avoid typing myUsername@spartan.hpc.unimelb.edu.au everytime I connect?"
        },
        {
            "location": "/edward_transition/",
            "text": "Edward to Spartan : A Short Transition Guide\n\n\nWhen is Edward Being Retired?\n\n\nEdward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects.\n\n\nEdward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.\n\n\nHow is Spartan Different?\n\n\nSpartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition.\n\n\nSpartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply.\n\n\nHowever, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling.\n\n\nThis means that job scripts will be different between the two systems and translation will be required.\n\n\nHow do we submit jobs in SLURM?\n\n\nJob submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system.\n\n\nCore command for checking queue is: showq (TORQUE) or squeue (SLURM)\n\n\nCore command for job submission is qsub< [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)\n\n\nWhat About Working Directories and Environments?\n\n\nTORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this.\n\n\nTORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.\n\n\nWhat about Job Status and Output?\n\n\nCore command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM)\n\n\nBoth TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the \n-j oe\n option in PBS).\n\n\nWhat are the user command differences?\n\n\n\n\n\n\n\n\nUser Command\n\n\nTORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nJob submission\n\n\nqsub [script_file]\n\n\nsbatch [script_file]\n\n\n\n\n\n\nJob delete\n\n\nqdel [job_id]\n\n\nscancel [job_id]\n\n\n\n\n\n\nJob status\n\n\nqstat [job_id]\n\n\nsqueue [job_id]\n\n\n\n\n\n\nJob status\n\n\nqstat -u [user_name]\n\n\nsqueue -u [user_name]\n\n\n\n\n\n\nNode list\n\n\npbsnodes -a\n\n\nsinfo -N\n\n\n\n\n\n\nQueue list\n\n\nqstat -Q\n\n\nsqueue\n\n\n\n\n\n\nCluster status\n\n\nshowq\n\n\nqstatus -a\n\n\n\n\n\n\n\n\nWhat are the job command differences?\n\n\n\n\n\n\n\n\nJob Specification\n\n\nTORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nScript directive\n\n\n#PBS\n\n\n#SBATCH\n\n\n\n\n\n\nQueue\n\n\n-q [queue]\n\n\n-p [queue]\n\n\n\n\n\n\nJob Name\n\n\n-N [name]\n\n\n--job-name=[name]\n\n\n\n\n\n\nNodes\n\n\n-l nodes=[count]\n\n\n-N [min[-max]]\n\n\n\n\n\n\nCPU Count\n\n\n-l ppn=[count]\n\n\n-n [count]\n\n\n\n\n\n\nWall Clock Limit\n\n\n-l walltime=[hh:mm:ss]\n\n\n-t [days-hh:mm:ss]\n\n\n\n\n\n\nEvent Address\n\n\n-M [address]\n\n\n--mail-user=[address]\n\n\n\n\n\n\nEvent Notification\n\n\n-m abe\n\n\n--mail-type=[events]\n\n\n\n\n\n\nMemory Size\n\n\n-l mem=[MB]\n\n\n--mem=[mem][M G T]\n\n\n\n\n\n\nProc Memory Size\n\n\n-l pmem=[MB]\n\n\n--mem-per-cpu=[mem][M G T]\n\n\n\n\n\n\n\n\nWhat are the environment commands differences?\n\n\n\n\n\n\n\n\nEnvironment\n\n\nCommand TORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nJob ID\n\n\n$PBS_JOBID\n\n\n$SLURM_JOBID\n\n\n\n\n\n\nSubmit Directory\n\n\n$PBS_O_WORKDIR\n\n\n$SLURM_SUBMIT_DIR\n\n\n\n\n\n\nSubmit Host\n\n\n$PBS_O_HOST\n\n\n$SLURM_SUBMIT_HOST\n\n\n\n\n\n\nNode List\n\n\n$PBS_NODEFILE\n\n\n$SLURM_JOB_NODELIST\n\n\n\n\n\n\nJob Array\n\n\nIndex $PBS_ARRAYID\n\n\n$SLURM_ARRAY_TASK_ID\n\n\n\n\n\n\n\n\nAutomation and Acknowledgements\n\n\nThere is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm\n\n\nThis guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016",
            "title": "_Edward Transition"
        },
        {
            "location": "/edward_transition/#edward-to-spartan-a-short-transition-guide",
            "text": "",
            "title": "Edward to Spartan : A Short Transition Guide"
        },
        {
            "location": "/edward_transition/#when-is-edward-being-retired",
            "text": "Edward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects.  Edward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.",
            "title": "When is Edward Being Retired?"
        },
        {
            "location": "/edward_transition/#how-is-spartan-different",
            "text": "Spartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition.  Spartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply.  However, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling.  This means that job scripts will be different between the two systems and translation will be required.",
            "title": "How is Spartan Different?"
        },
        {
            "location": "/edward_transition/#how-do-we-submit-jobs-in-slurm",
            "text": "Job submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system.  Core command for checking queue is: showq (TORQUE) or squeue (SLURM)  Core command for job submission is qsub< [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)",
            "title": "How do we submit jobs in SLURM?"
        },
        {
            "location": "/edward_transition/#what-about-working-directories-and-environments",
            "text": "TORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this.  TORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.",
            "title": "What About Working Directories and Environments?"
        },
        {
            "location": "/edward_transition/#what-about-job-status-and-output",
            "text": "Core command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM)  Both TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the  -j oe  option in PBS).",
            "title": "What about Job Status and Output?"
        },
        {
            "location": "/edward_transition/#what-are-the-user-command-differences",
            "text": "User Command  TORQUE (Edward)  SLURM (Spartan)      Job submission  qsub [script_file]  sbatch [script_file]    Job delete  qdel [job_id]  scancel [job_id]    Job status  qstat [job_id]  squeue [job_id]    Job status  qstat -u [user_name]  squeue -u [user_name]    Node list  pbsnodes -a  sinfo -N    Queue list  qstat -Q  squeue    Cluster status  showq  qstatus -a",
            "title": "What are the user command differences?"
        },
        {
            "location": "/edward_transition/#what-are-the-job-command-differences",
            "text": "Job Specification  TORQUE (Edward)  SLURM (Spartan)      Script directive  #PBS  #SBATCH    Queue  -q [queue]  -p [queue]    Job Name  -N [name]  --job-name=[name]    Nodes  -l nodes=[count]  -N [min[-max]]    CPU Count  -l ppn=[count]  -n [count]    Wall Clock Limit  -l walltime=[hh:mm:ss]  -t [days-hh:mm:ss]    Event Address  -M [address]  --mail-user=[address]    Event Notification  -m abe  --mail-type=[events]    Memory Size  -l mem=[MB]  --mem=[mem][M G T]    Proc Memory Size  -l pmem=[MB]  --mem-per-cpu=[mem][M G T]",
            "title": "What are the job command differences?"
        },
        {
            "location": "/edward_transition/#what-are-the-environment-commands-differences",
            "text": "Environment  Command TORQUE (Edward)  SLURM (Spartan)      Job ID  $PBS_JOBID  $SLURM_JOBID    Submit Directory  $PBS_O_WORKDIR  $SLURM_SUBMIT_DIR    Submit Host  $PBS_O_HOST  $SLURM_SUBMIT_HOST    Node List  $PBS_NODEFILE  $SLURM_JOB_NODELIST    Job Array  Index $PBS_ARRAYID  $SLURM_ARRAY_TASK_ID",
            "title": "What are the environment commands differences?"
        },
        {
            "location": "/edward_transition/#automation-and-acknowledgements",
            "text": "There is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm  This guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016",
            "title": "Automation and Acknowledgements"
        }
    ]
}