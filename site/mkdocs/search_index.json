{
    "docs": [
        {
            "location": "/", 
            "text": "Spartan is High Performance Computing (HPC) system operated by Research Platform Services (ResPlat) at The University of Melbourne. It combines a high performance bare-metal compute with flexible cloud infrastructure to suit a wide range of use-cases.\n\n\nIf your computing jobs take too long on your desktop computer, or are simply not possible due to a lack of speed and memory, a HPC system like Spartan can help.\n\n\nGetting Help\n\n\nTraining\n\n\nWe run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research. \n\n\nSignup here: \nhttp://melbourne.resbaz.edu.au/participate\n\n\nHelpdesk\n\n\nIf you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at \nhpc-support@unimelb.edu.au\n\n\nSpecifications\n\n\nSpartan has a number of partitions available for general usage.\n\n\nCloud\n\n\n100 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow.\n\n\nPhysical\n\n\n21 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI).\n\n\nGPU\n\n\n3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking. This is the partition to use if your software takes advantage of GPU acceleration (e.g. using CUDA).\n\n\nCiting Spartan\n\n\nIf you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.\n\n\nUniversity of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa\n\n\nOther Resources\n\n\nSpartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions.\n\n\nNectar\n\n\nNectar\n is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories.\n\n\nSpartan runs some of it's computation resources in the Nectar cloud.\n\n\nVictorian Life Sciences Computation Initiative (VLSCI)\n\n\nVLSCI\n run three large HPC systems for life sciences researchers.\n\n\n Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) \n \n\n\nMASSIVE\n is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.\n\n\n VicNode \n\n\nVicNode\n provides data storage and sharing services to Victorian researchers and their collaborators.", 
            "title": "Home"
        }, 
        {
            "location": "/#getting-help", 
            "text": "", 
            "title": "Getting Help"
        }, 
        {
            "location": "/#training", 
            "text": "We run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research.   Signup here:  http://melbourne.resbaz.edu.au/participate", 
            "title": "Training"
        }, 
        {
            "location": "/#helpdesk", 
            "text": "If you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at  hpc-support@unimelb.edu.au", 
            "title": "Helpdesk"
        }, 
        {
            "location": "/#specifications", 
            "text": "Spartan has a number of partitions available for general usage.  Cloud  100 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow.  Physical  21 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI).  GPU  3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking. This is the partition to use if your software takes advantage of GPU acceleration (e.g. using CUDA).", 
            "title": "Specifications"
        }, 
        {
            "location": "/#citing-spartan", 
            "text": "If you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.  University of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa", 
            "title": "Citing Spartan"
        }, 
        {
            "location": "/#other-resources", 
            "text": "Spartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions.  Nectar  Nectar  is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories.  Spartan runs some of it's computation resources in the Nectar cloud.  Victorian Life Sciences Computation Initiative (VLSCI)  VLSCI  run three large HPC systems for life sciences researchers.   Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE)     MASSIVE  is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.   VicNode   VicNode  provides data storage and sharing services to Victorian researchers and their collaborators.", 
            "title": "Other Resources"
        }, 
        {
            "location": "/status/", 
            "text": "Availability\n\n\nIs Spartan operating normally?\n\n\n(statuspage.io widget here)\n\n\nCurrent Usage\n\n\nHow busy is Spartan today?\n\n\n\n\n\n\n\n\n\nWait Time\n\n\nHow long will my job take to start?", 
            "title": "Status"
        }, 
        {
            "location": "/status/#availability", 
            "text": "Is Spartan operating normally?  (statuspage.io widget here)", 
            "title": "Availability"
        }, 
        {
            "location": "/status/#current-usage", 
            "text": "How busy is Spartan today?", 
            "title": "Current Usage"
        }, 
        {
            "location": "/status/#wait-time", 
            "text": "How long will my job take to start?", 
            "title": "Wait Time"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Prerequisite:\n You'll need a basic understanding of the Linux command line to use Spartan. But don't worry, you don't need to be an expert, and there are many resources out there to help you. \nThis tutorial\n is a good place to start.\n\n\n1. Create an account\n\n\nGo to \nKaraage\n to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one.\n\n\n\n\n2. Login to Spartan via SSH\n\n\nNote that your password for Spartan is created during sign-up, and is different to your university password.\n\n\nWindows\n\n\nDownload an SSH client such as \nPuTTY\n, set hostname as \nspartan.hpc.unimelb.edu.au\n  and click Open. You'll be asked for your Spartan username and password.\n\n\nPosix (Linux, OS X)\n\n\nYou'll already have an SSH client installed. Easy! Open a terminal and enter:\n\n\n$ ssh yourUsername@spartan.hpc.unimelb.edu.au\n\n\n\n\n3. Create a job\n\n\nSpartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset.\n\n\nCopy the example into your home directory, and change working directory:\n\n\n$ cp -r /usr/local/common/Python ~/\n\n\n$ cd ~/Python\n\n\nThe dataset is in \nminitwitter.csv\n, and the analysis code in \ntwitter_search_541635.py\n. The files ending in \n.slurm\n tell Spartan how to run your job. For example, \ntwitter_one_node_eight_cores.slurm\n requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for).\n\n\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --time=0-12:00:00\n\n# Load required modules\nmodule load Python/3.5.2-intel-2016.u3\n\n# Launch multiple process python code\necho \nSearching for mentions\n\ntime mpiexec -n 16 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m\necho \nSearching for topics\n\ntime mpiexec -n 16 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t\necho \nSearching for the keyword 'jumping'\n\ntime mpiexec -n 16 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping\n\n\n\n\n\n\n3. Submit your job\n\n\nFirst off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node. \nPlease don't run jobs on the login node!\n\n\nInstead, use the scheduling tool \nSlurm\n, and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available.\n\n\nGo ahead and launch your job using \nsbatch\n:\n\n\n$ sbatch twitter_one_node_sixteen_cores.slurm\n\n Submitted batch job 27300\n\n\n\n\nWe can check how it's progressing using \nsqueue\n:\n\n\n$ squeue --job 27300\n\n            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             27300     cloud twitter_   perryd  R      10:48      1 spartan040\n\n\n\n\nWhen complete, an output file is created which logs the output from your job, for the above this has the filename \nslurm-27300.out\n.\n\n\nYou can also perform interactive work using the \nsinteractive\n command. This is handy for testing and debugging. This will allocate and log you in to a computing node.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#1-create-an-account", 
            "text": "Go to  Karaage  to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one.", 
            "title": "1. Create an account"
        }, 
        {
            "location": "/getting_started/#2-login-to-spartan-via-ssh", 
            "text": "Note that your password for Spartan is created during sign-up, and is different to your university password.  Windows  Download an SSH client such as  PuTTY , set hostname as  spartan.hpc.unimelb.edu.au   and click Open. You'll be asked for your Spartan username and password.  Posix (Linux, OS X)  You'll already have an SSH client installed. Easy! Open a terminal and enter:  $ ssh yourUsername@spartan.hpc.unimelb.edu.au", 
            "title": "2. Login to Spartan via SSH"
        }, 
        {
            "location": "/getting_started/#3-create-a-job", 
            "text": "Spartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset.  Copy the example into your home directory, and change working directory:  $ cp -r /usr/local/common/Python ~/  $ cd ~/Python  The dataset is in  minitwitter.csv , and the analysis code in  twitter_search_541635.py . The files ending in  .slurm  tell Spartan how to run your job. For example,  twitter_one_node_eight_cores.slurm  requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for).  #!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --time=0-12:00:00\n\n# Load required modules\nmodule load Python/3.5.2-intel-2016.u3\n\n# Launch multiple process python code\necho  Searching for mentions \ntime mpiexec -n 16 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m\necho  Searching for topics \ntime mpiexec -n 16 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t\necho  Searching for the keyword 'jumping' \ntime mpiexec -n 16 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping", 
            "title": "3. Create a job"
        }, 
        {
            "location": "/getting_started/#3-submit-your-job", 
            "text": "First off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node.  Please don't run jobs on the login node!  Instead, use the scheduling tool  Slurm , and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available.  Go ahead and launch your job using  sbatch :  $ sbatch twitter_one_node_sixteen_cores.slurm  Submitted batch job 27300  We can check how it's progressing using  squeue :  $ squeue --job 27300             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             27300     cloud twitter_   perryd  R      10:48      1 spartan040  When complete, an output file is created which logs the output from your job, for the above this has the filename  slurm-27300.out .  You can also perform interactive work using the  sinteractive  command. This is handy for testing and debugging. This will allocate and log you in to a computing node.", 
            "title": "3. Submit your job"
        }, 
        {
            "location": "/managing_data/", 
            "text": "Chances are you need to run your HPC job against a dataset, perhaps quite a sizable one. There are a number of places to store data on Spartan while you're working with it, and ways to get data in and out.\n\n\nNot for Long-Term Storage\n\n\nWhile it's often essential to have fast nearby storage while working on your data, don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first). \n\n\nVicNode\n offers a range of data storage services that may suit your needs. If you're unsure, get in \ncontact\n with us.\n\n\nWhere to Store Your Data on Spartan\n\n\nProjects Directory\n\n\nYour projects directory is the best place to store research data while you're working on it. It's located at \n/data/projects/myprojectname\n.\n\n\nOthers in your project can access it, and 500 GB of storage is available per project. If you need more than this, \nget in touch\n and we'll try to find a solution.\n\n\nHome Directory\n\n\nYour home directory, i.e. \n/home/yourusername\n can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our \ngetting started\n tutorial or testing out new software.\n\n\nOthers in your project won't have access, and you're limited to 50GB of storage.\n\n\nScratch Space\n\n\nYou can store temporary working data while your job is running at \n/scratch/\n. This is handy if your job generates large files while it's running that you don't need to keep. Total scratch space is limited to 8TB, shared among all users.\n\n\nHow to Transfer Data In and Out of Spartan\n\n\nSecure Copy (scp)\n\n\nYou can use the \nscp\n command to move data from your local machine to Spartan. For example, to move \nmydata.dat\n from your current working directory to your project directory on Spartan:\n\n\n$ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n\n\nYou can transfer files from Spartan to your local machine by reversing the order of the arguments like so:\n\n\n$ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  local.dat\n\n\nFor Windows users, PuTTY provides an equivalent tool called \npscp\n. If you're data is located on a remote machine, you can SSH into that system first, and then use \nscp\n from that machine to transfer your data into Spartan.\n\n\nIf you'd prefer a GUI interface, you can use tools like \nFileZilla\n (cross-platform) or \nCyberDuck\n (OS X \n Windows).\n\n\nrsync\n\n\nRepeatedly transferring large files in and out of Spartan via \nscp\n can be tedious. A good alternative is \nrsync\n, which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for \nscp\n.\n\n\n$ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n\n\nNote that the first argument is the source, and the second is the destination which will be modified to match the source.\n\n\nVicNode\n\n\nVicNode provides data storage that can be mounted on Spartan. It's then very easy to transfer files back and fourth as needed. \nContact us\n with details of the VicNode resources you'd like to mount, and we can arrange this for you.\n\n\nUse the IO Node for Large Data Transfer Jobs\n\n\nThe approach above transfers files via the login node, which is fine for small files. For bigger transfers, we encourage you to use \nspartan-io.hpc.unimelb.edu.au\n instead. That will prevent resources on the login node being tied up (slowing things down for everyone), and is likely to result in faster transfers for you.\n\n\nThe IO node has access to the same home and project directories on Spartan. Simply substitute \nspartan-io\n in the commands above, for example \nscp local.dat myusername@spartan-io.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n to transfer \nlocal.dat\n to Spartan via the IO node.", 
            "title": "Managing Data"
        }, 
        {
            "location": "/managing_data/#not-for-long-term-storage", 
            "text": "While it's often essential to have fast nearby storage while working on your data, don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first).   VicNode  offers a range of data storage services that may suit your needs. If you're unsure, get in  contact  with us.", 
            "title": "Not for Long-Term Storage"
        }, 
        {
            "location": "/managing_data/#where-to-store-your-data-on-spartan", 
            "text": "Projects Directory  Your projects directory is the best place to store research data while you're working on it. It's located at  /data/projects/myprojectname .  Others in your project can access it, and 500 GB of storage is available per project. If you need more than this,  get in touch  and we'll try to find a solution.  Home Directory  Your home directory, i.e.  /home/yourusername  can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our  getting started  tutorial or testing out new software.  Others in your project won't have access, and you're limited to 50GB of storage.  Scratch Space  You can store temporary working data while your job is running at  /scratch/ . This is handy if your job generates large files while it's running that you don't need to keep. Total scratch space is limited to 8TB, shared among all users.", 
            "title": "Where to Store Your Data on Spartan"
        }, 
        {
            "location": "/managing_data/#how-to-transfer-data-in-and-out-of-spartan", 
            "text": "Secure Copy (scp)  You can use the  scp  command to move data from your local machine to Spartan. For example, to move  mydata.dat  from your current working directory to your project directory on Spartan:  $ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  You can transfer files from Spartan to your local machine by reversing the order of the arguments like so:  $ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  local.dat  For Windows users, PuTTY provides an equivalent tool called  pscp . If you're data is located on a remote machine, you can SSH into that system first, and then use  scp  from that machine to transfer your data into Spartan.  If you'd prefer a GUI interface, you can use tools like  FileZilla  (cross-platform) or  CyberDuck  (OS X   Windows).  rsync  Repeatedly transferring large files in and out of Spartan via  scp  can be tedious. A good alternative is  rsync , which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for  scp .  $ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  Note that the first argument is the source, and the second is the destination which will be modified to match the source.  VicNode  VicNode provides data storage that can be mounted on Spartan. It's then very easy to transfer files back and fourth as needed.  Contact us  with details of the VicNode resources you'd like to mount, and we can arrange this for you.", 
            "title": "How to Transfer Data In and Out of Spartan"
        }, 
        {
            "location": "/managing_data/#use-the-io-node-for-large-data-transfer-jobs", 
            "text": "The approach above transfers files via the login node, which is fine for small files. For bigger transfers, we encourage you to use  spartan-io.hpc.unimelb.edu.au  instead. That will prevent resources on the login node being tied up (slowing things down for everyone), and is likely to result in faster transfers for you.  The IO node has access to the same home and project directories on Spartan. Simply substitute  spartan-io  in the commands above, for example  scp local.dat myusername@spartan-io.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  to transfer  local.dat  to Spartan via the IO node.", 
            "title": "Use the IO Node for Large Data Transfer Jobs"
        }, 
        {
            "location": "/software/", 
            "text": "This page outlines usage and tips for some of the most popular software being used on Spartan. \n\n\nSpartan uses a modules system (\nlmod\n) to load and unload different packages, including different versions of the same software. This allows many different software packages to be installed on Spartan at once without interfering with each other\n\n\nYou can check what's currently installed using the \nmodule avail\n command, search using \nmodule spider\n, and load a particular module with the \nmodule load\n command. For example, to load MATLAB 2016a, use \nmodule load MATLAB/2016a\n.\n\n\nGenerally you shouldn't load modules from the login node, instead working on a compute node, either via an interactive session (launched with \nsinteractive\n), or from within your Slurm script.\n\n\nPython\n\n\nThere are multiple versions of Python installed on Spartan, which you can check using \nmodule spider Python\n. \n\n\nCommon packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using \npip install --user \npackage name\n. This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.\n\n\nR\n\n\nR version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.\n\n\nMATLAB\n\n\nMATLAB 2016a is installed on Spartan, along with all of the standard toolboxes. \n\n\nMATLAB can be invoked with a particular script using \nmatlab -nodisplay -nodesktop -r \"run my_script.m\"\n. You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.", 
            "title": "Software"
        }, 
        {
            "location": "/software/#python", 
            "text": "There are multiple versions of Python installed on Spartan, which you can check using  module spider Python .   Common packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using  pip install --user  package name . This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.", 
            "title": "Python"
        }, 
        {
            "location": "/software/#r", 
            "text": "R version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.", 
            "title": "R"
        }, 
        {
            "location": "/software/#matlab", 
            "text": "MATLAB 2016a is installed on Spartan, along with all of the standard toolboxes.   MATLAB can be invoked with a particular script using  matlab -nodisplay -nodesktop -r \"run my_script.m\" . You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.", 
            "title": "MATLAB"
        }, 
        {
            "location": "/faq/", 
            "text": "What's special about Spartan?\n\n\nMost modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed.\n\n\nFor certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of \nembarrassingly parallel\n. That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance.\n\n\nSpartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.\n\n\nHow do I get an account?\n\n\nAccess to Spartan requires an an account, which you can request using \nKaraage\n. \n\n\nAccounts are associated with a particular project; you can either join an existing project or create a new one.\n\n\nNew projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.\n\n\nHow do I access Spartan once I have an account?\n\n\nYou'll need an SSH client. Mac and Linux computers will already have one installed, just use the command \nssh yourUsername@spartan.hpc.unimelb.edu.au\n at your terminal.\n\n\nFor Windows, you'll need to download an SSH client such as \nPuTTY\n, set hostname as \nspartan.hpc.unimelb.edu.au\n  and click Open. You'll be asked for your Spartan username and password.\n\n\nMy password isn't working!\n\n\n\n\n\n\nMake sure you're using your Spartan password that you set in \nKaraage\n. \n Your Spartan password is not necessarily the same as your central university password.\n\n\n\n\n\n\nYou can request a password reset \nhere\n.\n\n\n\n\n\n\nWhat are Spartan's specifications?\n\n\nSpartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us.\n\n\nThe cloud partition nominally consists of 100 nodes with 8 cores each from the \nNectar\n research cloud, however it is capable of accessing more as the load on Spartan grows.\n\n\nThere also exist a number of specialist nodes with expanded memory or \nGPGPU\n hardware, as well as partitions dedicated to particular departments and research groups.\n\n\nWhat software is installed?\n\n\nSpartan uses a modules system (\nlmod\n) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the \nmodule avail\n command, and load a module with the \nmodule load\n command.\n\n\nTypically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with \nsinteractive\n). Instead you load the modules in your Slurm script before executing your particular software.\n\n\nWhat if the software I need is not installed?\n\n\nGet in contact\n with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that:\n\n\n\n\nIt works\n\n\nSoftware licenses are managed\n\n\nCode is compiled with the appropriate flags to maximize performance\n\n\nOthers users can also make use of the software.\n\n\n\n\nWhere do I go for help?\n\n\nFirst off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check \nhere\n for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at: \nhttp://melbourne.resbaz.edu.au/participate\n\n\nSecond, check the documentation here, as well as for the software you're running on Spartan (like Slurm).\n\n\nFinally, if you ever get stuck, please feel free to \nemail HPC support\n. We're here to help make your research more productive and enjoyable, and we'll do everything we can to help. \n\n\nHow do I get access to GPUs?\n\n\nYou'll need to add two parameters to your Slurm script, \n#SBATCH --partition gpu\n and \n#SBATCH --gres=gpu\n. You can access up four GPUs in a single job using \n#SBATCH --gres=gpu:4\n.\n\n\nCUDA 7, 7.5 and 8 are available, along with NVidia driver 367.48. \n\n\nHow do I submit a job?\n\n\nYou'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out \nGetting Started\n for an example.\n\n\nDo I need to know how to use Linux?\n\n\nJust the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as \nthis tutorial\n. \n\n\nHow do I create a multi-core job?\n\n\nThere are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this:\n\n\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n\n\n\n\nThis is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores.\n\n\nAlternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core:\n\n\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n\n\n\n\nThis approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick.\n\n\nKeep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.\n\n\nHow do I create a multi-node job?\n\n\nHere's an example of a job with two nodes, each using 12 cores.\n\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=12\n\n\n\n\nNote that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI.\n\n\nFor multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes. \n\n\nWhat other options are there for running my job?\n\n\nMany different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for \nSlurm\n (the job manager we use on Spartan) for details.\n\n\nHow do I create a job array?\n\n\nJob arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other.\n\n\nSay we have an array of files, \ndata_1.dat\n to \ndata_50.dat\n to process with \nmyProgram\n:\n\n\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --time=0-00:15:00\n#SBATCH --array=1-50\n\nmyProgram data_${SLURM_ARRAY_TASK_ID}.dat\n\n\n\n\nThis will create 50 jobs, each calling \nmyProgram\n with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)\n\n\nAre there more examples I can look at?\n\n\nIf you go to \n/usr/local/common/\n on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.\n\n\nHow do I make my program run fast on Spartan?\n\n\nSpartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, and so getting things to \n\n\nThe individual cores within Spartan aren't much faster than those\n\n\nHow do I cite Spartan in my publications?\n\n\nIf you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.\n\n\nUniversity of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa\n\n\nHow do I transition my work from the old HPC system Edward to Spartan?\n\n\nHere's a \nguide\n to help you.", 
            "title": "FAQ"
        }, 
        {
            "location": "/faq/#whats-special-about-spartan", 
            "text": "Most modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed.  For certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of  embarrassingly parallel . That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance.  Spartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.", 
            "title": "What's special about Spartan?"
        }, 
        {
            "location": "/faq/#how-do-i-get-an-account", 
            "text": "Access to Spartan requires an an account, which you can request using  Karaage .   Accounts are associated with a particular project; you can either join an existing project or create a new one.  New projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.", 
            "title": "How do I get an account?"
        }, 
        {
            "location": "/faq/#how-do-i-access-spartan-once-i-have-an-account", 
            "text": "You'll need an SSH client. Mac and Linux computers will already have one installed, just use the command  ssh yourUsername@spartan.hpc.unimelb.edu.au  at your terminal.  For Windows, you'll need to download an SSH client such as  PuTTY , set hostname as  spartan.hpc.unimelb.edu.au   and click Open. You'll be asked for your Spartan username and password.", 
            "title": "How do I access Spartan once I have an account?"
        }, 
        {
            "location": "/faq/#my-password-isnt-working", 
            "text": "Make sure you're using your Spartan password that you set in  Karaage .   Your Spartan password is not necessarily the same as your central university password.    You can request a password reset  here .", 
            "title": "My password isn't working!"
        }, 
        {
            "location": "/faq/#what-are-spartans-specifications", 
            "text": "Spartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us.  The cloud partition nominally consists of 100 nodes with 8 cores each from the  Nectar  research cloud, however it is capable of accessing more as the load on Spartan grows.  There also exist a number of specialist nodes with expanded memory or  GPGPU  hardware, as well as partitions dedicated to particular departments and research groups.", 
            "title": "What are Spartan's specifications?"
        }, 
        {
            "location": "/faq/#what-software-is-installed", 
            "text": "Spartan uses a modules system ( lmod ) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the  module avail  command, and load a module with the  module load  command.  Typically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with  sinteractive ). Instead you load the modules in your Slurm script before executing your particular software.", 
            "title": "What software is installed?"
        }, 
        {
            "location": "/faq/#what-if-the-software-i-need-is-not-installed", 
            "text": "Get in contact  with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that:   It works  Software licenses are managed  Code is compiled with the appropriate flags to maximize performance  Others users can also make use of the software.", 
            "title": "What if the software I need is not installed?"
        }, 
        {
            "location": "/faq/#where-do-i-go-for-help", 
            "text": "First off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check  here  for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at:  http://melbourne.resbaz.edu.au/participate  Second, check the documentation here, as well as for the software you're running on Spartan (like Slurm).  Finally, if you ever get stuck, please feel free to  email HPC support . We're here to help make your research more productive and enjoyable, and we'll do everything we can to help.", 
            "title": "Where do I go for help?"
        }, 
        {
            "location": "/faq/#how-do-i-get-access-to-gpus", 
            "text": "You'll need to add two parameters to your Slurm script,  #SBATCH --partition gpu  and  #SBATCH --gres=gpu . You can access up four GPUs in a single job using  #SBATCH --gres=gpu:4 .  CUDA 7, 7.5 and 8 are available, along with NVidia driver 367.48.", 
            "title": "How do I get access to GPUs?"
        }, 
        {
            "location": "/faq/#how-do-i-submit-a-job", 
            "text": "You'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out  Getting Started  for an example.", 
            "title": "How do I submit a job?"
        }, 
        {
            "location": "/faq/#do-i-need-to-know-how-to-use-linux", 
            "text": "Just the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as  this tutorial .", 
            "title": "Do I need to know how to use Linux?"
        }, 
        {
            "location": "/faq/#how-do-i-create-a-multi-core-job", 
            "text": "There are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this:  #SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8  This is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores.  Alternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core:  #SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1  This approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick.  Keep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.", 
            "title": "How do I create a multi-core job?"
        }, 
        {
            "location": "/faq/#how-do-i-create-a-multi-node-job", 
            "text": "Here's an example of a job with two nodes, each using 12 cores.  #SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=12  Note that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI.  For multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes.", 
            "title": "How do I create a multi-node job?"
        }, 
        {
            "location": "/faq/#what-other-options-are-there-for-running-my-job", 
            "text": "Many different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for  Slurm  (the job manager we use on Spartan) for details.", 
            "title": "What other options are there for running my job?"
        }, 
        {
            "location": "/faq/#how-do-i-create-a-job-array", 
            "text": "Job arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other.  Say we have an array of files,  data_1.dat  to  data_50.dat  to process with  myProgram :  #!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --time=0-00:15:00\n#SBATCH --array=1-50\n\nmyProgram data_${SLURM_ARRAY_TASK_ID}.dat  This will create 50 jobs, each calling  myProgram  with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)", 
            "title": "How do I create a job array?"
        }, 
        {
            "location": "/faq/#are-there-more-examples-i-can-look-at", 
            "text": "If you go to  /usr/local/common/  on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.", 
            "title": "Are there more examples I can look at?"
        }, 
        {
            "location": "/faq/#how-do-i-make-my-program-run-fast-on-spartan", 
            "text": "Spartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, and so getting things to   The individual cores within Spartan aren't much faster than those", 
            "title": "How do I make my program run fast on Spartan?"
        }, 
        {
            "location": "/faq/#how-do-i-cite-spartan-in-my-publications", 
            "text": "If you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.  University of Melbourne (2017) Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. https://doi.org/10.4225/49/58ead90dceaaa", 
            "title": "How do I cite Spartan in my publications?"
        }, 
        {
            "location": "/faq/#how-do-i-transition-my-work-from-the-old-hpc-system-edward-to-spartan", 
            "text": "Here's a  guide  to help you.", 
            "title": "How do I transition my work from the old HPC system Edward to Spartan?"
        }, 
        {
            "location": "/edward_transition/", 
            "text": "Edward to Spartan : A Short Transition Guide\n\n\nWhen is Edward Being Retired?\n\n\nEdward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects.\n\n\nEdward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.\n\n\nHow is Spartan Different?\n\n\nSpartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition.\n\n\nSpartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply.\n\n\nHowever, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling.\n\n\nThis means that job scripts will be different between the two systems and translation will be required.\n\n\nHow do we submit jobs in SLURM?\n\n\nJob submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system.\n\n\nCore command for checking queue is: showq (TORQUE) or squeue (SLURM)\n\n\nCore command for job submission is qsub\n [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)\n\n\nWhat About Working Directories and Environments?\n\n\nTORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this.\n\n\nTORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.\n\n\nWhat about Job Status and Output?\n\n\nCore command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM)\n\n\nBoth TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the \n-j oe\n option in PBS).\n\n\nWhat are the user command differences?\n\n\n\n\n\n\n\n\nUser Command\n\n\nTORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nJob submission\n\n\nqsub [script_file]\n\n\nsbatch [script_file]\n\n\n\n\n\n\nJob delete\n\n\nqdel [job_id]\n\n\nscancel [job_id]\n\n\n\n\n\n\nJob status\n\n\nqstat [job_id]\n\n\nsqueue [job_id]\n\n\n\n\n\n\nJob status\n\n\nqstat -u [user_name]\n\n\nsqueue -u [user_name]\n\n\n\n\n\n\nNode list\n\n\npbsnodes -a\n\n\nsinfo -N\n\n\n\n\n\n\nQueue list\n\n\nqstat -Q\n\n\nsqueue\n\n\n\n\n\n\nCluster status\n\n\nshowq\n\n\nqstatus -a\n\n\n\n\n\n\n\n\nWhat are the job command differences?\n\n\n\n\n\n\n\n\nJob Specification\n\n\nTORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nScript directive\n\n\n#PBS\n\n\n#SBATCH\n\n\n\n\n\n\nQueue\n\n\n-q [queue]\n\n\n-p [queue]\n\n\n\n\n\n\nJob Name\n\n\n-N [name]\n\n\n--job-name=[name]\n\n\n\n\n\n\nNodes\n\n\n-l nodes=[count]\n\n\n-N [min[-max]]\n\n\n\n\n\n\nCPU Count\n\n\n-l ppn=[count]\n\n\n-n [count]\n\n\n\n\n\n\nWall Clock Limit\n\n\n-l walltime=[hh:mm:ss]\n\n\n-t [days-hh:mm:ss]\n\n\n\n\n\n\nEvent Address\n\n\n-M [address]\n\n\n--mail-user=[address]\n\n\n\n\n\n\nEvent Notification\n\n\n-m abe\n\n\n--mail-type=[events]\n\n\n\n\n\n\nMemory Size\n\n\n-l mem=[MB]\n\n\n--mem=[mem][M G T]\n\n\n\n\n\n\nProc Memory Size\n\n\n-l pmem=[MB]\n\n\n--mem-per-cpu=[mem][M G T]\n\n\n\n\n\n\n\n\nWhat are the environment commands differences?\n\n\n\n\n\n\n\n\nEnvironment\n\n\nCommand TORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nJob ID\n\n\n$PBS_JOBID\n\n\n$SLURM_JOBID\n\n\n\n\n\n\nSubmit Directory\n\n\n$PBS_O_WORKDIR\n\n\n$SLURM_SUBMIT_DIR\n\n\n\n\n\n\nSubmit Host\n\n\n$PBS_O_HOST\n\n\n$SLURM_SUBMIT_HOST\n\n\n\n\n\n\nNode List\n\n\n$PBS_NODEFILE\n\n\n$SLURM_JOB_NODELIST\n\n\n\n\n\n\nJob Array\n\n\nIndex $PBS_ARRAYID\n\n\n$SLURM_ARRAY_TASK_ID\n\n\n\n\n\n\n\n\nAutomation and Acknowledgements\n\n\nThere is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm\n\n\nThis guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016", 
            "title": "_Edward Transition"
        }, 
        {
            "location": "/edward_transition/#edward-to-spartan-a-short-transition-guide", 
            "text": "", 
            "title": "Edward to Spartan : A Short Transition Guide"
        }, 
        {
            "location": "/edward_transition/#when-is-edward-being-retired", 
            "text": "Edward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects.  Edward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.", 
            "title": "When is Edward Being Retired?"
        }, 
        {
            "location": "/edward_transition/#how-is-spartan-different", 
            "text": "Spartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition.  Spartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply.  However, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling.  This means that job scripts will be different between the two systems and translation will be required.", 
            "title": "How is Spartan Different?"
        }, 
        {
            "location": "/edward_transition/#how-do-we-submit-jobs-in-slurm", 
            "text": "Job submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system.  Core command for checking queue is: showq (TORQUE) or squeue (SLURM)  Core command for job submission is qsub  [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)", 
            "title": "How do we submit jobs in SLURM?"
        }, 
        {
            "location": "/edward_transition/#what-about-working-directories-and-environments", 
            "text": "TORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this.  TORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.", 
            "title": "What About Working Directories and Environments?"
        }, 
        {
            "location": "/edward_transition/#what-about-job-status-and-output", 
            "text": "Core command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM)  Both TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the  -j oe  option in PBS).", 
            "title": "What about Job Status and Output?"
        }, 
        {
            "location": "/edward_transition/#what-are-the-user-command-differences", 
            "text": "User Command  TORQUE (Edward)  SLURM (Spartan)      Job submission  qsub [script_file]  sbatch [script_file]    Job delete  qdel [job_id]  scancel [job_id]    Job status  qstat [job_id]  squeue [job_id]    Job status  qstat -u [user_name]  squeue -u [user_name]    Node list  pbsnodes -a  sinfo -N    Queue list  qstat -Q  squeue    Cluster status  showq  qstatus -a", 
            "title": "What are the user command differences?"
        }, 
        {
            "location": "/edward_transition/#what-are-the-job-command-differences", 
            "text": "Job Specification  TORQUE (Edward)  SLURM (Spartan)      Script directive  #PBS  #SBATCH    Queue  -q [queue]  -p [queue]    Job Name  -N [name]  --job-name=[name]    Nodes  -l nodes=[count]  -N [min[-max]]    CPU Count  -l ppn=[count]  -n [count]    Wall Clock Limit  -l walltime=[hh:mm:ss]  -t [days-hh:mm:ss]    Event Address  -M [address]  --mail-user=[address]    Event Notification  -m abe  --mail-type=[events]    Memory Size  -l mem=[MB]  --mem=[mem][M G T]    Proc Memory Size  -l pmem=[MB]  --mem-per-cpu=[mem][M G T]", 
            "title": "What are the job command differences?"
        }, 
        {
            "location": "/edward_transition/#what-are-the-environment-commands-differences", 
            "text": "Environment  Command TORQUE (Edward)  SLURM (Spartan)      Job ID  $PBS_JOBID  $SLURM_JOBID    Submit Directory  $PBS_O_WORKDIR  $SLURM_SUBMIT_DIR    Submit Host  $PBS_O_HOST  $SLURM_SUBMIT_HOST    Node List  $PBS_NODEFILE  $SLURM_JOB_NODELIST    Job Array  Index $PBS_ARRAYID  $SLURM_ARRAY_TASK_ID", 
            "title": "What are the environment commands differences?"
        }, 
        {
            "location": "/edward_transition/#automation-and-acknowledgements", 
            "text": "There is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm  This guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016", 
            "title": "Automation and Acknowledgements"
        }
    ]
}