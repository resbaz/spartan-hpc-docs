{
    "docs": [
        {
            "location": "/", 
            "text": "In the coming weeks, we'll be switching our helpdesk system to ServiceNow. The current hpc-support@unimelb.edu.au contact email will continue to work, but there will also be a web-based form to submit support and software installation requests. We're not anticipating any disruptions, and the current helpdesk system will be run in parallel for existing tickets. We will let you know of the cut-over date once confirmed.\n\n\n\n\nSpartan is High Performance Computing (HPC) system operated by Research Platform Services (ResPlat) at The University of Melbourne. It combines a high performance bare-metal compute with flexible cloud infrastructure to suit a wide range of use-cases.\n\n\nIf your computing jobs take too long on your desktop computer, or are simply not possible due to a lack of speed and memory, a HPC system like Spartan can help.\n\n\nUse of this service is governed by the University's \ngeneral regulations for IT resources\n and our \nHPC Support Service Policy\n.\n\n\nSpartan Daily Weather Report (20180914)\n\n\n\n\nUtilisation of disk is at 31% from the Ceph pool.\n\n\nSpartan is very busy on cloud partion, with close to 99% node allocation.\n\n\nSpartan is busy on physical partition, with close to 90% node allocation.\n\n\nSpartan is very busy on on GPGPU partition with 100% node allocation.\n\n\nMany cloud nodes out (67), mainly due to qh2-uom migration.\n\n\n\n\nGetting Help\n\n\nTraining\n\n\nWe run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research. \n\n\nSignup here: \nhttp://melbourne.resbaz.edu.au/participate\n\n\nHelpdesk\n\n\nIf you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at \nhpc-support@unimelb.edu.au\n\n\nPlease submit one topic per ticket. If you require a assistance with a separate matter, compose a new ticket. Do not reply to existing or closed tickets.\n\n\nFor password resets please see \nthe FAQ\n or contact University Services on +61 3 8344 0999 or ext 40999 or email \nservice-centre@unimelb.edu.au\n.\n\n\nSpecifications\n\n\nSpartan has a number of partitions available for general usage. A full list of partitions can be viewed with the command \nsinfo -s\n.\n\n\nCloud\n\n\n208 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow.\n\n\nPhysical\n\n\n20 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI).\n\n\nGPU\n\n\n3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking.\n\n\nGPGPU\n\n\n72 nodes, each with four NVIDIA P100 GPUs. See \nhere\n for more details.\n\n\nbigmem\n\n\n2 nodes, each with 36 cores and 1.5 TB of RAM. This partition is suited to memory-intensive single-node workloads.\n\n\nOther Partitions\n\n\nThere are also special partitions which are outside normal walltime constraints. In particular, \nshortcloud\n and \nshortgpgpu\n should be used for quick test cases; the partitions have a maximum time constraint of one hour.\n\n\nCiting Spartan\n\n\nIf you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.\n\n\nLev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa\n\n\nIf you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper:\n\n\nThis research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne.  This Facility was established with the assistance of LIEF Grant LE170100200.\n\n\nOther Resources\n\n\nSpartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions.\n\n\nNectar\n\n\nNectar\n is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories.\n\n\nSpartan runs some of it's computation resources in the Nectar cloud.\n\n\nMelbourne Bioinformatics\n\n\nMelbourne Bioinformatics\n run two large HPC systems for life sciences researchers.\n\n\nMulti-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE)\n \n\n\nMASSIVE\n is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.", 
            "title": "Home"
        }, 
        {
            "location": "/#spartan-daily-weather-report-20180914", 
            "text": "Utilisation of disk is at 31% from the Ceph pool.  Spartan is very busy on cloud partion, with close to 99% node allocation.  Spartan is busy on physical partition, with close to 90% node allocation.  Spartan is very busy on on GPGPU partition with 100% node allocation.  Many cloud nodes out (67), mainly due to qh2-uom migration.", 
            "title": "Spartan Daily Weather Report (20180914)"
        }, 
        {
            "location": "/#getting-help", 
            "text": "", 
            "title": "Getting Help"
        }, 
        {
            "location": "/#training", 
            "text": "We run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research.   Signup here:  http://melbourne.resbaz.edu.au/participate", 
            "title": "Training"
        }, 
        {
            "location": "/#helpdesk", 
            "text": "If you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at  hpc-support@unimelb.edu.au  Please submit one topic per ticket. If you require a assistance with a separate matter, compose a new ticket. Do not reply to existing or closed tickets.  For password resets please see  the FAQ  or contact University Services on +61 3 8344 0999 or ext 40999 or email  service-centre@unimelb.edu.au .", 
            "title": "Helpdesk"
        }, 
        {
            "location": "/#specifications", 
            "text": "Spartan has a number of partitions available for general usage. A full list of partitions can be viewed with the command  sinfo -s .  Cloud  208 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow.  Physical  20 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI).  GPU  3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking.  GPGPU  72 nodes, each with four NVIDIA P100 GPUs. See  here  for more details.  bigmem  2 nodes, each with 36 cores and 1.5 TB of RAM. This partition is suited to memory-intensive single-node workloads.  Other Partitions  There are also special partitions which are outside normal walltime constraints. In particular,  shortcloud  and  shortgpgpu  should be used for quick test cases; the partitions have a maximum time constraint of one hour.", 
            "title": "Specifications"
        }, 
        {
            "location": "/#citing-spartan", 
            "text": "If you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.  Lev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa  If you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper:  This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne.  This Facility was established with the assistance of LIEF Grant LE170100200.", 
            "title": "Citing Spartan"
        }, 
        {
            "location": "/#other-resources", 
            "text": "Spartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions.  Nectar  Nectar  is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories.  Spartan runs some of it's computation resources in the Nectar cloud.  Melbourne Bioinformatics  Melbourne Bioinformatics  run two large HPC systems for life sciences researchers.  Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE)    MASSIVE  is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.", 
            "title": "Other Resources"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Prerequisite:\n You'll need a basic understanding of the Linux command line to use Spartan. But don't worry, you don't need to be an expert, and there are many resources out there to help you. \nThis tutorial\n is a good place to start.\n\n\n1. Create an account\n\n\nGo to \nKaraage\n to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one.\n\n\n\n\n2. Login to Spartan via SSH\n\n\nNote that your password for Spartan is created during sign-up, and is different to your university password.\n\n\nWindows\n\n\nDownload an SSH client such as \nPuTTY\n, set hostname as \nspartan.hpc.unimelb.edu.au\n  and click Open. You'll be asked for your Spartan username and password.\n\n\nPosix (Linux, OS X)\n\n\nYou'll already have an SSH client installed. Easy! Open a terminal and enter:\n\n\n$ ssh yourUsername@spartan.hpc.unimelb.edu.au\n\n\n\n\n3. Create a job\n\n\nSpartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset.\n\n\nCopy the example into your home directory, and change working directory:\n\n\n$ cp -r /usr/local/common/Python ~/\n\n\n$ cd ~/Python\n\n\nThe dataset is in \nminitwitter.csv\n, and the analysis code in \ntwitter_search_541635.py\n. The files ending in \n.slurm\n tell Spartan how to run your job. For example, \ntwitter_one_node_eight_cores.slurm\n requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for).\n\n\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --time=0-12:00:00\n\n# Load required modules\nmodule load Python/3.5.2-intel-2016.u3\n\n# Launch multiple process python code\necho \nSearching for mentions\n\ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m\necho \nSearching for topics\n\ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t\necho \nSearching for the keyword 'jumping'\n\ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping\n\n\n\n\n\n\n3. Submit your job\n\n\nFirst off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node. \nPlease don't run jobs on the login node!\n\n\nInstead, use the scheduling tool \nSlurm\n, and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available.\n\n\nGo ahead and launch your job using \nsbatch\n:\n\n\n$ sbatch twitter_one_node_eight_cores.slurm\n\n Submitted batch job 27300\n\n\n\n\nWe can check how it's progressing using \nsqueue\n:\n\n\n$ squeue --job 27300\n\n            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             27300     cloud twitter_   perryd  R      10:48      1 spartan040\n\n\n\n\nWhen complete, an output file is created which logs the output from your job, for the above this has the filename \nslurm-27300.out\n.\n\n\nYou can also perform interactive work using the \nsinteractive\n command. This is handy for testing and debugging. This will allocate and log you in to a computing node.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#1-create-an-account", 
            "text": "Go to  Karaage  to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one.", 
            "title": "1. Create an account"
        }, 
        {
            "location": "/getting_started/#2-login-to-spartan-via-ssh", 
            "text": "Note that your password for Spartan is created during sign-up, and is different to your university password.  Windows  Download an SSH client such as  PuTTY , set hostname as  spartan.hpc.unimelb.edu.au   and click Open. You'll be asked for your Spartan username and password.  Posix (Linux, OS X)  You'll already have an SSH client installed. Easy! Open a terminal and enter:  $ ssh yourUsername@spartan.hpc.unimelb.edu.au", 
            "title": "2. Login to Spartan via SSH"
        }, 
        {
            "location": "/getting_started/#3-create-a-job", 
            "text": "Spartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset.  Copy the example into your home directory, and change working directory:  $ cp -r /usr/local/common/Python ~/  $ cd ~/Python  The dataset is in  minitwitter.csv , and the analysis code in  twitter_search_541635.py . The files ending in  .slurm  tell Spartan how to run your job. For example,  twitter_one_node_eight_cores.slurm  requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for).  #!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --time=0-12:00:00\n\n# Load required modules\nmodule load Python/3.5.2-intel-2016.u3\n\n# Launch multiple process python code\necho  Searching for mentions \ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m\necho  Searching for topics \ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t\necho  Searching for the keyword 'jumping' \ntime mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping", 
            "title": "3. Create a job"
        }, 
        {
            "location": "/getting_started/#3-submit-your-job", 
            "text": "First off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node.  Please don't run jobs on the login node!  Instead, use the scheduling tool  Slurm , and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available.  Go ahead and launch your job using  sbatch :  $ sbatch twitter_one_node_eight_cores.slurm  Submitted batch job 27300  We can check how it's progressing using  squeue :  $ squeue --job 27300             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             27300     cloud twitter_   perryd  R      10:48      1 spartan040  When complete, an output file is created which logs the output from your job, for the above this has the filename  slurm-27300.out .  You can also perform interactive work using the  sinteractive  command. This is handy for testing and debugging. This will allocate and log you in to a computing node.", 
            "title": "3. Submit your job"
        }, 
        {
            "location": "/managing_data/", 
            "text": "Chances are you need to run your HPC job against a dataset, perhaps quite a sizable one. There are a number of places to store data on Spartan while you're working with it, and ways to get data in and out.\n\n\nWhere to Store Your Data on Spartan\n\n\nProjects Directory\n\n\nYour projects directory is the best place to store research data while you're working on it. It's located at \n/data/projects/\nprojectid\n for older projects, or \n/data/cephfs/\nprojectid\n for newer ones.\n\n\nOthers in your project can access it, and 500 GB of storage is available per project. If you need more than this, \nget in touch\n and we'll try to find a solution. In general 1 TB of project storage is available upon request, and up to 10 TB is possible if needed. Project storage beyond 10 TB will generally require some sort of co-investment, but this may be waived in some circumstances, particularly for high-value shared datasets.\n\n\nTo increase your project storage space from more than 1TB up to 10TB please fill in a \nCompute Storage Extension Request\n form and send to the Head of Research Compute Services, \nDr. Bernard Meade\n.\n\n\nHome Directory\n\n\nYour home directory, i.e. \n/home/yourusername\n can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our \ngetting started\n tutorial or testing out new software.\n\n\nOthers in your project won't have access, and you're limited to 50GB of storage.\n\n\nScratch Space\n\n\nYou can store temporary working data while your job is running at \n/tmp\n. This will map to a directory on our fast scratch network storage specific to your job ID, and clean up once your job is complete. It's also possible to write directly to \n/scratch/\n, for instance if you would like to share your working files across multiple nodes. In this case it's your own responsibility to avoid collisions (i.e. two processes writing to the same file at the same time), and clean up afterwards.\n\n\nN.B.\n Note that home, project and scratch are all network-based storage that can be accessed by multiple nodes and processes at the same time. Take care that you don't inadvertently write to the same file from multiple jobs at the same time.\n\n\nStaging\n\n\nLocal disk is typically faster than shared disks. If you find that your read-writes are slow and you are making use of a lot of I/O you may need to stage your data. \n\n\nSpartan has \n/data\n for /home and /projects (large, slower), \n/scratch\n for temporary storage data (faster), and as local disk, \n/var/local/tmp\n (fastest, not shared). You may need to copy data between these locations. \n\n\nHow to Transfer Data In and Out of Spartan\n\n\nSecure Copy (scp)\n\n\nYou can use the \nscp\n command to move data from your local machine to Spartan. For example, to move \nmydata.dat\n from your current working directory to your project directory on Spartan:\n\n\n$ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n\n\nYou can transfer files from Spartan to your local machine by reversing the order of the arguments like so:\n\n\n$ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  local.dat\n\n\nFor Windows users, PuTTY provides an equivalent tool called \npscp\n. If you're data is located on a remote machine, you can SSH into that system first, and then use \nscp\n from that machine to transfer your data into Spartan.\n\n\nIf you'd prefer a GUI interface, you can use tools like \nFileZilla\n (cross-platform) or \nCyberDuck\n (OS X \n Windows).\n\n\nrsync\n\n\nRepeatedly transferring large files in and out of Spartan via \nscp\n can be tedious. A good alternative is \nrsync\n, which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for \nscp\n.\n\n\n$ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat\n\n\nNote that the first argument is the source, and the second is the destination which will be modified to match the source.\n\n\nNot for Long-Term Storage\n\n\nWhile it's often essential to have fast nearby storage while working on your data, please don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first). \n\n\nMediaflux Integration\n\n\nResearch Platform Services provides a data management service utilising the \nMediaflux platform\n. This platform provides a persistent location for research data and meta-data.\nTo aid integration between Mediaflux and Spartan, Java clients are available on Spartan, allowing data to be downloaded from and uploaded to Mediaflux. Details on Mediaflux integration with Spartan can be found in Section 4 of the \nMediaflux support wiki\n\n\nData and Storage Solutions Beyond Spartan\n\n\nThe University offers a range of other data storage and management solutions to meet your needs, beyond the short-term storage available on Spartan, which are described \nhere\n. \n\n\nIn some cases it's possible to integrate these resources with your account on Spartan to streamline your workflow. \nGet in touch\n if you'd like to find out more for your particular application.", 
            "title": "Managing Data"
        }, 
        {
            "location": "/managing_data/#where-to-store-your-data-on-spartan", 
            "text": "Projects Directory  Your projects directory is the best place to store research data while you're working on it. It's located at  /data/projects/ projectid  for older projects, or  /data/cephfs/ projectid  for newer ones.  Others in your project can access it, and 500 GB of storage is available per project. If you need more than this,  get in touch  and we'll try to find a solution. In general 1 TB of project storage is available upon request, and up to 10 TB is possible if needed. Project storage beyond 10 TB will generally require some sort of co-investment, but this may be waived in some circumstances, particularly for high-value shared datasets.  To increase your project storage space from more than 1TB up to 10TB please fill in a  Compute Storage Extension Request  form and send to the Head of Research Compute Services,  Dr. Bernard Meade .  Home Directory  Your home directory, i.e.  /home/yourusername  can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our  getting started  tutorial or testing out new software.  Others in your project won't have access, and you're limited to 50GB of storage.  Scratch Space  You can store temporary working data while your job is running at  /tmp . This will map to a directory on our fast scratch network storage specific to your job ID, and clean up once your job is complete. It's also possible to write directly to  /scratch/ , for instance if you would like to share your working files across multiple nodes. In this case it's your own responsibility to avoid collisions (i.e. two processes writing to the same file at the same time), and clean up afterwards.  N.B.  Note that home, project and scratch are all network-based storage that can be accessed by multiple nodes and processes at the same time. Take care that you don't inadvertently write to the same file from multiple jobs at the same time.  Staging  Local disk is typically faster than shared disks. If you find that your read-writes are slow and you are making use of a lot of I/O you may need to stage your data.   Spartan has  /data  for /home and /projects (large, slower),  /scratch  for temporary storage data (faster), and as local disk,  /var/local/tmp  (fastest, not shared). You may need to copy data between these locations.", 
            "title": "Where to Store Your Data on Spartan"
        }, 
        {
            "location": "/managing_data/#how-to-transfer-data-in-and-out-of-spartan", 
            "text": "Secure Copy (scp)  You can use the  scp  command to move data from your local machine to Spartan. For example, to move  mydata.dat  from your current working directory to your project directory on Spartan:  $ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  You can transfer files from Spartan to your local machine by reversing the order of the arguments like so:  $ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  local.dat  For Windows users, PuTTY provides an equivalent tool called  pscp . If you're data is located on a remote machine, you can SSH into that system first, and then use  scp  from that machine to transfer your data into Spartan.  If you'd prefer a GUI interface, you can use tools like  FileZilla  (cross-platform) or  CyberDuck  (OS X   Windows).  rsync  Repeatedly transferring large files in and out of Spartan via  scp  can be tedious. A good alternative is  rsync , which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for  scp .  $ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat  Note that the first argument is the source, and the second is the destination which will be modified to match the source.", 
            "title": "How to Transfer Data In and Out of Spartan"
        }, 
        {
            "location": "/managing_data/#not-for-long-term-storage", 
            "text": "While it's often essential to have fast nearby storage while working on your data, please don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first).", 
            "title": "Not for Long-Term Storage"
        }, 
        {
            "location": "/managing_data/#mediaflux-integration", 
            "text": "Research Platform Services provides a data management service utilising the  Mediaflux platform . This platform provides a persistent location for research data and meta-data.\nTo aid integration between Mediaflux and Spartan, Java clients are available on Spartan, allowing data to be downloaded from and uploaded to Mediaflux. Details on Mediaflux integration with Spartan can be found in Section 4 of the  Mediaflux support wiki", 
            "title": "Mediaflux Integration"
        }, 
        {
            "location": "/managing_data/#data-and-storage-solutions-beyond-spartan", 
            "text": "The University offers a range of other data storage and management solutions to meet your needs, beyond the short-term storage available on Spartan, which are described  here .   In some cases it's possible to integrate these resources with your account on Spartan to streamline your workflow.  Get in touch  if you'd like to find out more for your particular application.", 
            "title": "Data and Storage Solutions Beyond Spartan"
        }, 
        {
            "location": "/software/", 
            "text": "This page outlines usage and tips for some of the most popular software being used on Spartan. \n\n\nSpartan uses a modules system (\nlmod\n) to load and unload different packages, including different versions of the same software. This allows many different software packages to be installed on Spartan at once without interfering with each other\n\n\nYou can check what's currently installed using the \nmodule avail\n command, search using \nmodule spider\n, and load a particular module with the \nmodule load\n command. For example, to load MATLAB 2016a, use \nmodule load MATLAB/2016a\n.\n\n\nGenerally you shouldn't load modules from the login node, instead working on a compute node, either via an interactive session (launched with \nsinteractive\n), or from within your Slurm script.\n\n\nJob Script Examples\n\n\nWe maintain example job scripts for various software packages on Spartan. You can browse these at \n/usr/local/common\n or at \nhttps://github.com/resbaz/spartan-examples\n\n\nPython\n\n\nThere are multiple versions of Python installed on Spartan, which you can check using \nmodule spider Python\n. \n\n\nCommon packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using \npip install --user \npackage name\n. This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.\n\n\nR\n\n\nR version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.\n\n\nMATLAB\n\n\nMATLAB 2016a is installed on Spartan, along with all of the standard toolboxes. \n\n\nMATLAB can be invoked with a particular script using \nmatlab -nodisplay -nodesktop -r \"run my_script.m\"\n. You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.", 
            "title": "Software"
        }, 
        {
            "location": "/software/#job-script-examples", 
            "text": "We maintain example job scripts for various software packages on Spartan. You can browse these at  /usr/local/common  or at  https://github.com/resbaz/spartan-examples", 
            "title": "Job Script Examples"
        }, 
        {
            "location": "/software/#python", 
            "text": "There are multiple versions of Python installed on Spartan, which you can check using  module spider Python .   Common packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using  pip install --user  package name . This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.", 
            "title": "Python"
        }, 
        {
            "location": "/software/#r", 
            "text": "R version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.", 
            "title": "R"
        }, 
        {
            "location": "/software/#matlab", 
            "text": "MATLAB 2016a is installed on Spartan, along with all of the standard toolboxes.   MATLAB can be invoked with a particular script using  matlab -nodisplay -nodesktop -r \"run my_script.m\" . You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.", 
            "title": "MATLAB"
        }, 
        {
            "location": "/guides/", 
            "text": "Spartan Desktop\n\n\nSpartan Desktop allows you to run software with a GUI (e.g. MATLAB) from within Spartan.\n\n\nContainers\n\n\nYou can run your software on Spartan using containers, rather than via the \nmanaged modules system\n. This article explains when you might want to do this, and how.\n\n\nGPGPU\n\n\nDetails on our new GPGPU service for project partners.", 
            "title": "Guides"
        }, 
        {
            "location": "/guides/#spartan-desktop", 
            "text": "Spartan Desktop allows you to run software with a GUI (e.g. MATLAB) from within Spartan.", 
            "title": "Spartan Desktop"
        }, 
        {
            "location": "/guides/#containers", 
            "text": "You can run your software on Spartan using containers, rather than via the  managed modules system . This article explains when you might want to do this, and how.", 
            "title": "Containers"
        }, 
        {
            "location": "/guides/#gpgpu", 
            "text": "Details on our new GPGPU service for project partners.", 
            "title": "GPGPU"
        }, 
        {
            "location": "/guides/desktop/", 
            "text": "Beta\n Note that Spartan Desktop is currently in beta, and is not yet ready for general use. You're welcome to give it a try and provide feedback, but there may be bugs or missing features.\n\n\n\n\nWhile HPC systems like Spartan are oriented to command line batch processing, certain applications benefit from access to an interactive desktop environment. For example, you might like to quickly view some intermediate data, without having to download it to your own computer, and taking advantage of software already installed on Spartan.\n\n\nSpartan Desktop is based on Strudel, a cross-platform application developed by \nCVL/MASSIVE\n. It will create an interactive job on Spartan, launch a desktop session, and connect you via a secure SSH tunnel using TurboVNC.\n\n\nGetting Started\n\n\n\n\nInstall TurboVNC and Strudel as per the instructions at https://www.massive.org.au/userguide/cluster-instructions/strudel\n\n\n\n\nAdd Spartan as a site to Strudel. Go to 'File -\n Manage Sites', and click 'New'. Set the name field as \nSpartan Desktop\n and URL as \nhttp://dashboard.hpc.unimelb.edu.au/spartan-rcg.json\n. Click Ok, and make sure the site is set as active.\n\n\n\n\n\n\n\n\nNow to connect... select \nspartan-rcg\n from the site menu, and enter your Spartan username. Enable the advanced options, and set the SSH tunnel cipher as \naes128-cbc\n. \n\n\n\n\n\n\n\n\nClick Login. Strudel will ask for your Spartan password, start a desktop session, and connect to it via TurboVNC.\n\n\n\n\n\n\nUsage\n\n\nFrom your desktop session, you can load modules and start jobs just as you would from a command line session. For example, to start MATLAB:", 
            "title": "_Spartan Desktop"
        }, 
        {
            "location": "/guides/desktop/#getting-started", 
            "text": "Install TurboVNC and Strudel as per the instructions at https://www.massive.org.au/userguide/cluster-instructions/strudel   Add Spartan as a site to Strudel. Go to 'File -  Manage Sites', and click 'New'. Set the name field as  Spartan Desktop  and URL as  http://dashboard.hpc.unimelb.edu.au/spartan-rcg.json . Click Ok, and make sure the site is set as active.     Now to connect... select  spartan-rcg  from the site menu, and enter your Spartan username. Enable the advanced options, and set the SSH tunnel cipher as  aes128-cbc .      Click Login. Strudel will ask for your Spartan password, start a desktop session, and connect to it via TurboVNC.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/desktop/#usage", 
            "text": "From your desktop session, you can load modules and start jobs just as you would from a command line session. For example, to start MATLAB:", 
            "title": "Usage"
        }, 
        {
            "location": "/guides/containers/", 
            "text": "Container frameworks such as Docker allow your application, its dependencies and operating system to be encapsulated in a single file that can then be run in isolation from the host system. It's a bit like a light-weight virtual machine. This allows your software to be more portable, for example if you need to share your analysis environment with a colleague, but the dependencies are complex to install and keep consistent. \n\n\nIn general, we recommend that you \ncheck\n if the software you need is already installed directly on Spartan, and if it's not, or the wrong version, get in contact to see if it can be installed for you. By allowing us to manage software for you, we can make sure that it's optimised for our particular hardware, and is available for everyone. \n\n\nHowever, there are cases when it might be more practical to bring your own container instead.\n\n\nWhen should I consider a container?\n\n\nWhen your software:\n\n\n\n\nHas complex or out-of-date dependencies that aren't easily installed on Spartan.\n\n\nAssumes a particular operating system, e.g. requires Ubuntu, but Spartan runs on Red Hat.\n\n\nIs highly-modified or a legacy version that isn't likely to be of use to other researchers.\n\n\nIs being run across heterogeneous infrastructure (e.g. Spartan, your laptop, other HPC systems and/or cloud), and containers make it easier to maintain consistency.\n\n\n\n\nWhat are the downsides of containers?\n\n\n\n\nThe images themselves can be very large, consuming your storage quota, and being slow to transfer to/from Spartan.\n\n\nThe container might be optimized for a particular processor architecture, running slowly (or not at all) on systems that differ.\n\n\nTheir makeup and integrity can be opaque (although many common software packages will have officially supported container images).\n\n\n\n\nContainers on Spartan\n\n\nIt general Docker isn't appropriate for HPC environments like Spartan in which regular users don't have administrator (root) access. A good alternative is \nSingularity\n, a container framework targeted for research use, which many HPC centres support (including Spartan). Check out \nSingularity's documentation\n to learn more, in conjunction with below which is specific to Spartan.\n\n\nCreating a Container\n\n\nIn general, it's not possible to build a new container on Spartan. Instead, you would fetch an existing container (from Docker or Singularity Hub), build one on your own computer, or on a computer in the cloud. For the latter, the \nMelbourne Research Cloud\n is suitable, and often quite convenient as file transfer between Spartan and cloud instances are fast since they are in the same data centre.\n\n\nRunning a Container on Spartan\n\n\nYou can load Singularity using our modules system like so:\n\n\nmodule load Singularity/2.5.0-intel-2017.u2\n\n\nAn example job script is available at \n/usr/local/common/Singularity\n, which is also \nmirrored\n on GitHub.", 
            "title": "_Containers"
        }, 
        {
            "location": "/guides/containers/#when-should-i-consider-a-container", 
            "text": "When your software:   Has complex or out-of-date dependencies that aren't easily installed on Spartan.  Assumes a particular operating system, e.g. requires Ubuntu, but Spartan runs on Red Hat.  Is highly-modified or a legacy version that isn't likely to be of use to other researchers.  Is being run across heterogeneous infrastructure (e.g. Spartan, your laptop, other HPC systems and/or cloud), and containers make it easier to maintain consistency.", 
            "title": "When should I consider a container?"
        }, 
        {
            "location": "/guides/containers/#what-are-the-downsides-of-containers", 
            "text": "The images themselves can be very large, consuming your storage quota, and being slow to transfer to/from Spartan.  The container might be optimized for a particular processor architecture, running slowly (or not at all) on systems that differ.  Their makeup and integrity can be opaque (although many common software packages will have officially supported container images).", 
            "title": "What are the downsides of containers?"
        }, 
        {
            "location": "/guides/containers/#containers-on-spartan", 
            "text": "It general Docker isn't appropriate for HPC environments like Spartan in which regular users don't have administrator (root) access. A good alternative is  Singularity , a container framework targeted for research use, which many HPC centres support (including Spartan). Check out  Singularity's documentation  to learn more, in conjunction with below which is specific to Spartan.", 
            "title": "Containers on Spartan"
        }, 
        {
            "location": "/guides/containers/#creating-a-container", 
            "text": "In general, it's not possible to build a new container on Spartan. Instead, you would fetch an existing container (from Docker or Singularity Hub), build one on your own computer, or on a computer in the cloud. For the latter, the  Melbourne Research Cloud  is suitable, and often quite convenient as file transfer between Spartan and cloud instances are fast since they are in the same data centre.", 
            "title": "Creating a Container"
        }, 
        {
            "location": "/guides/containers/#running-a-container-on-spartan", 
            "text": "You can load Singularity using our modules system like so:  module load Singularity/2.5.0-intel-2017.u2  An example job script is available at  /usr/local/common/Singularity , which is also  mirrored  on GitHub.", 
            "title": "Running a Container on Spartan"
        }, 
        {
            "location": "/faq/", 
            "text": "What's special about Spartan?\n\n\nMost modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed.\n\n\nFor certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of \nembarrassingly parallel\n. That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance.\n\n\nSpartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.\n\n\nHow do I get an account?\n\n\nAccess to Spartan requires an an account, which you can request using \nKaraage\n. \n\n\nAccounts are associated with a particular project; you can either join an existing project or create a new one.\n\n\nNew projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.\n\n\nHow do I access Spartan once I have an account?\n\n\nYou'll need an SSH client. Mac and Linux computers will already have one installed, just use the command \nssh yourUsername@spartan.hpc.unimelb.edu.au\n at your terminal.\n\n\nFor Windows, you'll need to download an SSH client such as \nPuTTY\n, set hostname as \nspartan.hpc.unimelb.edu.au\n and select Open. You'll be asked for your Spartan username and password.\n\n\nMy password isn't working!\n\n\n\n\n\n\nMake sure you're using your Spartan password that you set in \nKaraage\n. \n Your Spartan password is not necessarily the same as your central university password.\n\n\n\n\n\n\nYou can request a password reset \nhere\n.\n\n\n\n\n\n\nIf you are still having trouble, contact the University of Melbourne Service Desk on +61 3 8344 0999 or ext 40999 or email or \nservice-centre@unimelb.edu.au\n.\n\n\n\n\n\n\nHow do I add people to a project?\n\n\nIf you are a project leader you may invite people to join your project. Login to Karaage, and go to your \nKaraage project list\n, select the appropriate project, and select the \"Invite a new user\" option. The user will then receive an invitation link to join the project and set up an account. \n\n\nHowever\n if the belong to an institution that does not have a SAML login process (e.g., international researchers) it is worthwhile contacting the Spartan at \nhpc-support@unimelb.edu.au\n. Then the sysadmins will add the person manually to the project and reset their password.\n\n\nWhat are Spartan's specifications?\n\n\nSpartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us.\n\n\nThe cloud partition nominally consists of 100 nodes with 8 cores each from the \nNectar\n research cloud, however it is capable of accessing more as the load on Spartan grows.\n\n\nThere also exist a number of specialist nodes with expanded memory or \nGPGPU\n hardware, as well as partitions dedicated to particular departments and research groups.\n\n\nWhat software is installed?\n\n\nSpartan uses a modules system (\nlmod\n) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the \nmodule avail\n command, and load a module with the \nmodule load\n command.\n\n\nTypically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with \nsinteractive\n). Instead you load the modules in your Slurm script before executing your particular software.\n\n\nWhat if the software I need is not installed?\n\n\nGet in contact\n with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that:\n\n\n\n\nIt works\n\n\nSoftware licenses are managed\n\n\nCode is compiled with the appropriate flags to maximize performance\n\n\nOthers users can also make use of the software.\n\n\n\n\nWhere do I go for help?\n\n\nFirst off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check \nhere\n for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at: \nhttp://melbourne.resbaz.edu.au/participate\n\n\nSecond, check the documentation here, as well as for the software you're running on Spartan (like Slurm).\n\n\nFinally, if you ever get stuck, please feel free to \nemail HPC support\n. We're here to help make your research more productive and enjoyable, and we'll do everything we can to help. \n\n\nHow do I get access to GPUs?\n\n\nSpartan includes two partitions with GPUs (as well as a third private \nphysics-gpu\n partition). \n\n\nThe legacy \ngpu\n partition includes four Nvidia K80 GPUs per node, while the newer \ngpgpu\n partition includes four Nvidia P100 GPUs per node. \n\n\nThey can be specified in your job script with \n#SBATCH --partition gpu\n and \n#SBATCH --partition gpgpu\n, respectively.\n\n\nYou'll also need to include a generic resource request in your job script, for example \n#SBATCH --gres=gpu:2\n will request two GPUs for your job.\n\n\nA range of GPU-accelerated software such as TensorFlow is available on Spartan \nexample\n, as well as CUDA for developing your own GPU applications \nexample\n.\n\n\nN.B. The GPGPU partition is not automatically available to all Spartan users, and a dedicated project must be created to request access. See \nhere\n for more details.\n\n\nHow do I submit a job?\n\n\nYou'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out \nGetting Started\n for an example.\n\n\nDo I need to know how to use Linux?\n\n\nJust the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as \nthis tutorial\n. \n\n\nHow do I create a multi-core job?\n\n\nThere are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this:\n\n\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n\n\n\n\nThis is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores.\n\n\nAlternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core:\n\n\n#SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n\n\n\n\nThis approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick.\n\n\nKeep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.\n\n\nHow do I create a multi-node job?\n\n\nHere's an example of a job with two nodes, each using 12 cores.\n\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=12\n\n\n\n\nNote that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI.\n\n\nFor multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes. \n\n\nWhat other options are there for running my job?\n\n\nMany different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for \nSlurm\n (the job manager we use on Spartan) for details.\n\n\nHow do I create a job array?\n\n\nJob arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other.\n\n\nSay we have an array of files, \ndata_1.dat\n to \ndata_50.dat\n to process with \nmyProgram\n:\n\n\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --time=0-00:15:00\n#SBATCH --array=1-50\n\nmyProgram data_${SLURM_ARRAY_TASK_ID}.dat\n\n\n\n\nThis will create 50 jobs, each calling \nmyProgram\n with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)\n\n\nHow do I request more memory?\n\n\nBy default the scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. In some cases this might not be enough (e.g., very large dataset that needs to be loaded with low level of parallelisation).\n\n\nAdditional memory can be allocated with the \n--mem=[mem][M|G|T]\n directive (entire job) or \n--mem-per-cpu=[mem][M|G|T]\n (per core). Maximum should be based around total cores -1 (for system processes). The --mem-per-cpu directive is for threads for OpenMP applications and processor ranks for MPI.\n\n\nIt is best to reserve some memory (about 1 core's worth) for system processes.\n\n\nAre there more examples I can look at?\n\n\nIf you go to \n/usr/local/common/\n on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.\n\n\nHow do I make my program run fast on Spartan?\n\n\nSpartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, so improved performance is dependent on using parallel processing (MPI or OpenMP) or job arrays. \n\n\nHow do I cite Spartan in my publications?\n\n\nIf you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.\n\n\nLev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa\n\n\nHow do I transition my work from the old HPC system Edward to Spartan?\n\n\nHere's a \nguide\n to help you.\n\n\nHow do setup passwordless SSH login?\n\n\nA passwordless SSH for Spartan will make your life easier. You won't\neven need to remember your password!\n\n\nIf you have a *nix system (e.g., UNIX, Linux, MacOS X) open up a\nterminal on your \nlocal\n system and generate a keypair.\n\n\n$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nCreated directory '/home/user/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\n43:51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 user@localhost\n\n\n\n\nNow append the new public key to \n~/.ssh/authorized_keys\n on Spartan (you'll be asked for your password one last time).\n\n\n$ cat .ssh/id_rsa.pub | ssh username@spartan.hpc.unimelb.edu.au 'cat \n .ssh/authorized_keys'\n\n\n\n\nDepending on your version of SSH you might also have to do the following\nchanges:\n\n\n\n\nPut the public key in .ssh/authorized_keys2\n\n\nChange the permissions of .ssh to 700\n\n\nChange the permissions of .ssh/authorized_keys2 to 640\n\n\n\n\nYou can now SSH to Spartan without having to enter your password!\n\n\nHow can I avoid typing myUsername@spartan.hpc.unimelb.edu.au everytime I connect?\n\n\nAn SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname. \n\n\nCreate the text file in your \n~/.ssh\n directory with your preferred text editor, for example, \nnano\n.\n\n\nnano .ssh/config\n\n\n\n\nEnter the following (replacing \nusername\n with your actual username of course!):\n\n\nHost *\nServerAliveInterval 120\nHost spartan\n       Hostname spartan.hpc.unimelb.edu.au\n       User username\n\n\n\n\nNow to connect to Spartan, you need only type \nssh spartan\n.", 
            "title": "FAQ"
        }, 
        {
            "location": "/faq/#whats-special-about-spartan", 
            "text": "Most modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed.  For certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of  embarrassingly parallel . That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance.  Spartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.", 
            "title": "What's special about Spartan?"
        }, 
        {
            "location": "/faq/#how-do-i-get-an-account", 
            "text": "Access to Spartan requires an an account, which you can request using  Karaage .   Accounts are associated with a particular project; you can either join an existing project or create a new one.  New projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.", 
            "title": "How do I get an account?"
        }, 
        {
            "location": "/faq/#how-do-i-access-spartan-once-i-have-an-account", 
            "text": "You'll need an SSH client. Mac and Linux computers will already have one installed, just use the command  ssh yourUsername@spartan.hpc.unimelb.edu.au  at your terminal.  For Windows, you'll need to download an SSH client such as  PuTTY , set hostname as  spartan.hpc.unimelb.edu.au  and select Open. You'll be asked for your Spartan username and password.", 
            "title": "How do I access Spartan once I have an account?"
        }, 
        {
            "location": "/faq/#my-password-isnt-working", 
            "text": "Make sure you're using your Spartan password that you set in  Karaage .   Your Spartan password is not necessarily the same as your central university password.    You can request a password reset  here .    If you are still having trouble, contact the University of Melbourne Service Desk on +61 3 8344 0999 or ext 40999 or email or  service-centre@unimelb.edu.au .", 
            "title": "My password isn't working!"
        }, 
        {
            "location": "/faq/#how-do-i-add-people-to-a-project", 
            "text": "If you are a project leader you may invite people to join your project. Login to Karaage, and go to your  Karaage project list , select the appropriate project, and select the \"Invite a new user\" option. The user will then receive an invitation link to join the project and set up an account.   However  if the belong to an institution that does not have a SAML login process (e.g., international researchers) it is worthwhile contacting the Spartan at  hpc-support@unimelb.edu.au . Then the sysadmins will add the person manually to the project and reset their password.", 
            "title": "How do I add people to a project?"
        }, 
        {
            "location": "/faq/#what-are-spartans-specifications", 
            "text": "Spartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us.  The cloud partition nominally consists of 100 nodes with 8 cores each from the  Nectar  research cloud, however it is capable of accessing more as the load on Spartan grows.  There also exist a number of specialist nodes with expanded memory or  GPGPU  hardware, as well as partitions dedicated to particular departments and research groups.", 
            "title": "What are Spartan's specifications?"
        }, 
        {
            "location": "/faq/#what-software-is-installed", 
            "text": "Spartan uses a modules system ( lmod ) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the  module avail  command, and load a module with the  module load  command.  Typically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with  sinteractive ). Instead you load the modules in your Slurm script before executing your particular software.", 
            "title": "What software is installed?"
        }, 
        {
            "location": "/faq/#what-if-the-software-i-need-is-not-installed", 
            "text": "Get in contact  with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that:   It works  Software licenses are managed  Code is compiled with the appropriate flags to maximize performance  Others users can also make use of the software.", 
            "title": "What if the software I need is not installed?"
        }, 
        {
            "location": "/faq/#where-do-i-go-for-help", 
            "text": "First off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check  here  for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at:  http://melbourne.resbaz.edu.au/participate  Second, check the documentation here, as well as for the software you're running on Spartan (like Slurm).  Finally, if you ever get stuck, please feel free to  email HPC support . We're here to help make your research more productive and enjoyable, and we'll do everything we can to help.", 
            "title": "Where do I go for help?"
        }, 
        {
            "location": "/faq/#how-do-i-get-access-to-gpus", 
            "text": "Spartan includes two partitions with GPUs (as well as a third private  physics-gpu  partition).   The legacy  gpu  partition includes four Nvidia K80 GPUs per node, while the newer  gpgpu  partition includes four Nvidia P100 GPUs per node.   They can be specified in your job script with  #SBATCH --partition gpu  and  #SBATCH --partition gpgpu , respectively.  You'll also need to include a generic resource request in your job script, for example  #SBATCH --gres=gpu:2  will request two GPUs for your job.  A range of GPU-accelerated software such as TensorFlow is available on Spartan  example , as well as CUDA for developing your own GPU applications  example .  N.B. The GPGPU partition is not automatically available to all Spartan users, and a dedicated project must be created to request access. See  here  for more details.", 
            "title": "How do I get access to GPUs?"
        }, 
        {
            "location": "/faq/#how-do-i-submit-a-job", 
            "text": "You'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out  Getting Started  for an example.", 
            "title": "How do I submit a job?"
        }, 
        {
            "location": "/faq/#do-i-need-to-know-how-to-use-linux", 
            "text": "Just the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as  this tutorial .", 
            "title": "Do I need to know how to use Linux?"
        }, 
        {
            "location": "/faq/#how-do-i-create-a-multi-core-job", 
            "text": "There are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this:  #SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8  This is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores.  Alternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core:  #SBATCH --nodes=1\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1  This approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick.  Keep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.", 
            "title": "How do I create a multi-core job?"
        }, 
        {
            "location": "/faq/#how-do-i-create-a-multi-node-job", 
            "text": "Here's an example of a job with two nodes, each using 12 cores.  #SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=12  Note that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI.  For multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes.", 
            "title": "How do I create a multi-node job?"
        }, 
        {
            "location": "/faq/#what-other-options-are-there-for-running-my-job", 
            "text": "Many different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for  Slurm  (the job manager we use on Spartan) for details.", 
            "title": "What other options are there for running my job?"
        }, 
        {
            "location": "/faq/#how-do-i-create-a-job-array", 
            "text": "Job arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other.  Say we have an array of files,  data_1.dat  to  data_50.dat  to process with  myProgram :  #!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --time=0-00:15:00\n#SBATCH --array=1-50\n\nmyProgram data_${SLURM_ARRAY_TASK_ID}.dat  This will create 50 jobs, each calling  myProgram  with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)", 
            "title": "How do I create a job array?"
        }, 
        {
            "location": "/faq/#how-do-i-request-more-memory", 
            "text": "By default the scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. In some cases this might not be enough (e.g., very large dataset that needs to be loaded with low level of parallelisation).  Additional memory can be allocated with the  --mem=[mem][M|G|T]  directive (entire job) or  --mem-per-cpu=[mem][M|G|T]  (per core). Maximum should be based around total cores -1 (for system processes). The --mem-per-cpu directive is for threads for OpenMP applications and processor ranks for MPI.  It is best to reserve some memory (about 1 core's worth) for system processes.", 
            "title": "How do I request more memory?"
        }, 
        {
            "location": "/faq/#are-there-more-examples-i-can-look-at", 
            "text": "If you go to  /usr/local/common/  on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.", 
            "title": "Are there more examples I can look at?"
        }, 
        {
            "location": "/faq/#how-do-i-make-my-program-run-fast-on-spartan", 
            "text": "Spartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, so improved performance is dependent on using parallel processing (MPI or OpenMP) or job arrays.", 
            "title": "How do I make my program run fast on Spartan?"
        }, 
        {
            "location": "/faq/#how-do-i-cite-spartan-in-my-publications", 
            "text": "If you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support.  Lev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa", 
            "title": "How do I cite Spartan in my publications?"
        }, 
        {
            "location": "/faq/#how-do-i-transition-my-work-from-the-old-hpc-system-edward-to-spartan", 
            "text": "Here's a  guide  to help you.", 
            "title": "How do I transition my work from the old HPC system Edward to Spartan?"
        }, 
        {
            "location": "/faq/#how-do-setup-passwordless-ssh-login", 
            "text": "A passwordless SSH for Spartan will make your life easier. You won't\neven need to remember your password!  If you have a *nix system (e.g., UNIX, Linux, MacOS X) open up a\nterminal on your  local  system and generate a keypair.  $ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nCreated directory '/home/user/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/user/.ssh/id_rsa.\nYour public key has been saved in /home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\n43:51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 user@localhost  Now append the new public key to  ~/.ssh/authorized_keys  on Spartan (you'll be asked for your password one last time).  $ cat .ssh/id_rsa.pub | ssh username@spartan.hpc.unimelb.edu.au 'cat   .ssh/authorized_keys'  Depending on your version of SSH you might also have to do the following\nchanges:   Put the public key in .ssh/authorized_keys2  Change the permissions of .ssh to 700  Change the permissions of .ssh/authorized_keys2 to 640   You can now SSH to Spartan without having to enter your password!", 
            "title": "How do setup passwordless SSH login?"
        }, 
        {
            "location": "/faq/#how-can-i-avoid-typing-myusernamespartanhpcunimelbeduau-everytime-i-connect", 
            "text": "An SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname.   Create the text file in your  ~/.ssh  directory with your preferred text editor, for example,  nano .  nano .ssh/config  Enter the following (replacing  username  with your actual username of course!):  Host *\nServerAliveInterval 120\nHost spartan\n       Hostname spartan.hpc.unimelb.edu.au\n       User username  Now to connect to Spartan, you need only type  ssh spartan .", 
            "title": "How can I avoid typing myUsername@spartan.hpc.unimelb.edu.au everytime I connect?"
        }, 
        {
            "location": "/edward_transition/", 
            "text": "Edward to Spartan : A Short Transition Guide\n\n\nWhen is Edward Being Retired?\n\n\nEdward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects.\n\n\nEdward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.\n\n\nHow is Spartan Different?\n\n\nSpartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition.\n\n\nSpartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply.\n\n\nHowever, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling.\n\n\nThis means that job scripts will be different between the two systems and translation will be required.\n\n\nHow do we submit jobs in SLURM?\n\n\nJob submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system.\n\n\nCore command for checking queue is: showq (TORQUE) or squeue (SLURM)\n\n\nCore command for job submission is qsub\n [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)\n\n\nWhat About Working Directories and Environments?\n\n\nTORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this.\n\n\nTORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.\n\n\nWhat about Job Status and Output?\n\n\nCore command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM)\n\n\nBoth TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the \n-j oe\n option in PBS).\n\n\nWhat are the user command differences?\n\n\n\n\n\n\n\n\nUser Command\n\n\nTORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nJob submission\n\n\nqsub [script_file]\n\n\nsbatch [script_file]\n\n\n\n\n\n\nJob delete\n\n\nqdel [job_id]\n\n\nscancel [job_id]\n\n\n\n\n\n\nJob status\n\n\nqstat [job_id]\n\n\nsqueue [job_id]\n\n\n\n\n\n\nJob status\n\n\nqstat -u [user_name]\n\n\nsqueue -u [user_name]\n\n\n\n\n\n\nNode list\n\n\npbsnodes -a\n\n\nsinfo -N\n\n\n\n\n\n\nQueue list\n\n\nqstat -Q\n\n\nsqueue\n\n\n\n\n\n\nCluster status\n\n\nshowq\n\n\nqstatus -a\n\n\n\n\n\n\n\n\nWhat are the job command differences?\n\n\n\n\n\n\n\n\nJob Specification\n\n\nTORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nScript directive\n\n\n#PBS\n\n\n#SBATCH\n\n\n\n\n\n\nQueue\n\n\n-q [queue]\n\n\n-p [queue]\n\n\n\n\n\n\nJob Name\n\n\n-N [name]\n\n\n--job-name=[name]\n\n\n\n\n\n\nNodes\n\n\n-l nodes=[count]\n\n\n-N [min[-max]]\n\n\n\n\n\n\nCPU Count\n\n\n-l ppn=[count]\n\n\n-n [count]\n\n\n\n\n\n\nWall Clock Limit\n\n\n-l walltime=[hh:mm:ss]\n\n\n-t [days-hh:mm:ss]\n\n\n\n\n\n\nEvent Address\n\n\n-M [address]\n\n\n--mail-user=[address]\n\n\n\n\n\n\nEvent Notification\n\n\n-m abe\n\n\n--mail-type=[events]\n\n\n\n\n\n\nMemory Size\n\n\n-l mem=[MB]\n\n\n--mem=[mem][M G T]\n\n\n\n\n\n\nProc Memory Size\n\n\n-l pmem=[MB]\n\n\n--mem-per-cpu=[mem][M G T]\n\n\n\n\n\n\n\n\nWhat are the environment commands differences?\n\n\n\n\n\n\n\n\nEnvironment\n\n\nCommand TORQUE (Edward)\n\n\nSLURM (Spartan)\n\n\n\n\n\n\n\n\n\n\nJob ID\n\n\n$PBS_JOBID\n\n\n$SLURM_JOBID\n\n\n\n\n\n\nSubmit Directory\n\n\n$PBS_O_WORKDIR\n\n\n$SLURM_SUBMIT_DIR\n\n\n\n\n\n\nSubmit Host\n\n\n$PBS_O_HOST\n\n\n$SLURM_SUBMIT_HOST\n\n\n\n\n\n\nNode List\n\n\n$PBS_NODEFILE\n\n\n$SLURM_JOB_NODELIST\n\n\n\n\n\n\nJob Array\n\n\nIndex $PBS_ARRAYID\n\n\n$SLURM_ARRAY_TASK_ID\n\n\n\n\n\n\n\n\nAutomation and Acknowledgements\n\n\nThere is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm\n\n\nThis guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016", 
            "title": "_Edward Transition"
        }, 
        {
            "location": "/edward_transition/#edward-to-spartan-a-short-transition-guide", 
            "text": "", 
            "title": "Edward to Spartan : A Short Transition Guide"
        }, 
        {
            "location": "/edward_transition/#when-is-edward-being-retired", 
            "text": "Edward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects.  Edward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.", 
            "title": "When is Edward Being Retired?"
        }, 
        {
            "location": "/edward_transition/#how-is-spartan-different", 
            "text": "Spartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition.  Spartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply.  However, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling.  This means that job scripts will be different between the two systems and translation will be required.", 
            "title": "How is Spartan Different?"
        }, 
        {
            "location": "/edward_transition/#how-do-we-submit-jobs-in-slurm", 
            "text": "Job submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system.  Core command for checking queue is: showq (TORQUE) or squeue (SLURM)  Core command for job submission is qsub  [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)", 
            "title": "How do we submit jobs in SLURM?"
        }, 
        {
            "location": "/edward_transition/#what-about-working-directories-and-environments", 
            "text": "TORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this.  TORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.", 
            "title": "What About Working Directories and Environments?"
        }, 
        {
            "location": "/edward_transition/#what-about-job-status-and-output", 
            "text": "Core command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM)  Both TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the  -j oe  option in PBS).", 
            "title": "What about Job Status and Output?"
        }, 
        {
            "location": "/edward_transition/#what-are-the-user-command-differences", 
            "text": "User Command  TORQUE (Edward)  SLURM (Spartan)      Job submission  qsub [script_file]  sbatch [script_file]    Job delete  qdel [job_id]  scancel [job_id]    Job status  qstat [job_id]  squeue [job_id]    Job status  qstat -u [user_name]  squeue -u [user_name]    Node list  pbsnodes -a  sinfo -N    Queue list  qstat -Q  squeue    Cluster status  showq  qstatus -a", 
            "title": "What are the user command differences?"
        }, 
        {
            "location": "/edward_transition/#what-are-the-job-command-differences", 
            "text": "Job Specification  TORQUE (Edward)  SLURM (Spartan)      Script directive  #PBS  #SBATCH    Queue  -q [queue]  -p [queue]    Job Name  -N [name]  --job-name=[name]    Nodes  -l nodes=[count]  -N [min[-max]]    CPU Count  -l ppn=[count]  -n [count]    Wall Clock Limit  -l walltime=[hh:mm:ss]  -t [days-hh:mm:ss]    Event Address  -M [address]  --mail-user=[address]    Event Notification  -m abe  --mail-type=[events]    Memory Size  -l mem=[MB]  --mem=[mem][M G T]    Proc Memory Size  -l pmem=[MB]  --mem-per-cpu=[mem][M G T]", 
            "title": "What are the job command differences?"
        }, 
        {
            "location": "/edward_transition/#what-are-the-environment-commands-differences", 
            "text": "Environment  Command TORQUE (Edward)  SLURM (Spartan)      Job ID  $PBS_JOBID  $SLURM_JOBID    Submit Directory  $PBS_O_WORKDIR  $SLURM_SUBMIT_DIR    Submit Host  $PBS_O_HOST  $SLURM_SUBMIT_HOST    Node List  $PBS_NODEFILE  $SLURM_JOB_NODELIST    Job Array  Index $PBS_ARRAYID  $SLURM_ARRAY_TASK_ID", 
            "title": "What are the environment commands differences?"
        }, 
        {
            "location": "/edward_transition/#automation-and-acknowledgements", 
            "text": "There is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm  This guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016", 
            "title": "Automation and Acknowledgements"
        }, 
        {
            "location": "/gpu/", 
            "text": "Spartan hosts a GPGPU service, developed in conjunction with Research Platform Services, the Melbourne School of Engineering, Melbourne Bioinformatics, RMIT, La Trobe University, St Vincent's Institute of Medical\nResearch and Deakin University. It was funded through \nARC LIEF\n grant LE170100200.\n\n\nIt consists of 72 nodes, each with four NVIDIA P100 graphics cards, which can provide a theoretical maximum of around 900 teraflops.  These nodes will be presented in 3 subclusters and will be released in stages.  The first subcluster consisting of 28 nodes is available immediately.  The remaining subclusters will come online as demand increases.\n\n\nAccess\n\n\nThe GPGPU cluster is available to University researchers, as well as external institutions that partnered through the ARC LIEF grant.\n\n\nThose with existing Spartan accounts will need to create a new project dedicated to GPGPU use. Data can still be transferred from existing projects, but an independent GPGPU project is required for accounting purposes. \n\n\nThose new to Spartan can signup for a user account at \nhttps://dashboard.hpc.unimelb.edu.au/karaage/\n, as well as request a new project (or access to an existing one).\n\n\nUsage\n\n\nThe GPGPU cluster operates as per other Spartan partitions, and so the documentation on this page can be followed with respect to logging in, accessing software, and creating job scripts.\n\n\nTo request one or more GPGPUs in your job script, see: \nhttps://dashboard.hpc.unimelb.edu.au/faq/#how-do-i-get-access-to-gpus\n\n\nCitation\n\n\nIf you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper:\n\n\nThis research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne.  This Facility was established with the assistance of LIEF Grant LE170100200.", 
            "title": "_GPGPU"
        }, 
        {
            "location": "/gpu/#access", 
            "text": "The GPGPU cluster is available to University researchers, as well as external institutions that partnered through the ARC LIEF grant.  Those with existing Spartan accounts will need to create a new project dedicated to GPGPU use. Data can still be transferred from existing projects, but an independent GPGPU project is required for accounting purposes.   Those new to Spartan can signup for a user account at  https://dashboard.hpc.unimelb.edu.au/karaage/ , as well as request a new project (or access to an existing one).", 
            "title": "Access"
        }, 
        {
            "location": "/gpu/#usage", 
            "text": "The GPGPU cluster operates as per other Spartan partitions, and so the documentation on this page can be followed with respect to logging in, accessing software, and creating job scripts.  To request one or more GPGPUs in your job script, see:  https://dashboard.hpc.unimelb.edu.au/faq/#how-do-i-get-access-to-gpus", 
            "title": "Usage"
        }, 
        {
            "location": "/gpu/#citation", 
            "text": "If you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper:  This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne.  This Facility was established with the assistance of LIEF Grant LE170100200.", 
            "title": "Citation"
        }, 
        {
            "location": "/access_policy/", 
            "text": "Access and Service Denial Policy\n\n\nAccess Policy\n\n\nAccess to the HPC Services (Spartan), is broadly governed by the access policies for the research cloud and the \nUniversity of Melbourne IT policies, standards and guidelines\n.  Additional requirements are as follows:\n\n\n\n\nPrincipal Investigator\nAll projects are to be led by a University of Melbourne researcher (including students undertaking research as part of postgraduate studies) or researchers from institutions that have purchased access to the service.  The project may be initiated and managed by a non-researcher in support of a researcher or research group.\n\n\nResearch collaborator\nParticipants in a project can be researchers or research support staff from anywhere.  Any collaborators must abide by the policies governing the use of IT at the University of Melbourne, and any additional requirements as specified in either the Research Cloud End User Agreement and this document.\n\n\nResearch support projects\nWhere necessary, support projects may be established for the purpose of supporting the HPC Service, including training.\n\n\nProjects are subject to approval\nAll projects are subject to approval by the Head of Research Compute Services and have been deemed to meet the policies and guidelines mentioned above.  Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity.\n\n\nCo-investment resources\nExceptions to 1..4 above may occur where a faulty, department or research group purchases dedicated resources for their exclusive or priority use.  Access to these resources will be at the discretion of the co-investment party, however, usage of these resources remain subject to the University's IT policies, standards and guidelines.\n\n\nProject information required\nAll projects should provide the following:\n\n\nPrincipal Investigator (PI) name and position\n\n\nPI contact phone number, email address\n\n\nProject requestor's name and position (if different from PI)\n\n\nProject requestor's phone number, email address (if different from PI)\n\n\nPI faculty, department\n\n\nProject Field of Research primary code, and additional FOR codes.\n\n\nProject expiration (default is 12 months)\n\n\nExpected usage e.g.\n\n\nSoftware (Matlab, GAMESS, own code, etc.)\n\n\nHardware (Bare-metal, GPU, co-investment partition, etc.)\n\n\n\n\n\n\nSpecial requirements for the project\n\n\n\n\n\n\n\n\nService Denial\n\n\nUnder certain circumstances, access may be restricted or terminated.  These include:\n\n\n\n\nInappropriate use of the service\nIf a user engages in activity that contravenes the University IT policies, standards and guidelines, or the processes described here, their access to the service will be suspended, pending a review.  In the case where the user is also the project owner, the project may also be suspended, effectively denying access to other project participants.  If the activity is deemed to be malicious, the user and the project may be removed from the service.\n\n\nDamage to the service\nIf a user misuses the service, either wittingly or unwittingly, and causes the degrading or denial of service for other users, access will be suspended pending a review.  The user will be notified of the problem, and subject to the discretion of the support staff, their access will be reinstated.  Should the problem reoccur, the user account will be suspended again and the user will be contacted by support staff.  Upon review by support staff and approval of the Service Owner, the user's access will be either:\n\n\nReinstated if the review concludes that the user no longer presents a threat to the system\n\n\nRemoved if the review concludes the user is unable to correct the problem, and effectively demonstrates Inappropriate use of the service (see above).\n\n\n\n\n\n\n\n\nExpiration of the Project\n\n\nAll projects are required to include an expected expiry date (not more than 12 months).  Project owners and users will be notified in advance of the expiration of the project and given to opportunity to extend the project by 12 months as required.  Project owners may elect to allow the project to expire, and are required to ensure they have secured any data they wish to preserve.  If no request to extend the project is made, it will be assumed that the project may expire and will be removed after the expiration date.  The removal of the project will include the deletion of all content in the project directory.", 
            "title": "_Access and Service Denial Policy"
        }, 
        {
            "location": "/access_policy/#access-and-service-denial-policy", 
            "text": "", 
            "title": "Access and Service Denial Policy"
        }, 
        {
            "location": "/access_policy/#access-policy", 
            "text": "Access to the HPC Services (Spartan), is broadly governed by the access policies for the research cloud and the  University of Melbourne IT policies, standards and guidelines .  Additional requirements are as follows:   Principal Investigator\nAll projects are to be led by a University of Melbourne researcher (including students undertaking research as part of postgraduate studies) or researchers from institutions that have purchased access to the service.  The project may be initiated and managed by a non-researcher in support of a researcher or research group.  Research collaborator\nParticipants in a project can be researchers or research support staff from anywhere.  Any collaborators must abide by the policies governing the use of IT at the University of Melbourne, and any additional requirements as specified in either the Research Cloud End User Agreement and this document.  Research support projects\nWhere necessary, support projects may be established for the purpose of supporting the HPC Service, including training.  Projects are subject to approval\nAll projects are subject to approval by the Head of Research Compute Services and have been deemed to meet the policies and guidelines mentioned above.  Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity.  Co-investment resources\nExceptions to 1..4 above may occur where a faulty, department or research group purchases dedicated resources for their exclusive or priority use.  Access to these resources will be at the discretion of the co-investment party, however, usage of these resources remain subject to the University's IT policies, standards and guidelines.  Project information required\nAll projects should provide the following:  Principal Investigator (PI) name and position  PI contact phone number, email address  Project requestor's name and position (if different from PI)  Project requestor's phone number, email address (if different from PI)  PI faculty, department  Project Field of Research primary code, and additional FOR codes.  Project expiration (default is 12 months)  Expected usage e.g.  Software (Matlab, GAMESS, own code, etc.)  Hardware (Bare-metal, GPU, co-investment partition, etc.)    Special requirements for the project", 
            "title": "Access Policy"
        }, 
        {
            "location": "/access_policy/#service-denial", 
            "text": "Under certain circumstances, access may be restricted or terminated.  These include:   Inappropriate use of the service\nIf a user engages in activity that contravenes the University IT policies, standards and guidelines, or the processes described here, their access to the service will be suspended, pending a review.  In the case where the user is also the project owner, the project may also be suspended, effectively denying access to other project participants.  If the activity is deemed to be malicious, the user and the project may be removed from the service.  Damage to the service\nIf a user misuses the service, either wittingly or unwittingly, and causes the degrading or denial of service for other users, access will be suspended pending a review.  The user will be notified of the problem, and subject to the discretion of the support staff, their access will be reinstated.  Should the problem reoccur, the user account will be suspended again and the user will be contacted by support staff.  Upon review by support staff and approval of the Service Owner, the user's access will be either:  Reinstated if the review concludes that the user no longer presents a threat to the system  Removed if the review concludes the user is unable to correct the problem, and effectively demonstrates Inappropriate use of the service (see above).     Expiration of the Project  All projects are required to include an expected expiry date (not more than 12 months).  Project owners and users will be notified in advance of the expiration of the project and given to opportunity to extend the project by 12 months as required.  Project owners may elect to allow the project to expire, and are required to ensure they have secured any data they wish to preserve.  If no request to extend the project is made, it will be assumed that the project may expire and will be removed after the expiration date.  The removal of the project will include the deletion of all content in the project directory.", 
            "title": "Service Denial"
        }, 
        {
            "location": "/status/", 
            "text": "Availability\n\n\nIs Spartan operating normally?\n\n\n\n    \n\n    \n\n\n\n\nCurrent Usage\n\n\nHow busy is Spartan today?\n\n\n\n\n\n\n\n\n\n\n  \n\n\nHow to Interpret\n\n\n\n\n\n\nThis plot provides indicates how many CPUs (cores) are in-use on Spartan.\n\n\nUtilization may occasionally exceed 100% as resources are added/removed from service (utilization is always relative to the most recent available CPU count).\n\n\nThis data is acquired using the \nsacct\n command in Slurm to get a list of all recent jobs and their start/end times, and counting how many cores are allocated relative to total capacity.\n\n\n\n\n\n\n\n\n\n\n\nWait Time\n\n\nHow long will my job take to start?\n\n\n\n\n\n\n  \n\n    \n\n      Partition: \n\n    \n\n    \n\n      CPUs Requested: \n\n    \n\n    \n\n      Wall Time: \n\n    \n\n  \n\n\n\n\n\n\n\n\n  \n\n\nHow to Interpret\n\n\n\n\n\n\nThis plot provides data on how long previous jobs have taken to start, which can be used as guidance on how long your job might take to start.\n\n\nNote however that \"Past performance is no guarantee of future results\"; wait times can fluctuate quickly due to changes in usage or outages, and wait time could be considerably more or less than the historic average.\n\n\nDaily averages are shown, but points may be missing for days where there were no jobs matching the selected characteristics.\n\n\nThis data is acquired using the \nsacct\n command in Slurm to get a list of all recent jobs and their start/end times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n    window.onload = function() {\n\n        Plotly.d3.json(\"https://swift.rc.nectar.org.au:8888/v1/AUTH_5634a7ad82ad49579e4192f4db90191f/spartan_metrics/wait_time.json\" + '?' + Math.floor(Math.random() * 1000), function (data) {\n\n            function assignOptions(options, selector) {\n                $.each(options, function (val, text) {\n                    selector.append(\n                        $('<option></option>').val(text).html(text)\n                    );\n                });\n            }\n\n            function updatePlot() {\n                var graphData = [\n                    {   x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y'],\n                        text: 'hours',\n                        mode: 'markers',\n                        hoverinfo: 'x+text+y',\n                        marker: {\n                            size: 10\n                        },\n                        name: 'Daily Average'\n                    },\n                    {\n                        x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y_rolling_average'],\n                        hoverinfo: 'none',\n                        line: {\n                            color: 'rgb(150, 150, 150)',\n                            width: 1,\n                            dash: 'dash'\n                        },\n                        name: 'Rolling 14d Mean'\n                    }\n                ];\n\n                var layout = {\n                    title: 'Job Wait Time',\n                    width: 700,\n                    height: 400,\n                    yaxis: {\n                        title: 'Hours',\n                        hoverformat: '.2f hours'\n                    },\n                    legend: {\n                        x: 0.5, \n                        y: 1.2,\n                        orientation: 'h',\n                        xanchor: 'center',\n                    }\n                };\n                Plotly.newPlot('graph', graphData, layout);\n            }\n\n            var partitionSelect = $('#partitionSelect');\n            var coreSelect = $('#coreSelect');\n            var wallTimeSelect = $('#wallTimeSelect');\n\n            // Populate drop-down options\n            assignOptions(data['options']['partitions'], partitionSelect);\n            assignOptions(data['options']['cores'], coreSelect);\n            assignOptions(data['options']['wall_times'], wallTimeSelect);\n\n            // Attach listeners\n            partitionSelect.change(updatePlot);\n            coreSelect.change(updatePlot);\n            wallTimeSelect.change(updatePlot);\n\n            // Build initial plot\n            updatePlot()\n\n        });\n    }", 
            "title": "_Status"
        }, 
        {
            "location": "/status/#availability", 
            "text": "Is Spartan operating normally?", 
            "title": "Availability"
        }, 
        {
            "location": "/status/#current-usage", 
            "text": "How busy is Spartan today?     \n    How to Interpret    This plot provides indicates how many CPUs (cores) are in-use on Spartan.  Utilization may occasionally exceed 100% as resources are added/removed from service (utilization is always relative to the most recent available CPU count).  This data is acquired using the  sacct  command in Slurm to get a list of all recent jobs and their start/end times, and counting how many cores are allocated relative to total capacity.", 
            "title": "Current Usage"
        }, 
        {
            "location": "/status/#wait-time", 
            "text": "How long will my job take to start?   \n   \n     \n      Partition:  \n     \n     \n      CPUs Requested:  \n     \n     \n      Wall Time:  \n     \n      \n    How to Interpret    This plot provides data on how long previous jobs have taken to start, which can be used as guidance on how long your job might take to start.  Note however that \"Past performance is no guarantee of future results\"; wait times can fluctuate quickly due to changes in usage or outages, and wait time could be considerably more or less than the historic average.  Daily averages are shown, but points may be missing for days where there were no jobs matching the selected characteristics.  This data is acquired using the  sacct  command in Slurm to get a list of all recent jobs and their start/end times.      \n\n    window.onload = function() {\n\n        Plotly.d3.json(\"https://swift.rc.nectar.org.au:8888/v1/AUTH_5634a7ad82ad49579e4192f4db90191f/spartan_metrics/wait_time.json\" + '?' + Math.floor(Math.random() * 1000), function (data) {\n\n            function assignOptions(options, selector) {\n                $.each(options, function (val, text) {\n                    selector.append(\n                        $('<option></option>').val(text).html(text)\n                    );\n                });\n            }\n\n            function updatePlot() {\n                var graphData = [\n                    {   x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y'],\n                        text: 'hours',\n                        mode: 'markers',\n                        hoverinfo: 'x+text+y',\n                        marker: {\n                            size: 10\n                        },\n                        name: 'Daily Average'\n                    },\n                    {\n                        x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'],\n                        y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y_rolling_average'],\n                        hoverinfo: 'none',\n                        line: {\n                            color: 'rgb(150, 150, 150)',\n                            width: 1,\n                            dash: 'dash'\n                        },\n                        name: 'Rolling 14d Mean'\n                    }\n                ];\n\n                var layout = {\n                    title: 'Job Wait Time',\n                    width: 700,\n                    height: 400,\n                    yaxis: {\n                        title: 'Hours',\n                        hoverformat: '.2f hours'\n                    },\n                    legend: {\n                        x: 0.5, \n                        y: 1.2,\n                        orientation: 'h',\n                        xanchor: 'center',\n                    }\n                };\n                Plotly.newPlot('graph', graphData, layout);\n            }\n\n            var partitionSelect = $('#partitionSelect');\n            var coreSelect = $('#coreSelect');\n            var wallTimeSelect = $('#wallTimeSelect');\n\n            // Populate drop-down options\n            assignOptions(data['options']['partitions'], partitionSelect);\n            assignOptions(data['options']['cores'], coreSelect);\n            assignOptions(data['options']['wall_times'], wallTimeSelect);\n\n            // Attach listeners\n            partitionSelect.change(updatePlot);\n            coreSelect.change(updatePlot);\n            wallTimeSelect.change(updatePlot);\n\n            // Build initial plot\n            updatePlot()\n\n        });\n    }", 
            "title": "Wait Time"
        }
    ]
}