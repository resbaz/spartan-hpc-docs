{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"In the coming weeks, we'll be switching our helpdesk system to ServiceNow. The current hpc-support@unimelb.edu.au contact email will continue to work, but there will also be a web-based form to submit support and software installation requests. We're not anticipating any disruptions, and the current helpdesk system will be run in parallel for existing tickets. We will let you know of the cut-over date once confirmed. Spartan is High Performance Computing (HPC) system operated by Research Platform Services (ResPlat) at The University of Melbourne. It combines a high performance bare-metal compute with flexible cloud infrastructure to suit a wide range of use-cases. If your computing jobs take too long on your desktop computer, or are simply not possible due to a lack of speed and memory, a HPC system like Spartan can help. Use of this service is governed by the University's general regulations for IT resources . Spartan Daily Weather Report (20180829) Utilisation of disk is at 24% from the Ceph pool. Spartan is busy on physical partition, with close to 95% node allocation. Spartan is not as busy on cloud partion, with close to 98% node allocation. GPGPU partition very busy with 100% node allocation. Many cloud nodes out (69), mainly due to qh2-uom migration. Getting Help Training We run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research. Signup here: http://melbourne.resbaz.edu.au/participate Helpdesk If you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at hpc-support@unimelb.edu.au Please submit one topic per ticket. If you require a assistance with a separate matter, compose a new ticket. Do not reply to existing or closed tickets. For password resets please see the FAQ or contact University Services on +61 3 8344 0999 or ext 40999 or email service-centre@unimelb.edu.au . Specifications Spartan has a number of partitions available for general usage. A full list of partitions can be viewed with the command sinfo -s . Cloud 208 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow. Physical 20 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI). GPU 3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking. GPGPU 72 nodes, each with four NVIDIA P100 GPUs. See here for more details. bigmem 2 nodes, each with 36 cores and 1.5 TB of RAM. This partition is suited to memory-intensive single-node workloads. Other Partitions There are also special partitions which are outside normal walltime constraints. In particular, shortcloud and shortgpgpu should be used for quick test cases; the partitions have a maximum time constraint of one hour. Citing Spartan If you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support. Lev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa If you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper: This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200. Other Resources Spartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions. Nectar Nectar is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories. Spartan runs some of it's computation resources in the Nectar cloud. Melbourne Bioinformatics Melbourne Bioinformatics run two large HPC systems for life sciences researchers. Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) MASSIVE is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.","title":"Home"},{"location":"#spartan-daily-weather-report-20180829","text":"Utilisation of disk is at 24% from the Ceph pool. Spartan is busy on physical partition, with close to 95% node allocation. Spartan is not as busy on cloud partion, with close to 98% node allocation. GPGPU partition very busy with 100% node allocation. Many cloud nodes out (69), mainly due to qh2-uom migration.","title":"Spartan Daily Weather Report (20180829)"},{"location":"#getting-help","text":"","title":"Getting Help"},{"location":"#training","text":"We run regular one-day courses on HPC, shell scripting and parallel programming. ResPlat also offer training in a wide range of other digital tools to accelerate your research. Signup here: http://melbourne.resbaz.edu.au/participate","title":"Training"},{"location":"#helpdesk","text":"If you can't find an answer here, need advice, or otherwise stuck, you can contact our support team at hpc-support@unimelb.edu.au Please submit one topic per ticket. If you require a assistance with a separate matter, compose a new ticket. Do not reply to existing or closed tickets. For password resets please see the FAQ or contact University Services on +61 3 8344 0999 or ext 40999 or email service-centre@unimelb.edu.au .","title":"Helpdesk"},{"location":"#specifications","text":"Spartan has a number of partitions available for general usage. A full list of partitions can be viewed with the command sinfo -s . Cloud 208 nodes, each with 8 cores and 62 GB of RAM, allocated from the Melbourne node of the Nectar research cloud. This partition is best suited for general-purpose single-node jobs. Multiple node jobs will work, but communication between nodes will be comparatively slow. Physical 20 nodes, each with 12 cores and 251 GB of RAM. Each node is connected by high-speed 56GigE networking with 1.15 \u00b5sec latency, making this partition suited to multi-node jobs (e.g. those using OpenMPI). GPU 3 nodes, each with 12 cores, 251 GB of RAM, and four NVidia K80 GPUs. This partition also makes use of high-speed networking. GPGPU 72 nodes, each with four NVIDIA P100 GPUs. See here for more details. bigmem 2 nodes, each with 36 cores and 1.5 TB of RAM. This partition is suited to memory-intensive single-node workloads. Other Partitions There are also special partitions which are outside normal walltime constraints. In particular, shortcloud and shortgpgpu should be used for quick test cases; the partitions have a maximum time constraint of one hour.","title":"Specifications"},{"location":"#citing-spartan","text":"If you use Spartan to obtain results for a publication, we'd appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support. Lev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa If you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper: This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.","title":"Citing Spartan"},{"location":"#other-resources","text":"Spartan is just one of many research IT resources offered by The University of Melbourne, or available from other institutions. Nectar Nectar is a national initiative to provide cloud-based Infrastructure as a Service (IaaS) resources to researchers. It's based on OpenStack, and allows researchers on-demand access to computation instances, storage, and a variety of application platforms and Virtual Laboratories. Spartan runs some of it's computation resources in the Nectar cloud. Melbourne Bioinformatics Melbourne Bioinformatics run two large HPC systems for life sciences researchers. Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) MASSIVE is a HPC system at Monash University and the Australian Synchrotron which is optimized for imaging and visualization. It can run batched jobs, as well as provide a desktop environment for interactive work.","title":"Other Resources"},{"location":"access_policy/","text":"Access and Service Denial Policy Access Policy Access to the HPC Services (Spartan), is broadly governed by the access policies for the research cloud and the University of Melbourne IT policies, standards and guidelines . Additional requirements are as follows: Principal Investigator All projects are to be led by a University of Melbourne researcher (including students undertaking research as part of postgraduate studies) or researchers from institutions that have purchased access to the service. The project may be initiated and managed by a non-researcher in support of a researcher or research group. Research collaborator Participants in a project can be researchers or research support staff from anywhere. Any collaborators must abide by the policies governing the use of IT at the University of Melbourne, and any additional requirements as specified in either the Research Cloud End User Agreement and this document. Research support projects Where necessary, support projects may be established for the purpose of supporting the HPC Service, including training. Projects are subject to approval All projects are subject to approval by the Head of Research Compute Services and have been deemed to meet the policies and guidelines mentioned above. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Co-investment resources Exceptions to 1..4 above may occur where a faulty, department or research group purchases dedicated resources for their exclusive or priority use. Access to these resources will be at the discretion of the co-investment party, however, usage of these resources remain subject to the University's IT policies, standards and guidelines. Project information required All projects should provide the following: Principal Investigator (PI) name and position PI contact phone number, email address Project requestor's name and position (if different from PI) Project requestor's phone number, email address (if different from PI) PI faculty, department Project Field of Research primary code, and additional FOR codes. Project expiration (default is 12 months) Expected usage e.g. Software (Matlab, GAMESS, own code, etc.) Hardware (Bare-metal, GPU, co-investment partition, etc.) Special requirements for the project Service Denial Under certain circumstances, access may be restricted or terminated. These include: Inappropriate use of the service If a user engages in activity that contravenes the University IT policies, standards and guidelines, or the processes described here, their access to the service will be suspended, pending a review. In the case where the user is also the project owner, the project may also be suspended, effectively denying access to other project participants. If the activity is deemed to be malicious, the user and the project may be removed from the service. Damage to the service If a user misuses the service, either wittingly or unwittingly, and causes the degrading or denial of service for other users, access will be suspended pending a review. The user will be notified of the problem, and subject to the discretion of the support staff, their access will be reinstated. Should the problem reoccur, the user account will be suspended again and the user will be contacted by support staff. Upon review by support staff and approval of the Service Owner, the user's access will be either: Reinstated if the review concludes that the user no longer presents a threat to the system Removed if the review concludes the user is unable to correct the problem, and effectively demonstrates Inappropriate use of the service (see above). Expiration of the Project All projects are required to include an expected expiry date (not more than 12 months). Project owners and users will be notified in advance of the expiration of the project and given to opportunity to extend the project by 12 months as required. Project owners may elect to allow the project to expire, and are required to ensure they have secured any data they wish to preserve. If no request to extend the project is made, it will be assumed that the project may expire and will be removed after the expiration date. The removal of the project will include the deletion of all content in the project directory.","title":"_Access and Service Denial Policy"},{"location":"access_policy/#access-and-service-denial-policy","text":"","title":"Access and Service Denial Policy"},{"location":"access_policy/#access-policy","text":"Access to the HPC Services (Spartan), is broadly governed by the access policies for the research cloud and the University of Melbourne IT policies, standards and guidelines . Additional requirements are as follows: Principal Investigator All projects are to be led by a University of Melbourne researcher (including students undertaking research as part of postgraduate studies) or researchers from institutions that have purchased access to the service. The project may be initiated and managed by a non-researcher in support of a researcher or research group. Research collaborator Participants in a project can be researchers or research support staff from anywhere. Any collaborators must abide by the policies governing the use of IT at the University of Melbourne, and any additional requirements as specified in either the Research Cloud End User Agreement and this document. Research support projects Where necessary, support projects may be established for the purpose of supporting the HPC Service, including training. Projects are subject to approval All projects are subject to approval by the Head of Research Compute Services and have been deemed to meet the policies and guidelines mentioned above. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Co-investment resources Exceptions to 1..4 above may occur where a faulty, department or research group purchases dedicated resources for their exclusive or priority use. Access to these resources will be at the discretion of the co-investment party, however, usage of these resources remain subject to the University's IT policies, standards and guidelines. Project information required All projects should provide the following: Principal Investigator (PI) name and position PI contact phone number, email address Project requestor's name and position (if different from PI) Project requestor's phone number, email address (if different from PI) PI faculty, department Project Field of Research primary code, and additional FOR codes. Project expiration (default is 12 months) Expected usage e.g. Software (Matlab, GAMESS, own code, etc.) Hardware (Bare-metal, GPU, co-investment partition, etc.) Special requirements for the project","title":"Access Policy"},{"location":"access_policy/#service-denial","text":"Under certain circumstances, access may be restricted or terminated. These include: Inappropriate use of the service If a user engages in activity that contravenes the University IT policies, standards and guidelines, or the processes described here, their access to the service will be suspended, pending a review. In the case where the user is also the project owner, the project may also be suspended, effectively denying access to other project participants. If the activity is deemed to be malicious, the user and the project may be removed from the service. Damage to the service If a user misuses the service, either wittingly or unwittingly, and causes the degrading or denial of service for other users, access will be suspended pending a review. The user will be notified of the problem, and subject to the discretion of the support staff, their access will be reinstated. Should the problem reoccur, the user account will be suspended again and the user will be contacted by support staff. Upon review by support staff and approval of the Service Owner, the user's access will be either: Reinstated if the review concludes that the user no longer presents a threat to the system Removed if the review concludes the user is unable to correct the problem, and effectively demonstrates Inappropriate use of the service (see above). Expiration of the Project All projects are required to include an expected expiry date (not more than 12 months). Project owners and users will be notified in advance of the expiration of the project and given to opportunity to extend the project by 12 months as required. Project owners may elect to allow the project to expire, and are required to ensure they have secured any data they wish to preserve. If no request to extend the project is made, it will be assumed that the project may expire and will be removed after the expiration date. The removal of the project will include the deletion of all content in the project directory.","title":"Service Denial"},{"location":"edward_transition/","text":"Edward to Spartan : A Short Transition Guide When is Edward Being Retired? Edward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects. Edward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward. How is Spartan Different? Spartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition. Spartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply. However, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling. This means that job scripts will be different between the two systems and translation will be required. How do we submit jobs in SLURM? Job submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system. Core command for checking queue is: showq (TORQUE) or squeue (SLURM) Core command for job submission is qsub [jobscript] (TORQUE), or sbatch [jobscript] (SLURM) What About Working Directories and Environments? TORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this. TORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default. What about Job Status and Output? Core command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM) Both TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the -j oe option in PBS). What are the user command differences? User Command TORQUE (Edward) SLURM (Spartan) Job submission qsub [script_file] sbatch [script_file] Job delete qdel [job_id] scancel [job_id] Job status qstat [job_id] squeue [job_id] Job status qstat -u [user_name] squeue -u [user_name] Node list pbsnodes -a sinfo -N Queue list qstat -Q squeue Cluster status showq qstatus -a What are the job command differences? Job Specification TORQUE (Edward) SLURM (Spartan) Script directive #PBS #SBATCH Queue -q [queue] -p [queue] Job Name -N [name] --job-name=[name] Nodes -l nodes=[count] -N [min[-max]] CPU Count -l ppn=[count] -n [count] Wall Clock Limit -l walltime=[hh:mm:ss] -t [days-hh:mm:ss] Event Address -M [address] --mail-user=[address] Event Notification -m abe --mail-type=[events] Memory Size -l mem=[MB] --mem=[mem][M G T] Proc Memory Size -l pmem=[MB] --mem-per-cpu=[mem][M G T] What are the environment commands differences? Environment Command TORQUE (Edward) SLURM (Spartan) Job ID $PBS_JOBID $SLURM_JOBID Submit Directory $PBS_O_WORKDIR $SLURM_SUBMIT_DIR Submit Host $PBS_O_HOST $SLURM_SUBMIT_HOST Node List $PBS_NODEFILE $SLURM_JOB_NODELIST Job Array Index $PBS_ARRAYID $SLURM_ARRAY_TASK_ID Automation and Acknowledgements There is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm This guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016","title":"_Edward Transition"},{"location":"edward_transition/#edward-to-spartan-a-short-transition-guide","text":"","title":"Edward to Spartan : A Short Transition Guide"},{"location":"edward_transition/#when-is-edward-being-retired","text":"Edward, the University of Melbourne's general purpose High Performance Computer (HPC), has been operating since 2011, replacing the even older Alfred system. As of May 13th, Edward has run for a total compute time 19,810,086 hours over 1,260,177 jobs, serving 864 researchers over 368 projects. Edward will operate until November 2016, when the MOAB license expires and will be replaced by Spartan for general purpose HPC tasks. No new software will be installed on Edward.","title":"When is Edward Being Retired?"},{"location":"edward_transition/#how-is-spartan-different","text":"Spartan is a hybrid HPC/Cloud system. The design is the result of a thorough review of usage on the Edward system which revealed that it was being used for large numbers of single-core (76.35%) and low memory (96.83% under 4GB) of jobs. As a result, the new Spartan system will have a large number (c4000 cores, with 16 cores per node) from the NeCTAR Research cloud as one partition, and a much smaller traditional HPC \"physical\" partition. Spartan uses the lmod modules system instead of the traditional tcl-based modules system. In nearly all cases the same commands apply. However, Spartan will also use Simple Linux Utility for Resource Management (SLURM) for job management, whereas Edward used Terascale Open-source Resource and QUEue Manager (TORQUE), as the resource manager and Moab for job scheduling. This means that job scripts will be different between the two systems and translation will be required.","title":"How is Spartan Different?"},{"location":"edward_transition/#how-do-we-submit-jobs-in-slurm","text":"Job submission in SLURM is conceptual identical in SLURM as it in PBS and the structure is very similar as well. Setup and launch consists of writing a short script that initially makes resource requests (walltime, processors, memory, queues) and then commands (loading modules, changing directories, running executables against datasets etc), and optionally checking queueing system. Core command for checking queue is: showq (TORQUE) or squeue (SLURM) Core command for job submission is qsub [jobscript] (TORQUE), or sbatch [jobscript] (SLURM)","title":"How do we submit jobs in SLURM?"},{"location":"edward_transition/#what-about-working-directories-and-environments","text":"TORQUE jobs must include cd $PBS_O_WORKDIR to change to the directory where they were launched. SLURM jobs do not require this. TORQUE jobs do not parse the user's environment to the compute node by default; the #$PBS -V command is required. SLURM does this by default.","title":"What About Working Directories and Environments?"},{"location":"edward_transition/#what-about-job-status-and-output","text":"Core command for checking job qstat [jobid] (TORQUE), checkjob [jobid] (Moab), squeue -j [jobid] (SLURM), detailed command scontrol show job [jobid] (SLURM) Core command for deleting job qdel [jobid] (TORQUE), scancel [jobid] (SLURM) Both TORQUE and Slurm provide error and output files (combined into one by default in SLURM, like the -j oe option in PBS).","title":"What about Job Status and Output?"},{"location":"edward_transition/#what-are-the-user-command-differences","text":"User Command TORQUE (Edward) SLURM (Spartan) Job submission qsub [script_file] sbatch [script_file] Job delete qdel [job_id] scancel [job_id] Job status qstat [job_id] squeue [job_id] Job status qstat -u [user_name] squeue -u [user_name] Node list pbsnodes -a sinfo -N Queue list qstat -Q squeue Cluster status showq qstatus -a","title":"What are the user command differences?"},{"location":"edward_transition/#what-are-the-job-command-differences","text":"Job Specification TORQUE (Edward) SLURM (Spartan) Script directive #PBS #SBATCH Queue -q [queue] -p [queue] Job Name -N [name] --job-name=[name] Nodes -l nodes=[count] -N [min[-max]] CPU Count -l ppn=[count] -n [count] Wall Clock Limit -l walltime=[hh:mm:ss] -t [days-hh:mm:ss] Event Address -M [address] --mail-user=[address] Event Notification -m abe --mail-type=[events] Memory Size -l mem=[MB] --mem=[mem][M G T] Proc Memory Size -l pmem=[MB] --mem-per-cpu=[mem][M G T]","title":"What are the job command differences?"},{"location":"edward_transition/#what-are-the-environment-commands-differences","text":"Environment Command TORQUE (Edward) SLURM (Spartan) Job ID $PBS_JOBID $SLURM_JOBID Submit Directory $PBS_O_WORKDIR $SLURM_SUBMIT_DIR Submit Host $PBS_O_HOST $SLURM_SUBMIT_HOST Node List $PBS_NODEFILE $SLURM_JOB_NODELIST Job Array Index $PBS_ARRAYID $SLURM_ARRAY_TASK_ID","title":"What are the environment commands differences?"},{"location":"edward_transition/#automation-and-acknowledgements","text":"There is a git repository for converting PBS to SLURM: https://github.com/bjpop/pbs2slurm This guide was written by Lev Lafayette for Research Platforms, University of Melbourne. Version 0.2, June 30 , 2016","title":"Automation and Acknowledgements"},{"location":"faq/","text":"What's special about Spartan? Most modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed. For certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of embarrassingly parallel . That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance. Spartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking. How do I get an account? Access to Spartan requires an an account, which you can request using Karaage . Accounts are associated with a particular project; you can either join an existing project or create a new one. New projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators. How do I access Spartan once I have an account? You'll need an SSH client. Mac and Linux computers will already have one installed, just use the command ssh yourUsername@spartan.hpc.unimelb.edu.au at your terminal. For Windows, you'll need to download an SSH client such as PuTTY , set hostname as spartan.hpc.unimelb.edu.au and select Open. You'll be asked for your Spartan username and password. My password isn't working! Make sure you're using your Spartan password that you set in Karaage . Your Spartan password is not necessarily the same as your central university password. You can request a password reset here . If you are still having trouble, contact the University of Melbourne Service Desk on +61 3 8344 0999 or ext 40999 or email or service-centre@unimelb.edu.au . How do I add people to a project? If you are a project leader you may invite people to join your project. Login to Karaage, and go to your Karaage project list , select the appropriate project, and select the \"Invite a new user\" option. The user will then receive an invitation link to join the project and set up an account. However if the belong to an institution that does not have a SAML login process (e.g., international researchers) it is worthwhile contacting the Spartan at hpc-support@unimelb.edu.au . Then the sysadmins will add the person manually to the project and reset their password. What are Spartan's specifications? Spartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us. The cloud partition nominally consists of 100 nodes with 8 cores each from the Nectar research cloud, however it is capable of accessing more as the load on Spartan grows. There also exist a number of specialist nodes with expanded memory or GPGPU hardware, as well as partitions dedicated to particular departments and research groups. What software is installed? Spartan uses a modules system ( lmod ) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the module avail command, and load a module with the module load command. Typically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with sinteractive ). Instead you load the modules in your Slurm script before executing your particular software. What if the software I need is not installed? Get in contact with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that: It works Software licenses are managed Code is compiled with the appropriate flags to maximize performance Others users can also make use of the software. Where do I go for help? First off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check here for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at: http://melbourne.resbaz.edu.au/participate Second, check the documentation here, as well as for the software you're running on Spartan (like Slurm). Finally, if you ever get stuck, please feel free to email HPC support . We're here to help make your research more productive and enjoyable, and we'll do everything we can to help. How do I get access to GPUs? Spartan includes two partitions with GPUs (as well as a third private physics-gpu partition). The legacy gpu partition includes four Nvidia K80 GPUs per node, while the newer gpgpu partition includes four Nvidia P100 GPUs per node. They can be specified in your job script with #SBATCH --partition gpu and #SBATCH --partition gpgpu , respectively. You'll also need to include a generic resource request in your job script, for example #SBATCH --gres=gpu:2 will request two GPUs for your job. A range of GPU-accelerated software such as TensorFlow is available on Spartan example , as well as CUDA for developing your own GPU applications example . N.B. The GPGPU partition is not automatically available to all Spartan users, and a dedicated project must be created to request access. See here for more details. How do I submit a job? You'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out Getting Started for an example. Do I need to know how to use Linux? Just the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as this tutorial . How do I create a multi-core job? There are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 This is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores. Alternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core: #SBATCH --nodes=1 #SBATCH --ntasks=8 #SBATCH --cpus-per-task=1 This approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick. Keep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this. How do I create a multi-node job? Here's an example of a job with two nodes, each using 12 cores. #SBATCH --nodes=2 #SBATCH --ntasks=2 #SBATCH --cpus-per-task=12 Note that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI. For multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes. What other options are there for running my job? Many different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for Slurm (the job manager we use on Spartan) for details. How do I create a job array? Job arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other. Say we have an array of files, data_1.dat to data_50.dat to process with myProgram : #!/bin/bash #SBATCH --ntasks=1 #SBATCH --time=0-00:15:00 #SBATCH --array=1-50 myProgram data_${SLURM_ARRAY_TASK_ID}.dat This will create 50 jobs, each calling myProgram with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!) How do I request more memory? By default the scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. In some cases this might not be enough (e.g., very large dataset that needs to be loaded with low level of parallelisation). Additional memory can be allocated with the --mem=[mem][M|G|T] directive (entire job) or --mem-per-cpu=[mem][M|G|T] (per core). Maximum should be based around total cores -1 (for system processes). The --mem-per-cpu directive is for threads for OpenMP applications and processor ranks for MPI. It is best to reserve some memory (about 1 core's worth) for system processes. Are there more examples I can look at? If you go to /usr/local/common/ on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself. How do I make my program run fast on Spartan? Spartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, so improved performance is dependent on using parallel processing (MPI or OpenMP) or job arrays. How do I cite Spartan in my publications? If you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support. Lev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa How do I transition my work from the old HPC system Edward to Spartan? Here's a guide to help you. How do setup passwordless SSH login? A passwordless SSH for Spartan will make your life easier. You won't even need to remember your password! If you have a *nix system (e.g., UNIX, Linux, MacOS X) open up a terminal on your local system and generate a keypair. $ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Created directory '/home/user/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa. Your public key has been saved in /home/user/.ssh/id_rsa.pub. The key fingerprint is: 43:51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 user@localhost Now append the new public key to ~/.ssh/authorized_keys on Spartan (you'll be asked for your password one last time). $ cat .ssh/id_rsa.pub | ssh username@spartan.hpc.unimelb.edu.au 'cat .ssh/authorized_keys' Depending on your version of SSH you might also have to do the following changes: Put the public key in .ssh/authorized_keys2 Change the permissions of .ssh to 700 Change the permissions of .ssh/authorized_keys2 to 640 You can now SSH to Spartan without having to enter your password! How can I avoid typing myUsername@spartan.hpc.unimelb.edu.au everytime I connect? An SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname. Create the text file in your ~/.ssh directory with your preferred text editor, for example, nano . nano .ssh/config Enter the following (replacing username with your actual username of course!): Host * ServerAliveInterval 120 Host spartan Hostname spartan.hpc.unimelb.edu.au User username Now to connect to Spartan, you need only type ssh spartan .","title":"FAQ"},{"location":"faq/#whats-special-about-spartan","text":"Most modern HPC systems are built around a cluster of commodity computers tied together with very-fast networking. This allows computation to run across multiple cores in parallel, quickly sharing data between themselves as needed. For certain jobs, this architecture is essential to achieving high-performance. For others, however, this is not the case, and each node can run without communicating with the others in the cluster. This class of problems often comes under the guise of embarrassingly parallel . That is, they can be run as independent parallel tasks by splitting up the data or calculation into discrete chunks. In this case, high speed networking is unnecessary, and the resources can be better spent on utilizing more cores to achieve high performance. Spartan combines both approaches. By default, jobs run in the flexible cloud partition. For those that need it, there's a physical partition running on bare-metal hardware, interconnected with low-latency 56 GigE networking.","title":"What's special about Spartan?"},{"location":"faq/#how-do-i-get-an-account","text":"Access to Spartan requires an an account, which you can request using Karaage . Accounts are associated with a particular project; you can either join an existing project or create a new one. New projects are subject to approval by the Head of Research Compute Services. Projects must demonstrate an approved research goal or goals, or demonstrate potential to support research activity. Projects require a Principle Investigator and may have additional Research Collaborators.","title":"How do I get an account?"},{"location":"faq/#how-do-i-access-spartan-once-i-have-an-account","text":"You'll need an SSH client. Mac and Linux computers will already have one installed, just use the command ssh yourUsername@spartan.hpc.unimelb.edu.au at your terminal. For Windows, you'll need to download an SSH client such as PuTTY , set hostname as spartan.hpc.unimelb.edu.au and select Open. You'll be asked for your Spartan username and password.","title":"How do I access Spartan once I have an account?"},{"location":"faq/#my-password-isnt-working","text":"Make sure you're using your Spartan password that you set in Karaage . Your Spartan password is not necessarily the same as your central university password. You can request a password reset here . If you are still having trouble, contact the University of Melbourne Service Desk on +61 3 8344 0999 or ext 40999 or email or service-centre@unimelb.edu.au .","title":"My password isn't working!"},{"location":"faq/#how-do-i-add-people-to-a-project","text":"If you are a project leader you may invite people to join your project. Login to Karaage, and go to your Karaage project list , select the appropriate project, and select the \"Invite a new user\" option. The user will then receive an invitation link to join the project and set up an account. However if the belong to an institution that does not have a SAML login process (e.g., international researchers) it is worthwhile contacting the Spartan at hpc-support@unimelb.edu.au . Then the sysadmins will add the person manually to the project and reset their password.","title":"How do I add people to a project?"},{"location":"faq/#what-are-spartans-specifications","text":"Spartan consists of 21 bare-metal nodes with 12 cores connected via 56GigE ethernet, achieving a latency of 1.15 us. The cloud partition nominally consists of 100 nodes with 8 cores each from the Nectar research cloud, however it is capable of accessing more as the load on Spartan grows. There also exist a number of specialist nodes with expanded memory or GPGPU hardware, as well as partitions dedicated to particular departments and research groups.","title":"What are Spartan's specifications?"},{"location":"faq/#what-software-is-installed","text":"Spartan uses a modules system ( lmod ) to load and unload different packages, including different versions of the same software. You can check what's currently installed using the module avail command, and load a module with the module load command. Typically one doesn't load modules directly unless they're in an interactive session on a compute node (launched with sinteractive ). Instead you load the modules in your Slurm script before executing your particular software.","title":"What software is installed?"},{"location":"faq/#what-if-the-software-i-need-is-not-installed","text":"Get in contact with us and we can install it for you. Generally speaking, you should avoid compiling software on Spartan, unless you wrote it from scratch for your own use. By letting us handle it, we can make sure that: It works Software licenses are managed Code is compiled with the appropriate flags to maximize performance Others users can also make use of the software.","title":"What if the software I need is not installed?"},{"location":"faq/#where-do-i-go-for-help","text":"First off, we encourage researchers that are new to HPC to undertake training with us. It's free! And we can tailor a specific training program for you, for instance around a specific software package, if there is the demand. Check here for a calendar of when the next event is planned, along with the other training programs offered in coordination with ResBaz. Sign up to be notified of our next training events at: http://melbourne.resbaz.edu.au/participate Second, check the documentation here, as well as for the software you're running on Spartan (like Slurm). Finally, if you ever get stuck, please feel free to email HPC support . We're here to help make your research more productive and enjoyable, and we'll do everything we can to help.","title":"Where do I go for help?"},{"location":"faq/#how-do-i-get-access-to-gpus","text":"Spartan includes two partitions with GPUs (as well as a third private physics-gpu partition). The legacy gpu partition includes four Nvidia K80 GPUs per node, while the newer gpgpu partition includes four Nvidia P100 GPUs per node. They can be specified in your job script with #SBATCH --partition gpu and #SBATCH --partition gpgpu , respectively. You'll also need to include a generic resource request in your job script, for example #SBATCH --gres=gpu:2 will request two GPUs for your job. A range of GPU-accelerated software such as TensorFlow is available on Spartan example , as well as CUDA for developing your own GPU applications example . N.B. The GPGPU partition is not automatically available to all Spartan users, and a dedicated project must be created to request access. See here for more details.","title":"How do I get access to GPUs?"},{"location":"faq/#how-do-i-submit-a-job","text":"You'll need your data files and scripts, the software you want to run installed on Spartan, and a job script so that Spartan knows how to put everything together. Check out Getting Started for an example.","title":"How do I submit a job?"},{"location":"faq/#do-i-need-to-know-how-to-use-linux","text":"Just the basics to get started. We cover this in our introductory training course, and there are many online resources available to get you started, such as this tutorial .","title":"Do I need to know how to use Linux?"},{"location":"faq/#how-do-i-create-a-multi-core-job","text":"There are two options here. If you want to run a single instance of your program and have that program access 8 cores, you can do this: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 This is the typical approach if your program makes use of multi-threading or subprocesses to make use of the additional cores. Alternatively, if you'd like to run multiple instances (tasks) of your program, each on their own core: #SBATCH --nodes=1 #SBATCH --ntasks=8 #SBATCH --cpus-per-task=1 This approach might be used for jobs where there are multiple instances of a program running at once, but they communicate with each other (e.g. using OpenMPI) and so should be kept within a single node so that communication between tasks is quick. Keep in mind the number of cores that actually exist within a node, eight for the cloud partition and twelve for physical -- you can't request more than this.","title":"How do I create a multi-core job?"},{"location":"faq/#how-do-i-create-a-multi-node-job","text":"Here's an example of a job with two nodes, each using 12 cores. #SBATCH --nodes=2 #SBATCH --ntasks=2 #SBATCH --cpus-per-task=12 Note that you can't have a single instance of a program running across different nodes. Instead, you would usually run two instances of a program (one on each node), and have them pass messages between each other so they can work in parallel using a framework like OpenMPI. For multi-node jobs, it is usually preferable to use the physical partition, because this partition has faster networking between nodes.","title":"How do I create a multi-node job?"},{"location":"faq/#what-other-options-are-there-for-running-my-job","text":"Many different permutations of cores, memory, nodes, tasks and dependencies are possible to suit different use cases. Refer to the documentation for Slurm (the job manager we use on Spartan) for details.","title":"What other options are there for running my job?"},{"location":"faq/#how-do-i-create-a-job-array","text":"Job arrays are great for kicking off a large number of independent jobs at once. For instance, if you're batch processing a series of files, and the processing for each file can be performed independently of any other. Say we have an array of files, data_1.dat to data_50.dat to process with myProgram : #!/bin/bash #SBATCH --ntasks=1 #SBATCH --time=0-00:15:00 #SBATCH --array=1-50 myProgram data_${SLURM_ARRAY_TASK_ID}.dat This will create 50 jobs, each calling myProgram with a different data file. These jobs will run in any order, as soon as resources are available (potentially, all at the same time!)","title":"How do I create a job array?"},{"location":"faq/#how-do-i-request-more-memory","text":"By default the scheduler will set memory equal to the total amount on a compute node divided by the number of cores requested. In some cases this might not be enough (e.g., very large dataset that needs to be loaded with low level of parallelisation). Additional memory can be allocated with the --mem=[mem][M|G|T] directive (entire job) or --mem-per-cpu=[mem][M|G|T] (per core). Maximum should be based around total cores -1 (for system processes). The --mem-per-cpu directive is for threads for OpenMP applications and processor ranks for MPI. It is best to reserve some memory (about 1 core's worth) for system processes.","title":"How do I request more memory?"},{"location":"faq/#are-there-more-examples-i-can-look-at","text":"If you go to /usr/local/common/ on Spartan there are examples for a wide range of programs. You can copy these into your home directory and run them for yourself.","title":"Are there more examples I can look at?"},{"location":"faq/#how-do-i-make-my-program-run-fast-on-spartan","text":"Spartan, like almost all modern HPC systems, delivers high-performance by combining lots of smaller computers (nodes) together in a cluster. Each core within a node probably isn't much faster than on your own personal computer, so improved performance is dependent on using parallel processing (MPI or OpenMP) or job arrays.","title":"How do I make my program run fast on Spartan?"},{"location":"faq/#how-do-i-cite-spartan-in-my-publications","text":"If you use Spartan to obtain results, we'd very much appreciate if you'd cite our service, including the DOI below. This makes it easy for us demonstrate research impact, helping to secure ongoing funding for expansion and user support. Lev Lafayette, Greg Sauter, Linh Vu, Bernard Meade, \"Spartan Performance and Flexibility: An HPC-Cloud Chimera\", OpenStack Summit, Barcelona, October 27, 2016. doi.org/10.4225/49/58ead90dceaaa","title":"How do I cite Spartan in my publications?"},{"location":"faq/#how-do-i-transition-my-work-from-the-old-hpc-system-edward-to-spartan","text":"Here's a guide to help you.","title":"How do I transition my work from the old HPC system Edward to Spartan?"},{"location":"faq/#how-do-setup-passwordless-ssh-login","text":"A passwordless SSH for Spartan will make your life easier. You won't even need to remember your password! If you have a *nix system (e.g., UNIX, Linux, MacOS X) open up a terminal on your local system and generate a keypair. $ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Created directory '/home/user/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa. Your public key has been saved in /home/user/.ssh/id_rsa.pub. The key fingerprint is: 43:51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 user@localhost Now append the new public key to ~/.ssh/authorized_keys on Spartan (you'll be asked for your password one last time). $ cat .ssh/id_rsa.pub | ssh username@spartan.hpc.unimelb.edu.au 'cat .ssh/authorized_keys' Depending on your version of SSH you might also have to do the following changes: Put the public key in .ssh/authorized_keys2 Change the permissions of .ssh to 700 Change the permissions of .ssh/authorized_keys2 to 640 You can now SSH to Spartan without having to enter your password!","title":"How do setup passwordless SSH login?"},{"location":"faq/#how-can-i-avoid-typing-myusernamespartanhpcunimelbeduau-everytime-i-connect","text":"An SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname. Create the text file in your ~/.ssh directory with your preferred text editor, for example, nano . nano .ssh/config Enter the following (replacing username with your actual username of course!): Host * ServerAliveInterval 120 Host spartan Hostname spartan.hpc.unimelb.edu.au User username Now to connect to Spartan, you need only type ssh spartan .","title":"How can I avoid typing myUsername@spartan.hpc.unimelb.edu.au everytime I connect?"},{"location":"getting_started/","text":"Prerequisite: You'll need a basic understanding of the Linux command line to use Spartan. But don't worry, you don't need to be an expert, and there are many resources out there to help you. This tutorial is a good place to start. 1. Create an account Go to Karaage to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one. 2. Login to Spartan via SSH Note that your password for Spartan is created during sign-up, and is different to your university password. Windows Download an SSH client such as PuTTY , set hostname as spartan.hpc.unimelb.edu.au and click Open. You'll be asked for your Spartan username and password. Posix (Linux, OS X) You'll already have an SSH client installed. Easy! Open a terminal and enter: $ ssh yourUsername@spartan.hpc.unimelb.edu.au 3. Create a job Spartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset. Copy the example into your home directory, and change working directory: $ cp -r /usr/local/common/Python ~/ $ cd ~/Python The dataset is in minitwitter.csv , and the analysis code in twitter_search_541635.py . The files ending in .slurm tell Spartan how to run your job. For example, twitter_one_node_eight_cores.slurm requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for). #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=8 #SBATCH --time=0-12:00:00 # Load required modules module load Python/3.5.2-intel-2016.u3 # Launch multiple process python code echo Searching for mentions time mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m echo Searching for topics time mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t echo Searching for the keyword 'jumping' time mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping 3. Submit your job First off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node. Please don't run jobs on the login node! Instead, use the scheduling tool Slurm , and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available. Go ahead and launch your job using sbatch : $ sbatch twitter_one_node_eight_cores.slurm Submitted batch job 27300 We can check how it's progressing using squeue : $ squeue --job 27300 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 27300 cloud twitter_ perryd R 10:48 1 spartan040 When complete, an output file is created which logs the output from your job, for the above this has the filename slurm-27300.out . You can also perform interactive work using the sinteractive command. This is handy for testing and debugging. This will allocate and log you in to a computing node.","title":"Getting Started"},{"location":"getting_started/#1-create-an-account","text":"Go to Karaage to request a Spartan account using your University of Melbourne login. You can either join an existing project, or create a new one.","title":"1. Create an account"},{"location":"getting_started/#2-login-to-spartan-via-ssh","text":"Note that your password for Spartan is created during sign-up, and is different to your university password. Windows Download an SSH client such as PuTTY , set hostname as spartan.hpc.unimelb.edu.au and click Open. You'll be asked for your Spartan username and password. Posix (Linux, OS X) You'll already have an SSH client installed. Easy! Open a terminal and enter: $ ssh yourUsername@spartan.hpc.unimelb.edu.au","title":"2. Login to Spartan via SSH"},{"location":"getting_started/#3-create-a-job","text":"Spartan has some shared example code that we can borrow. We'll use the Python example which searches a Twitter dataset. Copy the example into your home directory, and change working directory: $ cp -r /usr/local/common/Python ~/ $ cd ~/Python The dataset is in minitwitter.csv , and the analysis code in twitter_search_541635.py . The files ending in .slurm tell Spartan how to run your job. For example, twitter_one_node_eight_cores.slurm requests 8 cores, and a wall time of 12 hours (i.e. maximum time job will run for). #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=8 #SBATCH --time=0-12:00:00 # Load required modules module load Python/3.5.2-intel-2016.u3 # Launch multiple process python code echo Searching for mentions time mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -m echo Searching for topics time mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -t echo Searching for the keyword 'jumping' time mpiexec -n 8 python3 twitter_search_541635.py -i /data/projects/COMP90024/twitter.csv -s jumping","title":"3. Create a job"},{"location":"getting_started/#3-submit-your-job","text":"First off, when you connect to Spartan, you're connecting to the login node (shared by all users), not an actual computing node. Please don't run jobs on the login node! Instead, use the scheduling tool Slurm , and scripts like the above. They tell Slurm where to run your job, how many cores you need, and how long it will take. Slurm will then allocate resources for your job, placing it in a queue if they're not yet available. Go ahead and launch your job using sbatch : $ sbatch twitter_one_node_eight_cores.slurm Submitted batch job 27300 We can check how it's progressing using squeue : $ squeue --job 27300 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 27300 cloud twitter_ perryd R 10:48 1 spartan040 When complete, an output file is created which logs the output from your job, for the above this has the filename slurm-27300.out . You can also perform interactive work using the sinteractive command. This is handy for testing and debugging. This will allocate and log you in to a computing node.","title":"3. Submit your job"},{"location":"gpu/","text":"Spartan hosts a GPGPU service, developed in conjunction with Research Platform Services, the Melbourne School of Engineering, Melbourne Bioinformatics, RMIT, La Trobe University, St Vincent's Institute of Medical Research and Deakin University. It was funded through ARC LIEF grant LE170100200. It consists of 72 nodes, each with four NVIDIA P100 graphics cards, which can provide a theoretical maximum of around 900 teraflops. These nodes will be presented in 3 subclusters and will be released in stages. The first subcluster consisting of 28 nodes is available immediately. The remaining subclusters will come online as demand increases. Access The GPGPU cluster is available to University researchers, as well as external institutions that partnered through the ARC LIEF grant. Those with existing Spartan accounts will need to create a new project dedicated to GPGPU use. Data can still be transferred from existing projects, but an independent GPGPU project is required for accounting purposes. Those new to Spartan can signup for a user account at https://dashboard.hpc.unimelb.edu.au/karaage/ , as well as request a new project (or access to an existing one). Usage The GPGPU cluster operates as per other Spartan partitions, and so the documentation on this page can be followed with respect to logging in, accessing software, and creating job scripts. To request one or more GPGPUs in your job script, see: https://dashboard.hpc.unimelb.edu.au/faq/#how-do-i-get-access-to-gpus Citation If you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper: This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.","title":"_GPGPU"},{"location":"gpu/#access","text":"The GPGPU cluster is available to University researchers, as well as external institutions that partnered through the ARC LIEF grant. Those with existing Spartan accounts will need to create a new project dedicated to GPGPU use. Data can still be transferred from existing projects, but an independent GPGPU project is required for accounting purposes. Those new to Spartan can signup for a user account at https://dashboard.hpc.unimelb.edu.au/karaage/ , as well as request a new project (or access to an existing one).","title":"Access"},{"location":"gpu/#usage","text":"The GPGPU cluster operates as per other Spartan partitions, and so the documentation on this page can be followed with respect to logging in, accessing software, and creating job scripts. To request one or more GPGPUs in your job script, see: https://dashboard.hpc.unimelb.edu.au/faq/#how-do-i-get-access-to-gpus","title":"Usage"},{"location":"gpu/#citation","text":"If you are using the LIEF GPGPU cluster for a publication, please include the following citation in the acknowledgements section of your paper: This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.","title":"Citation"},{"location":"managing_data/","text":"Chances are you need to run your HPC job against a dataset, perhaps quite a sizable one. There are a number of places to store data on Spartan while you're working with it, and ways to get data in and out. Where to Store Your Data on Spartan Projects Directory Your projects directory is the best place to store research data while you're working on it. It's located at /data/projects/ projectid for older projects, or /data/cephfs/ projectid for newer ones. Others in your project can access it, and 500 GB of storage is available per project. If you need more than this, get in touch and we'll try to find a solution. In general 1 TB of project storage is available upon request, and up to 10 TB is possible if needed. Project storage beyond 10 TB will generally require some sort of co-investment, but this may be waived in some circumstances, particularly for high-value shared datasets. Home Directory Your home directory, i.e. /home/yourusername can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our getting started tutorial or testing out new software. Others in your project won't have access, and you're limited to 50GB of storage. Scratch Space You can store temporary working data while your job is running at /tmp . This will map to a directory on our fast scratch network storage specific to your job ID, and clean up once your job is complete. It's also possible to write directly to /scratch/ , for instance if you would like to share your working files across multiple nodes. In this case it's your own responsibility to avoid collisions (i.e. two processes writing to the same file at the same time), and clean up afterwards. N.B. Note that home, project and scratch are all network-based storage that can be accessed by multiple nodes and processes at the same time. Take care that you don't inadvertently write to the same file from multiple jobs at the same time. Staging Local disk is typically faster than shared disks. If you find that your read-writes are slow and you are making use of a lot of I/O you may need to stage your data. Spartan has /data for /home and /projects (large, slower), /scratch for temporary storage data (faster), and as local disk, /var/local/tmp (fastest, not shared). You may need to copy data between these locations. How to Transfer Data In and Out of Spartan Secure Copy (scp) You can use the scp command to move data from your local machine to Spartan. For example, to move mydata.dat from your current working directory to your project directory on Spartan: $ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat You can transfer files from Spartan to your local machine by reversing the order of the arguments like so: $ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat local.dat For Windows users, PuTTY provides an equivalent tool called pscp . If you're data is located on a remote machine, you can SSH into that system first, and then use scp from that machine to transfer your data into Spartan. If you'd prefer a GUI interface, you can use tools like FileZilla (cross-platform) or CyberDuck (OS X Windows). rsync Repeatedly transferring large files in and out of Spartan via scp can be tedious. A good alternative is rsync , which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for scp . $ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat Note that the first argument is the source, and the second is the destination which will be modified to match the source. Not for Long-Term Storage While it's often essential to have fast nearby storage while working on your data, please don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first). Mediaflux Integration Research Platform Services provides a data management service utilising the Mediaflux platform . This platform provides a persistent location for research data and meta-data. To aid integration between Mediaflux and Spartan, Java clients are available on Spartan, allowing data to be downloaded from and uploaded to Mediaflux. Details on Mediaflux integration with Spartan can be found in Section 4 of the Mediaflux support wiki Data and Storage Solutions Beyond Spartan The University offers a range of other data storage and management solutions to meet your needs, beyond the short-term storage available on Spartan, which are described here . In some cases it's possible to integrate these resources with your account on Spartan to streamline your workflow. Get in touch if you'd like to find out more for your particular application.","title":"Managing Data"},{"location":"managing_data/#where-to-store-your-data-on-spartan","text":"Projects Directory Your projects directory is the best place to store research data while you're working on it. It's located at /data/projects/ projectid for older projects, or /data/cephfs/ projectid for newer ones. Others in your project can access it, and 500 GB of storage is available per project. If you need more than this, get in touch and we'll try to find a solution. In general 1 TB of project storage is available upon request, and up to 10 TB is possible if needed. Project storage beyond 10 TB will generally require some sort of co-investment, but this may be waived in some circumstances, particularly for high-value shared datasets. Home Directory Your home directory, i.e. /home/yourusername can be used to store small amounts of data, however this is generally discouraged. It's best suited to short-lived and non-critical data, for example while working through our getting started tutorial or testing out new software. Others in your project won't have access, and you're limited to 50GB of storage. Scratch Space You can store temporary working data while your job is running at /tmp . This will map to a directory on our fast scratch network storage specific to your job ID, and clean up once your job is complete. It's also possible to write directly to /scratch/ , for instance if you would like to share your working files across multiple nodes. In this case it's your own responsibility to avoid collisions (i.e. two processes writing to the same file at the same time), and clean up afterwards. N.B. Note that home, project and scratch are all network-based storage that can be accessed by multiple nodes and processes at the same time. Take care that you don't inadvertently write to the same file from multiple jobs at the same time. Staging Local disk is typically faster than shared disks. If you find that your read-writes are slow and you are making use of a lot of I/O you may need to stage your data. Spartan has /data for /home and /projects (large, slower), /scratch for temporary storage data (faster), and as local disk, /var/local/tmp (fastest, not shared). You may need to copy data between these locations.","title":"Where to Store Your Data on Spartan"},{"location":"managing_data/#how-to-transfer-data-in-and-out-of-spartan","text":"Secure Copy (scp) You can use the scp command to move data from your local machine to Spartan. For example, to move mydata.dat from your current working directory to your project directory on Spartan: $ scp local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat You can transfer files from Spartan to your local machine by reversing the order of the arguments like so: $ scp myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat local.dat For Windows users, PuTTY provides an equivalent tool called pscp . If you're data is located on a remote machine, you can SSH into that system first, and then use scp from that machine to transfer your data into Spartan. If you'd prefer a GUI interface, you can use tools like FileZilla (cross-platform) or CyberDuck (OS X Windows). rsync Repeatedly transferring large files in and out of Spartan via scp can be tedious. A good alternative is rsync , which only transfers the parts that have changed. It can work on single files, or whole directories, and the syntax is much same as for scp . $ rsync local.dat myusername@spartan.hpc.unimelb.edu.au:/data/projects/myproject/remote.dat Note that the first argument is the source, and the second is the destination which will be modified to match the source.","title":"How to Transfer Data In and Out of Spartan"},{"location":"managing_data/#not-for-long-term-storage","text":"While it's often essential to have fast nearby storage while working on your data, please don't use Spartan as your long-term data repository. It's not designed for that, may not conform to the requirements set by your institution or funding body, and we don't guarantee to store your data indefinitely (though we certainly won't get rid of it without asking you first).","title":"Not for Long-Term Storage"},{"location":"managing_data/#mediaflux-integration","text":"Research Platform Services provides a data management service utilising the Mediaflux platform . This platform provides a persistent location for research data and meta-data. To aid integration between Mediaflux and Spartan, Java clients are available on Spartan, allowing data to be downloaded from and uploaded to Mediaflux. Details on Mediaflux integration with Spartan can be found in Section 4 of the Mediaflux support wiki","title":"Mediaflux Integration"},{"location":"managing_data/#data-and-storage-solutions-beyond-spartan","text":"The University offers a range of other data storage and management solutions to meet your needs, beyond the short-term storage available on Spartan, which are described here . In some cases it's possible to integrate these resources with your account on Spartan to streamline your workflow. Get in touch if you'd like to find out more for your particular application.","title":"Data and Storage Solutions Beyond Spartan"},{"location":"software/","text":"This page outlines usage and tips for some of the most popular software being used on Spartan. Spartan uses a modules system ( lmod ) to load and unload different packages, including different versions of the same software. This allows many different software packages to be installed on Spartan at once without interfering with each other You can check what's currently installed using the module avail command, search using module spider , and load a particular module with the module load command. For example, to load MATLAB 2016a, use module load MATLAB/2016a . Generally you shouldn't load modules from the login node, instead working on a compute node, either via an interactive session (launched with sinteractive ), or from within your Slurm script. Job Script Examples We maintain example job scripts for various software packages on Spartan. You can browse these at /usr/local/common or at https://github.com/resbaz/spartan-examples Python There are multiple versions of Python installed on Spartan, which you can check using module spider Python . Common packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using pip install --user package name . This works well for pure-python packages, but you may encounter errors for those that link to other binary packages. R R version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide. MATLAB MATLAB 2016a is installed on Spartan, along with all of the standard toolboxes. MATLAB can be invoked with a particular script using matlab -nodisplay -nodesktop -r \"run my_script.m\" . You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.","title":"Software"},{"location":"software/#job-script-examples","text":"We maintain example job scripts for various software packages on Spartan. You can browse these at /usr/local/common or at https://github.com/resbaz/spartan-examples","title":"Job Script Examples"},{"location":"software/#python","text":"There are multiple versions of Python installed on Spartan, which you can check using module spider Python . Common packages like numpy are already installed with some versions, but may be missing from others. If a Python package is missing, let us know, and we can install it for you. Alternatively, you can install additional packages locally using pip install --user package name . This works well for pure-python packages, but you may encounter errors for those that link to other binary packages.","title":"Python"},{"location":"software/#r","text":"R version 3.2.1 and 3.4.0 are installed on Spartan, along with some common packages. If a package you need is missing, you can either install it locally, or contact us to install it system-wide.","title":"R"},{"location":"software/#matlab","text":"MATLAB 2016a is installed on Spartan, along with all of the standard toolboxes. MATLAB can be invoked with a particular script using matlab -nodisplay -nodesktop -r \"run my_script.m\" . You may need to add particular working directories so MATLAB can find all the scripts necessary for your job.","title":"MATLAB"},{"location":"status/","text":"Availability Is Spartan operating normally? Current Usage How busy is Spartan today? How to Interpret This plot provides indicates how many CPUs (cores) are in-use on Spartan. Utilization may occasionally exceed 100% as resources are added/removed from service (utilization is always relative to the most recent available CPU count). This data is acquired using the sacct command in Slurm to get a list of all recent jobs and their start/end times, and counting how many cores are allocated relative to total capacity. Wait Time How long will my job take to start? Partition: CPUs Requested: Wall Time: How to Interpret This plot provides data on how long previous jobs have taken to start, which can be used as guidance on how long your job might take to start. Note however that \"Past performance is no guarantee of future results\"; wait times can fluctuate quickly due to changes in usage or outages, and wait time could be considerably more or less than the historic average. Daily averages are shown, but points may be missing for days where there were no jobs matching the selected characteristics. This data is acquired using the sacct command in Slurm to get a list of all recent jobs and their start/end times. window.onload = function() { Plotly.d3.json(\"https://swift.rc.nectar.org.au:8888/v1/AUTH_5634a7ad82ad49579e4192f4db90191f/spartan_metrics/wait_time.json\" + '?' + Math.floor(Math.random() * 1000), function (data) { function assignOptions(options, selector) { $.each(options, function (val, text) { selector.append( $('<option></option>').val(text).html(text) ); }); } function updatePlot() { var graphData = [ { x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'], y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y'], text: 'hours', mode: 'markers', hoverinfo: 'x+text+y', marker: { size: 10 }, name: 'Daily Average' }, { x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'], y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y_rolling_average'], hoverinfo: 'none', line: { color: 'rgb(150, 150, 150)', width: 1, dash: 'dash' }, name: 'Rolling 14d Mean' } ]; var layout = { title: 'Job Wait Time', width: 700, height: 400, yaxis: { title: 'Hours', hoverformat: '.2f hours' }, legend: { x: 0.5, y: 1.2, orientation: 'h', xanchor: 'center', } }; Plotly.newPlot('graph', graphData, layout); } var partitionSelect = $('#partitionSelect'); var coreSelect = $('#coreSelect'); var wallTimeSelect = $('#wallTimeSelect'); // Populate drop-down options assignOptions(data['options']['partitions'], partitionSelect); assignOptions(data['options']['cores'], coreSelect); assignOptions(data['options']['wall_times'], wallTimeSelect); // Attach listeners partitionSelect.change(updatePlot); coreSelect.change(updatePlot); wallTimeSelect.change(updatePlot); // Build initial plot updatePlot() }); }","title":"_Status"},{"location":"status/#availability","text":"Is Spartan operating normally?","title":"Availability"},{"location":"status/#current-usage","text":"How busy is Spartan today? How to Interpret This plot provides indicates how many CPUs (cores) are in-use on Spartan. Utilization may occasionally exceed 100% as resources are added/removed from service (utilization is always relative to the most recent available CPU count). This data is acquired using the sacct command in Slurm to get a list of all recent jobs and their start/end times, and counting how many cores are allocated relative to total capacity.","title":"Current Usage"},{"location":"status/#wait-time","text":"How long will my job take to start? Partition: CPUs Requested: Wall Time: How to Interpret This plot provides data on how long previous jobs have taken to start, which can be used as guidance on how long your job might take to start. Note however that \"Past performance is no guarantee of future results\"; wait times can fluctuate quickly due to changes in usage or outages, and wait time could be considerably more or less than the historic average. Daily averages are shown, but points may be missing for days where there were no jobs matching the selected characteristics. This data is acquired using the sacct command in Slurm to get a list of all recent jobs and their start/end times. window.onload = function() { Plotly.d3.json(\"https://swift.rc.nectar.org.au:8888/v1/AUTH_5634a7ad82ad49579e4192f4db90191f/spartan_metrics/wait_time.json\" + '?' + Math.floor(Math.random() * 1000), function (data) { function assignOptions(options, selector) { $.each(options, function (val, text) { selector.append( $('<option></option>').val(text).html(text) ); }); } function updatePlot() { var graphData = [ { x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'], y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y'], text: 'hours', mode: 'markers', hoverinfo: 'x+text+y', marker: { size: 10 }, name: 'Daily Average' }, { x: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['x'], y: data['data'][wallTimeSelect.val()][partitionSelect.val()][coreSelect.val()]['y_rolling_average'], hoverinfo: 'none', line: { color: 'rgb(150, 150, 150)', width: 1, dash: 'dash' }, name: 'Rolling 14d Mean' } ]; var layout = { title: 'Job Wait Time', width: 700, height: 400, yaxis: { title: 'Hours', hoverformat: '.2f hours' }, legend: { x: 0.5, y: 1.2, orientation: 'h', xanchor: 'center', } }; Plotly.newPlot('graph', graphData, layout); } var partitionSelect = $('#partitionSelect'); var coreSelect = $('#coreSelect'); var wallTimeSelect = $('#wallTimeSelect'); // Populate drop-down options assignOptions(data['options']['partitions'], partitionSelect); assignOptions(data['options']['cores'], coreSelect); assignOptions(data['options']['wall_times'], wallTimeSelect); // Attach listeners partitionSelect.change(updatePlot); coreSelect.change(updatePlot); wallTimeSelect.change(updatePlot); // Build initial plot updatePlot() }); }","title":"Wait Time"},{"location":"guides/","text":"Spartan Desktop Spartan Desktop allows you to run software with a GUI (e.g. MATLAB) from within Spartan. Containers You can run your software on Spartan using containers, rather than via the managed modules system . This article explains when you might want to do this, and how. GPGPU Details on our new GPGPU service for project partners.","title":"Guides"},{"location":"guides/#spartan-desktop","text":"Spartan Desktop allows you to run software with a GUI (e.g. MATLAB) from within Spartan.","title":"Spartan Desktop"},{"location":"guides/#containers","text":"You can run your software on Spartan using containers, rather than via the managed modules system . This article explains when you might want to do this, and how.","title":"Containers"},{"location":"guides/#gpgpu","text":"Details on our new GPGPU service for project partners.","title":"GPGPU"},{"location":"guides/containers/","text":"Container frameworks such as Docker allow your application, its dependencies and operating system to be encapsulated in a single file that can then be run in isolation from the host system. It's a bit like a light-weight virtual machine. This allows your software to be more portable, for example if you need to share your analysis environment with a colleague, but the dependencies are complex to install and keep consistent. In general, we recommend that you check if the software you need is already installed directly on Spartan, and if it's not, or the wrong version, get in contact to see if it can be installed for you. By allowing us to manage software for you, we can make sure that it's optimised for our particular hardware, and is available for everyone. However, there are cases when it might be more practical to bring your own container instead. When should I consider a container? When your software: Has complex or out-of-date dependencies that aren't easily installed on Spartan. Assumes a particular operating system, e.g. requires Ubuntu, but Spartan runs on Red Hat. Is highly-modified or a legacy version that isn't likely to be of use to other researchers. Is being run across heterogeneous infrastructure (e.g. Spartan, your laptop, other HPC systems and/or cloud), and containers make it easier to maintain consistency. What are the downsides of containers? The images themselves can be very large, consuming your storage quota, and being slow to transfer to/from Spartan. The container might be optimized for a particular processor architecture, running slowly (or not at all) on systems that differ. Their makeup and integrity can be opaque (although many common software packages will have officially supported container images). Containers on Spartan It general Docker isn't appropriate for HPC environments like Spartan in which regular users don't have administrator (root) access. A good alternative is Singularity , a container framework targeted for research use, which many HPC centres support (including Spartan). Check out Singularity's documentation to learn more, in conjunction with below which is specific to Spartan. Creating a Container In general, it's not possible to build a new container on Spartan. Instead, you would fetch an existing container (from Docker or Singularity Hub), build one on your own computer, or on a computer in the cloud. For the latter, the Melbourne Research Cloud is suitable, and often quite convenient as file transfer between Spartan and cloud instances are fast since they are in the same data centre. Running a Container on Spartan You can load Singularity using our modules system like so: module load Singularity/2.5.0-intel-2017.u2 An example job script is available at /usr/local/common/Singularity , which is also mirrored on GitHub.","title":"_Containers"},{"location":"guides/containers/#when-should-i-consider-a-container","text":"When your software: Has complex or out-of-date dependencies that aren't easily installed on Spartan. Assumes a particular operating system, e.g. requires Ubuntu, but Spartan runs on Red Hat. Is highly-modified or a legacy version that isn't likely to be of use to other researchers. Is being run across heterogeneous infrastructure (e.g. Spartan, your laptop, other HPC systems and/or cloud), and containers make it easier to maintain consistency.","title":"When should I consider a container?"},{"location":"guides/containers/#what-are-the-downsides-of-containers","text":"The images themselves can be very large, consuming your storage quota, and being slow to transfer to/from Spartan. The container might be optimized for a particular processor architecture, running slowly (or not at all) on systems that differ. Their makeup and integrity can be opaque (although many common software packages will have officially supported container images).","title":"What are the downsides of containers?"},{"location":"guides/containers/#containers-on-spartan","text":"It general Docker isn't appropriate for HPC environments like Spartan in which regular users don't have administrator (root) access. A good alternative is Singularity , a container framework targeted for research use, which many HPC centres support (including Spartan). Check out Singularity's documentation to learn more, in conjunction with below which is specific to Spartan.","title":"Containers on Spartan"},{"location":"guides/containers/#creating-a-container","text":"In general, it's not possible to build a new container on Spartan. Instead, you would fetch an existing container (from Docker or Singularity Hub), build one on your own computer, or on a computer in the cloud. For the latter, the Melbourne Research Cloud is suitable, and often quite convenient as file transfer between Spartan and cloud instances are fast since they are in the same data centre.","title":"Creating a Container"},{"location":"guides/containers/#running-a-container-on-spartan","text":"You can load Singularity using our modules system like so: module load Singularity/2.5.0-intel-2017.u2 An example job script is available at /usr/local/common/Singularity , which is also mirrored on GitHub.","title":"Running a Container on Spartan"},{"location":"guides/desktop/","text":"Beta Note that Spartan Desktop is currently in beta, and is not yet ready for general use. You're welcome to give it a try and provide feedback, but there may be bugs or missing features. While HPC systems like Spartan are oriented to command line batch processing, certain applications benefit from access to an interactive desktop environment. For example, you might like to quickly view some intermediate data, without having to download it to your own computer, and taking advantage of software already installed on Spartan. Spartan Desktop is based on Strudel, a cross-platform application developed by CVL/MASSIVE . It will create an interactive job on Spartan, launch a desktop session, and connect you via a secure SSH tunnel using TurboVNC. Getting Started Install TurboVNC and Strudel as per the instructions at https://www.massive.org.au/userguide/cluster-instructions/strudel Add Spartan as a site to Strudel. Go to 'File - Manage Sites', and click 'New'. Set the name field as Spartan Desktop and URL as http://dashboard.hpc.unimelb.edu.au/spartan-rcg.json . Click Ok, and make sure the site is set as active. Now to connect... select spartan-rcg from the site menu, and enter your Spartan username. Enable the advanced options, and set the SSH tunnel cipher as aes128-cbc . Click Login. Strudel will ask for your Spartan password, start a desktop session, and connect to it via TurboVNC. Usage From your desktop session, you can load modules and start jobs just as you would from a command line session. For example, to start MATLAB:","title":"_Spartan Desktop"},{"location":"guides/desktop/#getting-started","text":"Install TurboVNC and Strudel as per the instructions at https://www.massive.org.au/userguide/cluster-instructions/strudel Add Spartan as a site to Strudel. Go to 'File - Manage Sites', and click 'New'. Set the name field as Spartan Desktop and URL as http://dashboard.hpc.unimelb.edu.au/spartan-rcg.json . Click Ok, and make sure the site is set as active. Now to connect... select spartan-rcg from the site menu, and enter your Spartan username. Enable the advanced options, and set the SSH tunnel cipher as aes128-cbc . Click Login. Strudel will ask for your Spartan password, start a desktop session, and connect to it via TurboVNC.","title":"Getting Started"},{"location":"guides/desktop/#usage","text":"From your desktop session, you can load modules and start jobs just as you would from a command line session. For example, to start MATLAB:","title":"Usage"}]}